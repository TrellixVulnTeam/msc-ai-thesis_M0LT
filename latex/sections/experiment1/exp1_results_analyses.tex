\section{Detecting Age-Related Linguistic Patterns in Dialogue and Discourse}
\label{sec:exp1_results}

We first report results on \emph{discourse}
% dataset
to check whether we replicate
previous findings. Then, we focus on \emph{dialogue} to answer our research questions. We report accuracy and $F_1$ for each age group.

\subsection{Classification Performance on Discourse}
Table~\ref{tab:blog_classification} reports the results.
% obtained by all tested models.
% on the \emph{discourse} dataset.
%, BAC. 
As can be seen,
all models are well above the majority class baseline in terms of both accuracy (0.472) and 
% age group-specific 
$F_1$s (0.642). This overall confirms previous evidence \citep{schler2006effects} that language features of (written) \emph{discourse} can predict, to some extent, the age group to which the person
% speaker
belongs. At the same time, BERT fine-tuned on the age classification task stands out as our best-performing model by achieving highest accuracy (0.742) and highest $F_1$ in all age groups.
% BiLSTM and LSTM rank second (0.720) and third (0.714) in terms of accuracy, respectively, while a somehow more mixed pattern is observed for $F_1$ scores.
%, where $n$-gram models come into play.
% , where $n$-gram models can perform on par with or better than LSTMs on certain age groups.
LSTM ranks  second (0.663) in terms of accuracy, while the second best $F_1$ scores for all age groups are distributed among the bigram and trigram models.
Overall, these results indicate that powerful neural models that are capable of representing the linguistic context % (BERT \emph{in primis}) 
have a great advantage on this dataset over simpler $n$-gram models, which are more than 10 accuracy points behind. 

Finally, it should be noted that our best results are slightly lower than those obtained by~\citet{schler2006effects}. This could be due to two main reasons: First, they experiment with a
differently pre-processed and smaller
% (smaller)
% significantly smaller 
dataset than ours.\footnote{They are left with roughly 511K datapoints after pre-processing, 
while we experiment with around 677K.}
% which also has a different majority class and corresponding baseline (see Table~\ref{tab:blog_classification}). 
Second, while in our approach all models are trained end-to-end on the task,~\citet{schler2006effects} use
hand-crafted features that are specific to the dataset, 
which could constitute an advantage.

\subsection{Classification Performance on Dialogue}
Table~\ref{tab:bnc_classification_ws} reports the results obtained on the dialogue dataset. As can be seen, BERT fine-tuned on the task is again the best-performing model in terms of accuracy (0.729), which confirms the effectiveness of this model in detecting age-related linguistic differences. At the same time, it can be noted that the model based on trigrams is basically on par with it in terms of accuracy (0.722) and well above both LSTM and BiLSTM (0.693 and 0.691, respectively). A similar pattern is shown for $F_1$ scores, where BERT fine-tuned and the trigram model achieve comparable performance, with LSTMs being overall behind. 

Overall, our results indicate that predicting the age group to which a speaker belongs, using text-based models, is possible also for \emph{dialogue} data, though the task appears to be somehow more challenging compared to when performed on discourse.
Note that the improvement with respect to the majority/random baseline is lower in dialogue (22.9 and 27 accuracy point increase w.r.t baselines for best model for dialogue and discourse datasets, respectively).

% \san{do we need a comment on the comparable accuracy, but with different N of classes and majority baseline?}\raq{maybe say: note that the improvement wrt to the random baseline is lower in dialogue}.
At the same time, the different ranking of models observed between discourse and dialogue suggests possibly different strategies used by models to solve the task. In particular, the very good performance of the trigram model in \emph{dialogue} suggests that leveraging `local' linguistic features captured by $n$-grams is extremely effective in this setup. This could indicate that differences among various age groups are at the level of
% \textcolor{red}{
local lexical constructions.
% }
% \raq{local lexical constructions?}
% \raq{the next bit doesn't stand up, not only because of topics, but also because without stop words I don't see how we can make any claims regarding style at this stage}
% lexical and stylistic features of the language used, rather than involving topical aspects. \san{if we don't include the analysis on topics, this cannot be claimed}
This deserves further analysis, which we carry out in the next section.
% \san{We should add a comment on how performance is with respect to topics: we refer to the plot}


% Here we have the tables with the numbers (accuracy, precision, etc.) for each model on each dataset. Highlight what are best models on each dataset, what are differences between the two datasets, etc.
% Are there differences or not? Yes. Models above baselines.

% What are the features that drive the prediction? We look into two best-performing models.




% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l c c c c}
%     \toprule
%     \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(13-17)}$ & $\boldsymbol{F}_1^{(23-27)}$ & $\boldsymbol{F}_1^{(33+)}$\\
%     % -plus)}$\\
%      & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     % Baseline (
%     Majority class
%     % ) 
%     & 0.472
%     % (0.000) 
%     & * & 0.642
%     % (0.000)
%     & *\\
%     % Best model by \citeauthor
%     \citet{schler2006effects} & 0.762 & 0.860 & 0.748 & 0.504 \\
%     \midrule
%     unigram & 0.603 (0.001) & 0.760 (0.003) & 0.706 (0.001) & 0.491 (0.003)\\
%     bigram & 0.627 (0.001) & 0.788 (0.001) & 0.715 (0.001) & \textcolor{blue}{0.504} (0.002)\\
%     trigram & 0.625 (0.002) & \textcolor{blue}{0.789} (0.001) & 0.716 (0.002) & 0.485 (0.003)\\
%     \midrule
%     LSTM & 0.714 (0.005) & 0.772 (0.007) & 0.740 (0.004) & 0.501 (0.006) \\
%     BiLSTM & \textcolor{blue}{0.720} (0.001) & 0.778 (0.005) & \textcolor{blue}{0.746} (0.001) & 0.486 (0.016)\\
%     \midrule
%     BERT$_{frozen}$
%     % -base-uncased (frozen)
%     & 0.604 (0.001) & 0.627 (0.011) & 0.666 (0.005) & 0.198 (0.018)\\
%     BERT$_{FT}$
%     % -base-uncased (fine-tuned)
%     & \textbf{0.731} (0.002) & \textbf{0.791} (0.003) & \textbf{0.752} (0.005) & \textbf{0.521} (0.020)\\
%     % \hline
%     % GPT-2 medium (frozen) & 0.621 (0.003) & 0.654 (0.012) & 0.678 (0.009) & 0.247 (0.032)\\
%     \bottomrule
%     \end{tabular}
%     \caption{Discourse dataset. \len{\textbf{Excluding} stopwords} Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest. *: $F_1$ is actually $0/0$.}
%     % \san{If we agree with the abbreviations used in this table, we could use them also in Table 3}}
%     \label{tab:blog_classification}
% \end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l c c c c}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(13-17)}$ & $\boldsymbol{F}_1^{(23-27)}$ & $\boldsymbol{F}_1^{(33+)}$\\
    % -plus)}$\\
     & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    % Baseline (
    Majority class
    % ) 
    & 0.472
    % (0.000) 
    & * & 0.642
    % (0.000)
    & *\\
    % Best model by \citeauthor
    \citet{schler2006effects}** & 0.762 & 0.860 & 0.748 & 0.504 \\
    % Majority class & 0.430 & * & 0.760 & 0.504 \\
    \midrule
    unigram & 0.601 (0.001) & 0.764 (0.001) & 0.704 (0.001) & 0.498 (0.003)\\
    bigram & 0.625 (0.001) & \textcolor{blue}{0.790} (0.001) & \textcolor{blue}{0.712} (0.001) & \textcolor{blue}{0.518} (0.001)\\
    trigram & 0.623 (0.001) & \textcolor{blue}{0.790} (0.001) & 0.712 (0.002) & 0.498 (0.002)\\
    \midrule
    LSTM & \textcolor{blue}{0.663} (0.005) & 0.748 (0.003) & 0.664 (0.010) & 0.502 (0.004) \\
    BiLSTM & 0.618 (0.008) & 0.732 (0.003) & 0.579 (0.016) & 0.509 (0.004)\\
    \midrule
    BERT$_{frozen}$
    & 0.623 (0.002) & 0.658 (0.006) & 0.678 (0.007) & 0.256 (0.041)\\
    BERT$_{FT}$
    % -base-uncased (fine-tuned)
    & \textbf{0.742} (0.010) & \textbf{0.813} (0.007) & \textbf{0.749} (0.013) & \textbf{0.592} (0.009)\\
    % \hline
    % GPT-2 medium (frozen) & 0.621 (0.003) & 0.654 (0.012) & 0.678 (0.009) & 0.247 (0.032)\\
    \bottomrule
    \end{tabular}
    \caption{Discourse dataset. Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest. *: $F_1$ is actually $0/0$. **: these results were obtained with a different final dataset.}
    % \san{If we agree with the abbreviations used in this table, we could use them also in Table 3}}
    \label{tab:blog_classification}
\end{table*}

\begin{table*}[h]
    \centering
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{l c c c}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(19-29)}$  & $\boldsymbol{F}_1^{(50+)}$ \\ 
    % -plus)}$ \\
     & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better \\
    \midrule
    Random
    % Baseline (random guessing) 
    & 0.500
    % (0.000) 
    & 0.500
    % (0.000) 
    & 0.500 
    % (0.000)
    \\ \midrule
    unigram & 0.701 (0.007) & 0.708 (0.009)  & 0.693 (0.004)\\
    bigram & 0.719 (0.002) & 0.724 (0.003) & 0.714 (0.003)\\
    trigram &  \textcolor{blue}{0.722} (0.001) & \textcolor{blue}{0.727} (0.003) & \textcolor{blue}{0.717} (0.001)\\ \midrule
    LSTM &  0.693 (0.003) & 0.696 (0.005) & 0.691 (0.007)\\
    BiLSTM & 0.691 (0.009) & 0.702 (0.017) & 0.679 (0.007) \\ \midrule
    BERT$_{frozen}$
    % -base uncased (frozen) 
    & 0.675 (0.003) & 0.677 (0.008) & 0.673 (0.010)\\
    BERT$_{FT}$
    % -base uncased (fine-tuned) 
    & \textbf{0.729} (0.002) & \textbf{0.730} (0.011) & \textbf{0.727} (0.010)\\
    % \hline
    % GPT-2 medium (frozen) & 0.671 (0.003) & 0.665 (0.014) & 0.675 (0.017) \\
    \bottomrule
    \end{tabular}
    % }
    \caption{Dialogue dataset. Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest.}
    \label{tab:bnc_classification_ws}
\end{table*}

% \begin{table*}[h]
%     \centering
%     % \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l c c c}
%     \toprule
%     \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(19-29)}$  & $\boldsymbol{F}_1^{(50+)}$ \\ 
%     % -plus)}$ \\
%      & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better \\
%     \midrule
%     Random
%     % Baseline (random guessing) 
%     & 0.500
%     % (0.000) 
%     & 0.500
%     % (0.000) 
%     & 0.500 
%     % (0.000)
%     \\ \midrule
%     unigram & 0.702 (0.006) & 0.713 (0.006)  & 0.690 (0.006)\\
%     bigram & 0.703 (0.006) & 0.713 (0.005) & 0.693 (0.008)\\
%     trigram &  \textcolor{blue}{0.709} (0.007) & \textbf{0.718} (0.007) & 0.700 (0.008)\\ \midrule
%     LSTM & 0.696 (0.005) & 0.689 (0.018) & \textcolor{blue}{0.701} (0.016)\\
%     BiLSTM & 0.684 (0.007) & 0.688 (0.018) & 0.679 (0.016) \\ \midrule
%     BERT$_{frozen}$
%     % -base uncased (frozen) 
%     & 0.673 (0.005) & 0.679 (0.013) & 0.667 (0.018)\\
%     BERT$_{FT}$
%     % -base uncased (fine-tuned) 
%     & \textbf{0.710} (0.006) & \textcolor{blue}{0.717} (0.007) & \textbf{0.703} (0.014)\\
%     % \hline
%     % GPT-2 medium (frozen) & 0.671 (0.003) & 0.665 (0.014) & 0.675 (0.017) \\
%     \bottomrule
%     \end{tabular}
%     % }
%     \caption{Dialogue dataset \len{Excluding stopwords}. Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest.}
%     \label{tab:bnc_classification}
% \end{table*}


\begin{table*}[h]
\resizebox{\linewidth}{!}{
% \small
\begin{tabular}{@{}lllll@{}}
\toprule
\bf actual age & \bf both correct                & \bf both wrong                 & \bf BERT$_{FT}$ correct | trigram wrong                              & \bf trigram correct | BERT$_{FT}$ wrong                                                            \\ \midrule
19-29     & oh that's cool              & A retrospective exhibition & what even on the green slope?             & really?                                                                    \\
19-29     & a text and then I'll do it  & chuck them in those pots   & yeah you told me to do you told me to do  & and she like won't eat any carbs and she's like                            \\
19-29     & yeah                        & mm                         & somebody made the f***ing table           & do you not like total greens? \\ 
% 19-29     & yeah they go and like work for Pixar and stuff                        & yeah but that's for er storage reasons I suppose                         & ...           & ... \\
% 19-29     & I don't know? sounded crazy                        & ...                         & ...           & ... \\
% 19-29     & do you have exams again?                        & ...                         & ...           & ... \\
\midrule
% well er it's not acceptable just to ditch \textless{}unclear\textgreater{} \\ \hline
50+       & I said no I don't have them & yeah                       & really?                                   & my under stairs in the kitchen                                             \\
50+       & that's of course            & no no that's alright       & it's still we we frequently walk that way & in the first place                                                         \\
50+       & oh right                    & what a tragic life         & since this this was new this house?       & thank you very much\\
% 50+       & yeah ah that one in your sewing room hasn't got anything in it                    & ...         & ...       & ...\\
% 50+       & was in the suffragette film and that was very interesting                    & ...         & ...       & ...\\
% 50+       & association and that has to be voted on                    & ...         & ...       & ...\\\bottomrule
\bottomrule
\end{tabular}
}
\caption{Examples
% cherry-picked examples per age group 
where both models are correct/wrong or only BERT$_{FT}$/trigram is correct. Illustrative example: the sentence, \textit{A retrospective exhibition} was uttered by a speaker of age 19-29, but was incorrectly classified by both models (BERT$_{FT}$ and trigram) as coming from a person age 50+.}\label{tab:qualexamples}
\end{table*}

% \begin{figure}[h]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=1\columnwidth]{figures/bnc_rb_bert_trigram_acc_freq_topics_w_none.png}
%         \caption{BERT$_{FT}$ and trigram test accuracies per topic for most frequent topics (including none/no info).}
%         \label{fig:bnc_tri_bert_acc_topics_w_none}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=1\columnwidth]{figures/bnc_rb_trigram_bert_case_dist_hue_age_ws.png}
%         \caption{Distribution of predicted cases by trigram and BERT$_{FT}$ models for dialogue, split by age groups.}
%         \label{fig:bnc_tri_bert_cases_age}
%      \end{subfigure}
%         \caption{}
%         \label{fig:bnc_blog_age_topic_dist_incl_unk}
% \end{figure}

% \begin{figure}[h]%{0.45\textwidth}
%      \centering
%     %  \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.45\textwidth]{figures/bnc_rb_bert_trigram_acc_freq_topics_w_none.png}
%         \caption{BERT$_{FT}$ and trigram test accuracies per topic for most frequent topics (including none/no info).}
%         \label{fig:bnc_tri_bert_acc_topics_w_none}
% \end{figure}
%      \hfill
% \begin{figure}[h]%{0.45\textwidth}
%     %  \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=0.45\textwidth]{figures/bnc_rb_trigram_bert_case_dist_hue_age_ws.png}
%         \caption{Distribution of predicted cases by trigram and BERT$_{FT}$ models for dialogue, split by age groups.}
%         \label{fig:bnc_tri_bert_cases_age}
%     %  \end{subfigure}
%         \caption{}
%         \label{fig:bnc_blog_age_topic_dist_incl_unk}
% \end{figure}

\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/bnc_rb_bert_trigram_acc_freq_topics_w_none.png} % first figure itself
        \caption{BERT$_{FT}$ and trigram test accuracies per topic for most frequent topics (including none/no info).}\label{fig:bnc_tri_bert_acc_topics_w_none}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/bnc_rb_trigram_bert_case_dist_hue_age_ws.png} % second figure itself
        \caption{Distribution of predicted cases by trigram and BERT$_{FT}$ models for dialogue, split by age groups.}\label{fig:bnc_blog_age_topic_dist_incl_unk}
    \end{minipage}
\end{figure}

\section{Age Detection Analyses}
\label{sec:exp1_analyses}
Predominantly with the goal of obtaining insight about age-related distinguishing features in dialogue that can inform our subsequent experiment on age-adaptive dialogue generation (Experiment 2 in Chapter \ref{ch:experiment2}), we focus our analysis on the classification results obtained on the dialogue dataset. In particular, we compare the two best-performing models, namely BERT$_{FT}$
and the one using trigrams, and aim to shed light on  what cues they use to solve the task.
We first analyze how these models perform with respect to utterances of various topics.
Secondly, we compare the prediction patterns of the two models, which allows us to highlight easy and hard examples.
Finally, we focus on the trigram model and report the $n$-grams that turn out to be most informative to distinguish between age groups.


\subsection{Performance Against Topic}

As described in Section~\ref{sec:datadialogue}, each utterance in the dialogue dataset is annotated with one label which is representative of its topic.\footnote{In particular, it represents one of the utterance's topic, i.e., the one most frequently used in the whole data.} This information is not explicitly available to the models.
To explore how the two models deal with utterances in different topical contexts, we compare the accuracy they achieve on the 15 most frequent topics. The results are shown in Figure~\ref{fig:bnc_tri_bert_acc_topics_w_none}. Two main observations can be made: Firstly, some topics turn out to be generally easier/harder than others, i.e., both models achieve higher/lower performance. To illustrate, both models achieve an accuracy well above 70\% on topics like \emph{food}, \emph{holidays} or \emph{university}, while topics such as \emph{tv}, \emph{family} or \emph{no info} appear to be generally more challenging for both models. While this could be due to (a combination of) various factors, one intuitive possibility is that certain topics allow for more discriminative language features, which could be at the level of the lexicon or the style used to talk about them.

Secondly, some topics appear to be easier for one model rather than the other, and \emph{vice versa}. To illustrate, the trigram model outperforms BERT on the topics \emph{weather}, \emph{holidays} and \emph{tv}, while an opposite pattern is observed for \emph{work}, \emph{health}, and \emph{future plans}. We conjecture that these patterns could be indicative of different strategies and cues exploited by various models to make a prediction. 
% We explore this issue more in-depth in the following section, where we compare the predictions by the two models and qualitatively inspect some examples.

\subsection{Comparing Model Predictions}
We split the data for analysis by whether or not both models make the same correct or incorrect prediction, or whether they differ. Table~\ref{tab:bnc_tri_bert_cases} shows the breakdown of these results.
As can be seen, a quite large fraction of samples are correctly classified by both models (63.17\%), while in 19.78\% cases neither of the models make a correct prediction. The remaining cases are comparably split between cases where only one of the two is correct, with BERT slightly outperforming the trigram model by 1.23 percentage points. %, but the other is not.
As shown in Figure~\ref{fig:bnc_blog_age_topic_dist_incl_unk}, the 19-29 age group appears to be be slightly easier compared to the 50+ group, where models are observed to make more errors: the trigram misclassifies 50+ utterances 1.12 times as often as 19-29 utterances, and 1.17 times as often by BERT$_{FT}$.

To qualitatively inspect what the utterances falling into these classes look like, in Table~\ref{tab:qualexamples} we show a few cherry-picked cases for each age group. 
% \raq{
We notice that, not surprisingly, both models have trouble with backchanneling utterances consisting of a single word, such as \emph{yeah}, \emph{mm}, or \emph{really?}, which are used by both age groups.
% }
For example, both models seem to consider \emph{yeah} as a `young' cue, which leads to wrong predictions when \emph{yeah} is used by a speaker in the 50+ group. As for the utterance \emph{really?}, BERT$_{FT}$ assigns it to the 50+ group, while the trigram model makes the opposite prediction.
% \raq{
This indicates that certain utterances simply do not contain sufficient distinguishing information, and model predictions that are based on them should therefore not be considered reliable.
% should not be considered reliable??

This seems to be particularly the case for short utterances. Indeed, through comparing the average length of the utterances incorrectly classified by both models (rightmost column of Table~\ref{tab:bnc_tri_bert_cases}), we notice that they are much shorter than those belonging to the other cases. This is interesting, and indicates a key challenge in the analysis of dialogue data: 
on average, shorter utterances contain less signal. On the other hand, short utterances can provide rich conversational signal in dialogue; for example, backchanneling, exclamations, or other acknowledging acts. As a consequence, using length alone as a filter is not an appropriate approach, as it can remove aspects of language use key to differentiating speaker groups.


\subsection{Most Informative N-grams}
%\js{todo: analysis on trigrams: both quant and qual because we select them based on their effectiveness (top 10) + perform more qual investigation (e.g. nouns, content words, idiosyncratic expressions; also related to point below) (optional, but very nice) analysis of features wrt topics: are there features that are not dependent on the topic under discussion (i.e., more "linguistic" features)?}

\begin{table}[H]
    \centering
    \begin{tabular}{@{}l l @{\hspace*{25pt}} l l@{}}
    \toprule
    \multicolumn{2}{c}{19-29} & \multicolumn{2}{c}{50+}\\
    \textbf{coef.} & \textbf{n-gram} & \textbf{coef.} & \textbf{n-gram}\\
    \midrule
    -3.20 & um & 2.37 & yes\\
    -2.84 & cool & 2.12 & you know\\
    -2.58 & s**t & 2.09 & wonderful\\
    -2.12 & hmm & 1.90 & how weird\\
    -2.09 & like & 1.84 & chinese\\
    -2.02 & was like & 1.73 & right\\
    -1.96 & love & 1.71 & building\\
    -1.96 & as well & 1.66 & right right\\
    -1.88 & as in & 1.55 & so erm\\
    -1.84 & cute & 1.43 & mm mm\\
    -1.82 & uni & 1.41 & cheers\\
    -1.79 & massive & 1.39 & shed\\
    -1.79 & wanna & 1.37 & pain\\
    -1.79 & f**k & 1.36 & we know\\
    -1.72 & tut & 1.08 & yeah exactly\\
    \bottomrule
    \end{tabular}
    \vspace{3mm}
    \caption{
    % \len{Fix scape between table and caption.} 
    For each age group, top 15 most informative $n$-grams used by the trigram model. \textbf{coef.} is the coefficient (and sign) of the corresponding $n$-gram for the logistic regression model: the higher its absolute value, the higher the utterance's odds to belong to one age group.
    % Greater absolute value of \textbf{coef.} indicates occurrence of the $n$-gram results in the model assigning higher odds to the utterance belonging to a certain age group.
    * indicates masking of foul language.}
    % present in the dialogue.}
    \label{tab:top_ngrams_ws}
\end{table}

% \begin{table}[b!]
%     \centering
%     \begin{tabular}{@{}l l @{\hspace*{25pt}} l l@{}}
%     \toprule
%     \multicolumn{2}{c}{19-29} & \multicolumn{2}{c}{50+}\\
%     \textbf{coef.} & \textbf{n-gram} & \textbf{coef.} & \textbf{n-gram}\\
%     \midrule
%     -3.19 & um & 2.29 & yes\\
%     -2.91 & cool & 2.21 & wonderful\\
%     -2.70 & s**t & 1.91 & building\\
%     -2.25 & cute & 1.86 & right right\\
%     -2.15 & uni & 1.80 & something like\\
%     -2.14 & hmm & 1.73 & garden\\
%     -1.97 & wanna & 1.69 & right\\
%     -1.93 & f**k & 1.68 & ordinary\\
%     -1.91 & like & 1.67 & shed\\
%     -1.85 & massive & 1.63 & operation\\
%     -1.83 & yeah course & 1.58 & born\\
%     -1.81 & love & 1.57 & mother\\
%     -1.79 & tut & 1.55 & photographs\\
%     -1.74 & b***h & 1.51 & email\\
%     -1.68 & like oh & 1.08 & anything like\\
%     \bottomrule
%     \end{tabular}
%     \caption{For each age group, top 15 most informative $n$-grams used by the trigram model. \textbf{coef.} is the coefficient (and sign) of the corresponding $n$-gram for the logistic regression model: the higher its absolute value, the higher the utterance's odds to belong to one age group.
%     % Greater absolute value of \textbf{coef.} indicates occurrence of the $n$-gram results in the model assigning higher odds to the utterance belonging to a certain age group.
%     * indicates masking of foul language.}
%     % present in the dialogue.}
%     \label{tab:top_ngrams}
% \end{table}


Analyzing the most informative $n$-grams 
% \raq{
used by the trigram model
%} 
allows us to qualitatively compare the linguistic differences inherent to each age group. In Table~\ref{tab:top_ngrams_ws} we report the top 15 $n$-grams per group.
% within the BNC. 
% \raq{Table XX shows the top 15 n-grams per group.} 
We find, firstly and intuitively, that colloquial language seems somewhat generational, with unigrams particularly indicative of younger speakers consisting of words such as 
\emph{cool} and \emph{massive}, and
% and foul language such as \emph{s**t}, \emph{f**k} and \emph{b***h}, and
% \textit{`awesome' and `col' `massive' `lol' `literally' `mate'}, 
for older speakers, words like
\emph{wonderful}.
% and  \emph{ordinary}.
% \textit{`cheers' `gosh' `right' `wonderful'} 
% \san{some of these examples are not in the table} 
These unigrams are both informative to the model and indicative of differences in both formality and `slang' use across age groups.

These most informative $n$-grams also indicate differences in back-channeling use between age groups; younger speaker's language is more characterized by the use of \emph{hmm}, \emph{um}, \emph{yeah course}, while the top $n$-grams in the older category will more likely use
\emph{yes}, \emph{right}, \emph{right right}.
% `sure sure' and `right right'. 
% `mhm' `yeah course', while the top n-grams in the older category will more likely use `sure sure' and `right right'.
A feature of younger language also apparent from these examples is in their use of more informal language: \emph{yeah course} rather than \emph{yes}.
% `yeah course' rather than `yes'. 
This informal language use also extends to the use of foul language, which make up a percent of the most informative unigrams shown in Table~\ref{tab:top_ngrams_ws}. % (see, e.g., \emph{s**t}, \emph{f**k} and \emph{b***h}).

Interestingly, while topic words make up many of the most informative $n$-grams for older speakers in Table~\ref{tab:top_ngrams_ws}, younger speakers are more defined by their use of slang words such as \textit{wanna}, foul language, or adjectives such as \textit{cute}, \emph{cool}, and \emph{massive}.
% love}. 
A key finding from~\citet{schler2006effects} is in the sentiment of language playing an important role, something which some of the most informative $n$-grams suggest may also be true for the dialogue dataset. As Table~\ref{tab:top_ngrams_ws} demonstrates, younger speakers use more dramatic language % in the younger speakers 
such as negative foul words, and positive \textit{love, cute, cool}; all words with a strong connotative meaning. 
%\textit{hate, really good, oh gosh, excited, scared}, whereas older speakers are comparatively more moderate. \san{Not sure we have examples of dramatic language in the table anymore; in any case, this paragraph needs to be updated} 
This prompts us to hypothesize that further inspection is needed to determine whether the same sentiment pattern will be true of dialogue as it has been reported to be in discourse.

\begin{table}[H]
    \centering
    \begin{tabular}{@{}l  c  c @{}}
    \toprule
    \textbf{} & \textbf{\% cases} & \textbf{avg.\ length ($\pm$std)*}\\
    \midrule
    both correct & 63.17\% & 13.51 ($\pm$18.98) \\
    both wrong & 19.78\% & 5.82 ($\pm$8.33) \\
    only Trigram correct & 7.91 \% & 10.44 ($\pm$11.66) \\
    only BERT correct & 9.14 \% & 11.53 ($\pm$12.12) \\ \bottomrule
    \end{tabular}
    \caption{Percentage (\% cases) of (non-)overlapping (in)correctly predicted cases between trigram and BERT$_{FT}$. *Utterance length measured in tokens.}
    % \caption{Percentages of (non-)overlapping (in)correctly predicted cases between trigram and BERT$_{FT}$ models for BNC. *Pre-processed sequence length is measured in tokens.}
    \label{tab:bnc_tri_bert_cases}
\end{table}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{@{}l  c  c @{}}
%     \toprule
%     \textbf{} & \textbf{\% cases} & \textbf{avg.\ length ($\pm$std)*}\\
%     \midrule
%     both correct & 63.57\% & 7.33 ($\pm$10.10) \\
%     both wrong & 22.89\% & 3.77 ($\pm$4.53) \\
%     only Trigram correct & 6.68 \% & 6.37 ($\pm$6.16) \\
%     only BERT correct & 6.86 \% & 6.86 ($\pm$7.87) \\ \bottomrule
%     \end{tabular}
%     \caption{Percentage (\% cases) of (non-)overlapping (in)correctly predicted cases between trigram and BERT$_{FT}$. *Utterance length measured in tokens.}
%     % \caption{Percentages of (non-)overlapping (in)correctly predicted cases between trigram and BERT$_{FT}$ models for BNC. *Pre-processed sequence length is measured in tokens.}
%     \label{tab:bnc_tri_bert_cases_no_stopwords}
% \end{table}

% Notes on the imbalanced BNC.
% \begin{itemize}
%     \item Attempts were made to account for the original BNC's bias (i.e., the 19-29 age bracket accounts for roughly 80\% of the total considered subset).
%     \item Method 1: weighted loss.
%     \item Method 2: weighted random sampling (i.e., up-sampling of the minority class).
%     \item Weighted random sampling outperformed weighted loss in terms of validation accuracy and $F_1$ scores, but still failed to surpass the baseline.
%     \item  \textit{In terms of test accuracy}, the $n$-gram models succeeded in beating the baseline (predicting the majority class), whereas the best LSTM and fine-tuned BERT-based failed to do so.
%     \item However, the neural discriminators still outperformed all the other models with respect to minority class $F_1$ scores, indicating that (1) the $n$-gram models aren't very useful for correctly classifying the minority class, and that (2) weighted random sampling improved the models' efficacy with respect to the minority class.
%     \item See Appendix \ref{age_disc_bnc} for these results.
% \end{itemize}


