% \textbf{Things to keep in mind when writing this section:}
% \begin{itemize}
%     \item Doesn't have to be longer than 1 page.
%     \item (optional): anecdotal opener to draw attention.
%     \item Brief introduction to the topic of controllable text generation and adaptive dialogue systems.
%     \item Description of the research problem and your questions.
%     \item Placeholder for my contributions and approach.
%     \item Placeholder for overview of remainder of paper.
%     \item \textit{For consistency, maybe start it off with an introduction that you can re-use for your final version. Of course, this will be rewritten for later versions to include findings.}
%     \item The three most important papers to keep in mind for this section are: PPLM, DialoGPT, and Personalized Dialogue Generation with Diversified Traits
% \end{itemize}


 

In recent years, we have witnessed promising advances in natural language processing (NLP) tasks, such as language modeling, reading comprehension, machine translation, controllable text generation, and conversational response generation \citep{radford2019language, DBLP:journals/corr/BahdanauCB14, dathathri2019plug, madotto-etal-2020-plug}. \cite{vaswani2017attention}'s Transformer architecture plays a central role in many of the state of the art (SotA) solutions to these problems. Transformer-based language models (LMs) pre-trained on massive amounts of textual data, most famously OpenAI's GPT-2 (Generative Pre-trained Transformer-2), have demonstrated their usefulness for several of the aforementioned NLP tasks \citep{radford2019language}. For instance, controllable text generation and producing dialogue responses have improved greatly because of GPT-based hybrid models. %Given the non-trivial requirements for sufficiently massive training corpora and computational resources, GPT-based solutions are worth considering for these two problems.

\len{CTG comes out of the blue in the following paragraph. Introduce it a little bit by describing what is is, and why/how it is an important task.}

\begin{itemize}
    \item Controllable text generation entails generating text samples that possess a predefined textual property, like having a positive sentiment, or being about a certain topic.
    \item Controlling more fine-grained linguistic properties, like resemblance of age-specific vernacular, still poses an important, yet unsolved/insufficiently studied (?) challenge.
    \item Personalized interaction between humans and AI systems is crucial to obtain systems that can be trusted by users and are perceived as natural.
    \item (Age-)adaptive language generation can be used to personalize AI-powered personal assistants like Siri and Alexa, improving user experience and trust.
    \item It is important for AI-power conversational agents to be accessible to varying user profiles, rather than targeted at one particular user group. 
    \item In this work, I/we focus on one aspect that may influence successful personalization of conversational agents: user age profile.
\end{itemize}

Controllable text generation (CTG) aims to enforce abstract properties, like writing style, on the passages being produced. Fine-tuning large-scale LMs for writing-style adaptation is extremely expensive, but \cite{dathathri2019plug} and \cite{li-etal-2020-optimus} propose methods that both excel at the task, while bypassing significant retraining costs. Dialogue response generation is the task of producing replies to a conversational agent's prompts, in a manner that is ideally both non-repetitive and relevant to the course of the conversation. With DialoGPT, \cite{zhang2019dialogpt} also manage to leverage GPT-2's powerful fluency for dialogue tasks, by framing them as language modeling tasks where multi-turn dialogue sessions are seen as long texts.

% CTG and dialogue response generation share the overarching objective of producing grammatically correct text that is distinct from any training instance. LJ/RF: this isn't necessarily correct. Every generated response doesn't have to be distinct from any training objective.

\len{Introduce dialogue response generation a bit more. Also emphasize its importance. And then introduce the combined task and its importance.}
A blend of CTG and dialogue response generation, i.e., controllable dialogue response generation, is an interesting and only partially explored route. It ties closely to one of Artificial Intelligence's long-standing goals of achieving human-like conversation with machines, as humans are known to adapt their language use to the characteristics of their interlocutor \citep{gallois2015communication}. Adaptive dialogue generation is difficult due to the challenge of representing traits, like age, gender, or other persona-labeled traits via language expression \citep{zheng2019personalized}.
% and the lack of persona trait labeled dialogue datasets. \cite{zheng2019personalized} explore the problem of personalized dialogue generation, and introduced \texttt{PersonalDialog} a large-scale multi-turn Chinese Mandarin dialogue dataset with personality trait labeling, and persona-aware adaptive dialogue generation models using RNNs and attention mechanisms. 

In this thesis, I investigate the problem of controllable dialogue generation, with a focus on adapting responses to users' age. As a preliminary research objective, I aim to study to what extent a classifier can detect age-related linguistic differences in natural language, and which features are most helpful in age-group detection. Do they (i.e., the linguistic or latent features exploited by the classifier) match the age-related informative features reported in previous work? 
After empirically confirming that speaker age detection is possible, I explore whether large-scale LMs, e.g. GPT-2, can be leveraged for text generation, controlled for age-groups. And what role does the used data play in the differences in output and performance between regular GPT-2 and controllable GPT-2?
Finally, my research focuses on the degree to which such a CTG model is successful in generating dialogue that is adaptive w.r.t. age, such that it has a detectable effect on the perception of the user.

The remainder of this thesis is structured as follows: Chapter \ref{sec:background} positions the subject of controlled text generation in its theoretical background, and and \ref{sec:related_work} compares it to the most relevant related work. The methodology in Chapter \ref{ch:methods} gives detailed explanations of the most important modeling methods and techniques used for this research. \len{The code used to produce the results can be found on GitHub\footnote{\url{https://github.com/lennertjansen/msc-ai-thesis}}. (Is this the best place to mention this?)}

% The main contributions of this paper are ...

% The remainder of this work is structured as follows, ...

\begin{itemize}
    \item When introducing your own work and proposing your hypothesis, use the following argument: \textit{This idea that age prediction from text is more challenging than topic or sentiment prediction could be an indication that controlled language generation for age-differences is also a more nuanced problem than topical steered text generation.}
\end{itemize}