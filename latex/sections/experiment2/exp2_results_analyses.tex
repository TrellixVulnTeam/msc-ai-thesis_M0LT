\newpage
\section{Results of controlled dialogue generation}

\begin{table*}[h]
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    Baseline & 29.60 ($\pm$17.54) & \textbf{0.89} ($\pm$0.09) & \textbf{0.93} ($\pm$0.07) & 0.88 ($\pm$0.10) & 50.0\%\\
    \midrule
    B$_{100MCW}$ & 27.80 ($\pm$16.42) & 0.83 ($\pm$0.12) & 0.91 ($\pm$0.06) & \textcolor{blue}{0.88} ($\pm$0.09) & 50.0\%\\
    B$_{Y, FB, 80}$ & 29.13 ($\pm$15.17) & 0.86 ($\pm$0.10) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.11) & 70\%\\
    B$_{O, FB, 80}$ & \textbf{26.42} ($\pm$8.87) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.92} ($\pm$0.05) & \textbf{0.89} ($\pm$0.10) & 62.2\%\\
    B$_{Y, FB, 85}$ & 28.16 ($\pm$14.52) & 0.87 ($\pm$0.10) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.10) & 69.3\%\\
    B$_{O, FB, 85}$ & 26.79 ($\pm$8.89) & \textcolor{blue}{0.88} ($\pm$0.09) & \textcolor{blue}{0.92} ($\pm$0.05) & 0.88 ($\pm$0.10) & 64.2\%\\
    B$_{Y, 100MIU}$ & 29.16 ($\pm$14.91) & \textbf{0.89} ($\pm$0.09) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.11) & 52.5\%\\
    B$_{O, 100MIU}$ & \textcolor{blue}{26.63} ($\pm$8.36) & 0.87 ($\pm$0.10) & 0.92 ($\pm$0.07) & 0.88 ($\pm$0.10) & 53.7\%\\
    \midrule
    D$_{Y, GPT2}$ & 31.95 ($\pm$14.29) & 0.82 ($\pm$0.17) & 0.87 ($\pm$0.14) & 0.83 ($\pm$0.16) & \textbf{77.7\%}\\
    D$_{O, GPT2}$ & 33.63 ($\pm$24.40) & 0.80 ($\pm$0.18) & 0.87 ($\pm$0.11) & 0.81 ($\pm$0.21) & \textcolor{blue}{72.7}\%\\
    D*$_{Y, DGPT}$ & 41.54 ($\pm$10.87) & 0.91 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.86 ($\pm$0.09) & \textbf{84.1}\%\\
    D*$_{O, DGPT}$ & 38.16 ($\pm$10.77) & 0.87 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 55.6\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Including stopwords.} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws}
\end{table*}



% TABLE 5.6 WITH YOUNG AND OLD PROB. JUST IN CASE %
% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l c c c c c c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Young prob.} & \textbf{Old prob.} & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & - & - & $\uparrow$ better\\
%     \midrule
%     \midrule
%     Baseline & 27.45 ($\pm$7.27) & 0.90 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.52 ($\pm$0.32) & 0.48 ($\pm$0.32) & 57.5\%\\
%     \midrule
%     B$_{100MCW}$ & 26.68 ($\pm$8.77) & 0.89 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.50 ($\pm$0.29) & 0.50 ($\pm$0.29) & 51.7\%\\
%     B$_{Y, FB}$ & 27.11 ($\pm$7.45) & \textbf{0.91} ($\pm$0.09) & \textbf{0.92} ($\pm$0.04) & \textbf{0.87} ($\pm$0.09) & 0.60 ($\pm$0.29) & 0.40 ($\pm$0.29) & 68.3\%\\
%     B$_{O, FB}$ & 25.99 ($\pm$6.41) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.35 ($\pm$0.31) & 0.65 ($\pm$0.31) & 62.5\%\\
%     B$_{Y, 100MIU}$ & 28.48 ($\pm$11.96) & 0.88 ($\pm$0.12) & 0.91 ($\pm$0.06) & 0.86 ($\pm$0.10) & 0.62 ($\pm$0.31) & 0.38 ($\pm$0.31) &69.2\%\\
%     B$_{O, 100MIU}$ & \textbf{25.57} ($\pm$7.44) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & \textbf{0.87} ($\pm$0.09) & 0.38 ($\pm$0.31) & 0.62 ($\pm$0.31) & 58.3\%\\
%     \midrule
%     D$_{Y, GPT2}$ & 33.02 ($\pm$12.24) & 0.85 ($\pm$0.16) & 0.89 ($\pm$0.07) & 0.83 ($\pm$0.12) & 0.70 ($\pm$0.31) & 0.30 ($\pm$0.31) & 73.9\%\\
%     D$_{O, GPT2}$ & 32.86 ($\pm$18.08) & 0.80 ($\pm$0.21) & 0.84 ($\pm$0.13) & 0.79 ($\pm$0.19) & 0.37 ($\pm$0.29) & 0.63 ($\pm$0.29) & 63.3\%\\
%     D$_{Y, GPT2*}$ & 30.98 ($\pm$13.95) & 0.86 ($\pm$0.15) & 0.90 ($\pm$0.06) & 0.84 ($\pm$0.11) & \textcolor{blue}{0.76} ($\pm$0.29) & 0.29 ($\pm$0.29) & 80.0\%\\
%     D$_{O, GPT2*}$ & 34.81 ($\pm$26.76) & 0.84 ($\pm$0.17) & 0.85 ($\pm$0.13) & 0.77 ($\pm$0.23) & 0.31 ($\pm$0.25) & \textcolor{red}{0.69} ($\pm$0.25) & 75.8\%\\
%     \bottomrule
%     \end{tabular}
%     \caption{Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Young and old accuracy are the assigned probabilities of belonging to the young or old age categories.}
%     \label{tab:ctg_results}
% \end{table*}

\paragraph{Notes on Table \ref{tab:ctg_results}}

\begin{itemize}
    \item These are initial results.
    \item All metrics are averaged over 120 samples: 30 samples per group of sequence lengths 8, 16, 32, and 64.
    \item Young and old accuracy (last two columns) denote the probability of belonging to the young or old age-groups assigned by the best performing BERT-based classifier.
    \item I use the same parameter settings as Table 6 of \cite{dathathri2019plug} to make the results comparable, i.e.:
    \begin{itemize}
        \item Step-size 0.02. Step-size is $\alpha$ in Equation \ref{eq:H_update_rule}.
        \item Temperature 1.0.
        \item Number of update iterations: 3.
        \item $\gamma$ 1.5.
        \item GM-scale 0.9.
        \item KL-scale 0.01.
    \end{itemize}
    \item The current baseline is uncontrolled/unperturbed GPT-2.
    \item There are four settings for BoW-based control:
    \begin{itemize}
        \item Young frequency-based wordlist.
        \item Old frequency-based wordlist.
        \item Young + most informative unigrams as wordlist.
        \item Old + most informative unigrams as wordlist.
    \end{itemize}
    \item Initial observations:
    \begin{itemize}
        \item Fractions of distinct uni-,bi, and trigrams do not change.
        \item Perplexity seems to improve when controlling generation for each age-group, which isn't necessarily what one would expect.
        \item The baseline starts off with a higher average probability of belonging to the young age group
        \item Controlling for young-language does result in a slightly greater assigned probability of belonging to young age-bracket.
        \item Controlling for old-language results in a doubling of the assigned probability of belonging to the old age-bracket.
    \end{itemize}
\end{itemize}

\subsection{Bag-of-words control}

\subsubsection{Frequency-based wordlist generation}

Steps taken to create age-specific wordlists (full imbalanced BNC used):
\begin{itemize}
    \item Remove all stopwords. List of stopwords from NLTK's English stopword list. \textbf{TODO: does this make sense? What if differences in use of stopwords are strong indicators of an age-group's speech?}
    \item Order all unique words by frequency per age-group.
    \item For both lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
    \item From both sets of words, remove the words that are in the \textit{union} (i.e., the overlapping set) of the young and old sets.
    \item For both sets, order the words by frequency.
    \item For both remaining lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
    \item \textbf{TODO:} remove curse-words?
    \item Resulting wordlist lengths:
        \begin{itemize}
            \item Young (19-29): 90 words
            \item Old (50 plus): 225 words
        \end{itemize}
\end{itemize}

\subsubsection{Most informative unigrams as wordlists}

\subsection{Discriminator-based control}




\textbf{Notes on the experimental details of Table \ref{tab:ctg_results_ws}:}

\begin{itemize}
    \item All sequences are generated unconditionally. I.e., from $\texttt{|<endoftext>|}$ token.
    \item All results are averaged over 240 samples.
    \item 
\end{itemize}


\section{Controlled text generation analyses}

\subsection{Quantitative analyses}

\len{TODO - Add examples of generated sequences along with their model's configurations, age-group, etc. Similar to dialogue snippets earlier.}

\subsubsection{Quantitative 1: the effects of generated sequence length}

\begin{itemize}
    \item \textit{Main question: how is generated sequence length related to fluency and control?}
    
    \item \textit{Study the relationship between generated sequence length (measured in number of tokens) and automated evaluation metrics (i.e., perplexity, dist-n, and accuracy).}
    
    \item \textit{For every metric and for (all?) models, plot sequence length on the x-axis, and the average metric with confidence intervals on the y-axis.}
    
    \item \textit{Which patterns do you observe?} 
\end{itemize}

\subsubsection{Quantitative 2: the effects of PPLM-parameters on fluency and control}

\begin{itemize}
    \item \textit{Plot and examine the relationship between fluency and control, and various PPLM-parameters (step-size, number of iterations, temperature, top $k$, gamma, KL-scale).}
    \item \textit{Which patterns do you observe?}
    \item \len{How much sense does it make to study this, though? Is that the purpose of my thesis? Hasn't this been studied enough in the PPLM-paper? Which parameters do I choose?}
\end{itemize}

\subsubsection{Quantitative 3: BERT-classifier visualizations.}

\begin{itemize}
    \item \textit{\textbf{NB:} This is more relevant to the classification experiments, than to the controlled generation experiments.}
    
    \item \textit{Use BertViz to visualize what parts of sequences BERT's transformer heads and neurons are focusing on.}
    
    \item \url{https://github.com/jessevig/bertviz}
\end{itemize}




\subsection{Qualitative analyses}

\subsubsection{Qualitative 1: summary statistics and qualitative inspection of various cases}

\begin{itemize}
    \item \textit{Similar to (error)case analyses of Experiment 1.}
    
    \item \textit{Provide summary statistics and (\textbf{qualitative}) inspection of generated sequences per case.}
    
    \item \textit{Cases could be: (1) sequences with low, average, high, or very-high perplexity. (2) (in)correctly classified generated sequences.}
    
    \item \textit{What patterns do you observe among, e.g., misclassified sequences with low perplexity?}
    
    \item \textit{Provide table of examples per case and age-group. Similar to table \ref{tab:qualexamples}}
\end{itemize}


\subsubsection{Qualitative 2: Human evaluation of fluency, grammaticality, and relevancy}

\begin{itemize}
    \item \textit{Generate and sample text passages for a variety of model-configurations and age-groups.}
    \item \textit{Have a group of human participants rate these sequences on a scale from 1 to 5 for their (1) fluency, (2) grammaticality, (3) relevance to the prompt (if there is one)}
    \item \textit{Average the ratings, and compare the human evaluation metrics to the automated evaluation metrics reported in Table \ref{tab:ctg_results_ws}}
\end{itemize}



