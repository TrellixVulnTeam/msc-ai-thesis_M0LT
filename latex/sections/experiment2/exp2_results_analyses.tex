\newpage
\section{Controlled Dialogue Generation Results}
\label{sec:exp2_results}
\len{NB - If you choose to use confidence intervals instead of std's, be sure to revise this section so it still makes sense.}
% \len{Old text for the unprompted results. FROM HERE....}

% Table~\ref{tab:ctg_results_ws} reports the automated evaluation results of our controllable text generation models. It can be seen that uncontrolled GPT-2 baseline has a slight bias towards generating "young-sounding" language (57.5\% accuracy). Furthermore, it appears that perturbing GPT-2's output distribution with the 100 most common words across all ages results in a slight de-biasing of the generated text (54.1\% accuracy). Achieving detectable control seems possible, because all GPT-2-based models surpass both baselines in terms of accuracy, with the exception of both BoW-setups using the 100 most informative unigrams.

% Frequency-based BoW-models outperform those using the most informative unigrams, as illustrated by their higher average accuracy (66.75\% versus. 53.1\%), and lower average perplexity (27.48 versus 27.90). 
% % \len{Offer an explanation.} 
% Discriminator-based models achieve noticeably better accuracies, with an average improvement of 8.45\% over the best performing BoW-based models. However, discriminator-based models do show more signs of disfluency and repetitiveness compared to the BoW-models, as depicted by the worse perplexities and Dist-$n |_{n = 1,2,3}$ scores.

% % The accuracies of our uncontrolled DialoGPT baseline (78.1\%) and the 100MCW baseline (80.7\%), suggest that DialoGPT is heavily biased towards producing young-sounding language. This can be attributable to DialoGPT having been fine-tuned on Reddit threads, as the majority of Reddit users are between the ages 20 and 29~\footnote{\url{https://www.statista.com/statistics/1125159/reddit-us-app-users-age/}}
% % \len{Find a reference for this statement} 
% \citep{zhang2019dialogpt}. DialoGPT's strong propensity for generating younger sounding language makes it a less desirable choice for our human evaluation experiments, because it requires non-standard parameter settings to produce detectably older sounding text.

% Overall, the results show that, for most models, a plug-and-play approach to controlling generated dialogue responses to possess detectable age-specific linguistic features is achievable. The most promising models being either discriminator-based, or frequency-based bag-of-words models. Discriminator-based models achieve more detectable levels of control than their BoW-based counterparts, at the cost of perplexity and repetitiveness. This could be attributable to the more complex activation-space updates that are used by discriminator-models. Furthermore, GPT-2's preference to generate young-sounding language is severely less pronounced than that of DialoGPT, making it easier to control, given equal parameter settings.

% \len{...TO HERE}

The quantitative results of generating younger (19 to 29) and older (50 and over) sounding responses to neutral prompts are reported in Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model}, respectively. The metrics per row averaged over $N = 30 \cdot 9 = 270$ samples (See Section \ref{subsec:experimental_details_generation}). In these tables, the underlying language model being used in a setup (i.e., a row in a table) is indicated by the prefixes G- for GPT-2 and D- for DialoGPT. Additionally, both tables report the results of the unperturbed GPT-2 and DialoGPT baselines (labeled G-baseline and D-baseline, respectively), and those of the 100 most common age-independent bag-of-words setups for both GPT-2 and DialoGPT (labeled G-100MCW and D-100MCW, respectively). The accuracies for these two setups (i.e., baseline and 100 most common words) are omitted because they do not aim to generate responses that resemble any target age group. Moreover, two bag-of-words (BoW) setups are reported per underlying language model (GPT-2 or DialoGPT): the frequency-based BoW setup (indicated by the suffix, -BoW$_{FB}$), and the 100 most informative unigram setup (indicated by the suffix, -BoW$_{100MIU}$). Detailed descriptions of how and why these aforementioned wordlists are constructed are provided in Section \ref{subsec:att_model_dev}. Finally, the discriminator-based setups are indicated by the suffix, -Discrim. The aforementioned reporting conventions also hold for the tables containing the results of response generation to younger and older sounding prompts (i.e., Tables \ref{tab:ctg_results_ws_young_prompt_young_model}, \ref{tab:ctg_results_ws_young_prompt_old_model}, \ref{tab:ctg_results_ws_old_prompt_young_model}, and \ref{tab:ctg_results_ws_old_prompt_old_model}). The results of those experiments are discussed in Section \ref{subsec:ctg_anal_prompt_class}.

As one would expect, Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model} show that the GPT-2 baseline consistently scores among the best on perplexity (best perplexity compared to young generation models, and second best compared to old-generation models) and diversity of generation (the Dist-$n |_{n = 1,2,3}$ scores are almost consistently in the upper registers). Similarly, the unperturbed DialoGPT baseline also scores best in terms of perplexity, when compared to other DialoGPT-based setups. This means that the responses generated by GPT-2 and DialoGPT are found to be among the least perplexing to GPT-1. This is unsurprising, as GPT-2, and thereby also DialoGPT, are pre-trained in similar fashion to GPT-1 \citep{radford2018improving, radford2019language, zhang2019dialogpt}. Additionally, the target probabilities ($\bar{P}_Y = 0.62$ in Table~\ref{tab:ctg_results_ws_neutral_prompt_young_models}, and $\bar{P}_O = 0.38$ in Table~\ref{tab:ctg_results_ws_neutral_prompt_old_model}) indicate that the GPT-2 baseline is biased towards generating young language. That is, given a neutral prompt, GPT-2 is inclined to produce responses that are likely to contain features learned to be young by BERT$_{FT}$. This could be attributable to GPT-2 being pre-trained on WebText, a corpus of high-quality documents scraped from web pages, which could be over-represented by millennials~\footnote{\url{https://www.statista.com/statistics/272365/age-distribution-of-internet-users-worldwide/}}. Moreover, DialoGPT has very strong bias towards generating younger sounding responses, given a neutral prompt (0.76 average probability to contain detectable young features). This is most likely due to DialoGPT having been fine-tuned on Reddit threads \citep{zhang2019dialogpt}, as the majority of Reddit users are between the ages 20 and 29~\footnote{\url{https://www.statista.com/statistics/1125159/reddit-us-app-users-age/}}.

The G-BoW$_{100MCW}$ setup performs on par with baseline w.r.t. perplexity and diversity (second best perplexity, second best Dist-2, best Dist-3), when compared to the young generation models. A similar pattern can be seen for old models, where G-100MCW has the second best Dist-2, and best Dist-3 scores. Additionally, the target probabilities seem virtually unaffected by the 100MCW setups, suggesting that perturbing GPT-2's output with an age-agnostic bag-of-words of the most frequently used words in the dialogue dataset does not noticeably shift the writing style towards that of the younger or older age group. This is to be expected, as such a wordlist should be an unbiased representation of the dataset's language style, given similar sample sizes per class.

BoW-based models seem to generate responses that are slightly more likely to contain features of the target age (young GPT2-BoW$_{FB}$ models result in 0.06 target probability improvement over the baseline, and old GPT2-BoW$_{FB}$ model in 0.04 target probability improvement over baseline). However, these differences are could also be due to randomness. For the 50-plus style GPT-2-based response models, the BoW$_{100MIU}$ setup does not manage to generate older sounding language than the baseline. Furthermore, BoW-based models seem to barely impact the syntactical structure of generated responses, because changes are made at the token-level \cite{dathathri2019plug}. This is confirmed by the barely altered perplexity and Dist-scores. \len{But this will be studied in more detail in the qualitative analyses.} 

The GPT-2 discriminator-based old-style model manages to generate responses that more convincingly resemble the style of the target group (0.38 $\bar{P}_O$ improvement over the GPT-2 baseline). However, this comes at the cost of perplexity ($+19.65$ compared to baseline) and diversity (much lower dist-scores and higher corresponding standard deviations). By contrast, the GPT-2 discriminator-based young setup does not generate convincingly more young-sounding responses (only 0.04 average target probability improvement over baseline), despite having noticeably worse and less precise perplexity ($4.59$ increase over baseline) and diversity.

The unperturbed DialoGPT baseline produces more perplexing and less diverse text than GPT-2 according to GPT-1 perplexity and the Dist-$n |_{n = 1,2,3}$ scores. The higher perplexity is to be expected, as DialoGPT pre-training and fine-tuning method deviates more from GPT-1's than GPT-2, making it likely to produce more unexpected sentences \citep{zhang2019dialogpt}. When using DialoGPT as an underlying language model, the BoW-based models (including 100MCW) seem to reinforce young-bias for DialoGPT, regardless of age (i.e., for all young-targeted BoW-based models $\bar{P}_Y$ goes up, and $\bar{P}_O$ goes down for all old-targeted BoW-based models). And the discriminator-based DialoGPT models, are superior w.r.t. target probability (+0.10 $\bar{P}_Y$ difference compared to DialoGPT baseline for young-models, and +0.34 $\bar{P}_O$ compared to baseline for old-models). However, this again comes at the cost of much higher and more volatile perplexity.

Overall, according to these results it seems to be possible to control dialogue responses for a certain age group. The used underlying language models are biased (in varying degrees) towards generating younger-sounding language. In these tested PPLM-setups, there seems to be a tradeoff between increased control and decreasing perplexity and diversity of generated language. Furthermore, the BoW-based models achieve less detectable levels of control, but preserve the fluency and diversity of generated text. In other words, the discriminator-based models make more invasive changes to the unperturbed sentences, which can result in less fluent and more repetitive text. However, they do produce more detectably age-appropriate passages.

\textbf{Initial observations and interpretations neutrally prompted ctg results (Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model})}
\begin{itemize}
    \item Style of prompt heavily influences control of response. Also confirmed by Tables \ref{tab:ctg_results_ws_young_prompt_young_model}, \ref{tab:ctg_results_ws_young_prompt_old_model}, \ref{tab:ctg_results_ws_old_prompt_young_model}, and \ref{tab:ctg_results_ws_old_prompt_old_model}. Remind reader of ctg formula $p(\textbf{x} | a, \texttt{prompt})$.
    % \item GPT-2 baseline scores best w.r.t. fluency (lowest perplexity) and diversity of generation (highest Dist-2, second highest Dist-3).
    % \item GPT-2 + BoW$_{100MCW}$ on par with baseline w.r.t. fluency and diversity (second best perplexity, second best Dist-2, best Dist-3).
    % \item Very similar pattern for old models: baseline second best perplexity and Dist-3, best Dist-1 and Dist-2; 100MCW second best Dist-2, and best Dist-3.
    % \item Baseline is moderately biased towards generating young language. I.e., GPT-2 is inclined to produce responses, given a neutral prompt, that are likely to contain features learned to be young by BERT$_{FT}$.
    % \item BoW-based models seem to result in language that is slightly more likely to contain features of target age (Young bow-models result in 0.06 probability increase wrt baseline, and old-bow-fb model in 0.04 increase wrt baseline). Differences are also possibly due to randomness.
    % \item BoW-based models barely impact the syntactical structure of generated sequences, i.e., changes made at lexical-level. This is shown by the barely altered perplexity (sometimes improved , G-B$_{100MIU,O}$ 0.25 lower average perplexity than baseline), and Dist-scores. \len{But this will be studied in more detail in the qualitative analyses.} 
    % \item BoW-fb slightly superior to BoW-100MIU -> consistently higher accuracy for BERT$_{FT}$.
    % \item For Old-models, BoW still doesn't fully overcome young-bias to result in convincingly old language.
    % \item Discriminator-based old model manages to generate language that is more convincingly similar to target age (0.38 old prob improvement). This comes at the cost of fluency and diversity (much higher perplexity and std wrt baseline), noticeable lower dist-scores and higher std's.
    % \item GPT2-disc young not convincingly more young sounding response. Noticeably worse and less precise fluency and diversity though. Only 0.04 average probability improvement.
    % \item DialoGPT has very strong young-bias, given a neutral prompt (0.76 average probability to contain detectable young features). --> Most likely due to DialoGPT being GPT-2, fine-tuned on Reddit Thread data \citep{zhang2019dialogpt} and include reddit age dist reference.
    % \item DialoGPT baseline produces less fluent and less diverse text than GPT-2 according to GPT-1 perplexity.
    % \item BoW-based models (including 100MCW) seem to reinforce young-bias for DialoGPT, regardless of age (young prob goes up for both BoW, and old prob goes down for both BoW-models).
    % \item Discriminator based models, again, outperform significantly w.r.t. target probability (0.10 prob difference wrt baseline for young, and 0.34 prob difference wrt baseline for old). Again, at the cost of on average worse and more volatile perplexity.
    \item Janie's suggestion: it could be that there are some detectable young-sounding tokens that make BoW-based young control easier, and that old-sounding features are more salient at the structural/syntactical level and harder to control for.
    % \item Overall, it seems to be possible to control dialogue responses for a certain age group. The used base language models are biased towards generating young-sounding language. In the current PPLM-setup, there seems to be a tradeoff between increased control and decreasing fluency and diversity. BoW-based models achieve less detectable levels of control, but preserve the fluency and diversity of generated text. Discriminator based models make more invasive changes to the unperturbed sentences, which can result in less fluent and more repetitive text. However, they do produce more detectably age-appropriate passages.
    \item \len{TODO - Examine the nonsensical generated sequences that are correctly labeled as old or young. What patterns do you see? It could be that BERT$_{FT}$ is picking up non-language patterns that give away age.}
\end{itemize}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & \textbf{27.50} (6.58) & 0.87 (0.09) & \textbf{0.94} (0.04) & \textcolor{blue}{0.90} (0.06) & 0.62 (0.42) & -\\
    G-100MCW & \textcolor{blue}{27.56} (6.60) & 0.86 (0.10) & \textcolor{blue}{0.93} (0.04) & \textbf{0.90} (0.05) & 0.63 (0.42) & -\\
    \midrule
    G-BoW$_{FB}$ & 27.91 (7.18) & 0.87 (0.10) & 0.93 (0.05) & \textcolor{blue}{0.90} (0.06) & 0.69 (0.41) & 70.4\%\\
    G-BoW$_{100MIU}$ & 28.37 (7.31) & 0.87 (0.09) & \textcolor{blue}{0.93} (0.04) & \textcolor{blue}{0.90} (0.06) & 0.67 (0.41) & 67.4\%\\
    \midrule
    G-Discrim & 32.09 (18.98) & 0.77 (0.20) & 0.86 (0.13) & 0.84 (0.15) & 0.66 (0.43) & 67.8\%\\
    \midrule
    \midrule
    D-baseline & 37.52 (12.06) & 0.86 (0.13) & 0.90 (0.08) & 0.85 (0.10) & 0.76 (0.37) & -\\
    D-100MCW & 37.80 (10.89) & 0.85 (0.14) & 0.89 (0.10) & 0.85 (0.10) & 0.82 (0.33) & -\\
    \midrule
    D-BoW$_{FB}$ & 38.53 (12.64) & 0.87 (0.12) & 0.90 (0.08) & 0.86 (0.10) & 0.82 (0.33) & 83.0\%\\
    D-BoW$_{100MIU}$ & 38.67 (11.70) & \textcolor{blue}{0.88} (0.11) & 0.91 (0.07) & 0.86 (0.10) & \textbf{0.87} (0.28) & \textcolor{blue}{88.5\%}\\
    \midrule
    D-Discrim & 42.01 (16.94) & \textbf{0.90} (0.12) & 0.86 (0.14) & 0.77 (0.22) & \textcolor{blue}{0.86} (0.29) & \textbf{85.9\%}\\
    \bottomrule
    \end{tabular}
    \caption{\len{Neutral prompt - Young model} Results of age-controlled language generation. ppl. is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. $\bar{P}(Young)$ is the sample's average probability to contain features learned to be young by BERT$_{FT}$. Acc. is BERT$_{FT}$'s accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_neutral_prompt_young_models}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c }
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & \textcolor{blue}{27.50} ($\pm$6.58) & \textbf{0.87} ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.38 ($\pm$0.42) & -\\
    G-100MCW & 27.56 ($\pm$6.60) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.05) & 0.37 ($\pm$0.42) & -\\
    \midrule
    G-BoW$_{FB}$ & 27.58 ($\pm$7.07) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.42 ($\pm$0.42) & 43.0\%\\
    G-BoW$_{100MIU}$ & \textbf{27.25} ($\pm$6.15) & \textbf{0.87} ($\pm$0.09) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.38 ($\pm$0.42) & 37.4\%\\
    \midrule
    G-Discrim & 47.15 ($\pm$47.56) & 0.73 ($\pm$0.24) & 0.75 ($\pm$0.28) & 0.75 ($\pm$0.27) & \textbf{0.76} ($\pm$0.36) & \textbf{74.3\%}\\
    \midrule
    \midrule
    D-baseline & 37.52 ($\pm$12.06) & 0.86 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.85 ($\pm$0.10) & 0.24 ($\pm$0.37) & -\\
    D-100MCW & 37.80 ($\pm$10.89) & 0.85 ($\pm$0.14) & 0.89 ($\pm$0.10) & 0.85 ($\pm$0.10) & 0.18 ($\pm$0.33) & -\\
    \midrule
    D-BoW$_{FB}$ & 37.85 ($\pm$11.17) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.22 ($\pm$0.35) & 21.5\%\\
    D-BoW$_{100MIU}$ & 37.91 ($\pm$12.27) & \textcolor{blue}{0.87} ($\pm$0.11) & 0.90 ($\pm$0.07) & 0.85 ($\pm$0.10) & 0.22 ($\pm$0.34) & 21.9\%\\
    \midrule
    D-Discrim & 41.17 ($\pm$20.72) & 0.87 ($\pm$0.12) & 0.89 ($\pm$0.13) & 0.83 ($\pm$0.16) & \textcolor{blue}{0.57} ($\pm$0.41) & \textcolor{blue}{56.7\%}\\
    \bottomrule
    \end{tabular}
    \caption{\len{Neutral prompt - Old model} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. $\boldsymbol{\bar{P}_Y}$ and $\boldsymbol{\bar{P}_O}$ are the respective average young and old probabilities assigned by the best BERT$_{FT}$. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_neutral_prompt_old_model}
\end{table*}


% TABLE 5.6 WITH YOUNG AND OLD PROB. JUST IN CASE %
% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l c c c c c c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Young prob.} & \textbf{Old prob.} & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & - & - & $\uparrow$ better\\
%     \midrule
%     \midrule
%     Baseline & 27.45 ($\pm$7.27) & 0.90 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.52 ($\pm$0.32) & 0.48 ($\pm$0.32) & 57.5\%\\
%     \midrule
%     B$_{100MCW}$ & 26.68 ($\pm$8.77) & 0.89 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.50 ($\pm$0.29) & 0.50 ($\pm$0.29) & 51.7\%\\
%     B$_{Y, FB}$ & 27.11 ($\pm$7.45) & \textbf{0.91} ($\pm$0.09) & \textbf{0.92} ($\pm$0.04) & \textbf{0.87} ($\pm$0.09) & 0.60 ($\pm$0.29) & 0.40 ($\pm$0.29) & 68.3\%\\
%     B$_{O, FB}$ & 25.99 ($\pm$6.41) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 0.35 ($\pm$0.31) & 0.65 ($\pm$0.31) & 62.5\%\\
%     B$_{Y, 100MIU}$ & 28.48 ($\pm$11.96) & 0.88 ($\pm$0.12) & 0.91 ($\pm$0.06) & 0.86 ($\pm$0.10) & 0.62 ($\pm$0.31) & 0.38 ($\pm$0.31) &69.2\%\\
%     B$_{O, 100MIU}$ & \textbf{25.57} ($\pm$7.44) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & \textbf{0.87} ($\pm$0.09) & 0.38 ($\pm$0.31) & 0.62 ($\pm$0.31) & 58.3\%\\
%     \midrule
%     D$_{Y, GPT2}$ & 33.02 ($\pm$12.24) & 0.85 ($\pm$0.16) & 0.89 ($\pm$0.07) & 0.83 ($\pm$0.12) & 0.70 ($\pm$0.31) & 0.30 ($\pm$0.31) & 73.9\%\\
%     D$_{O, GPT2}$ & 32.86 ($\pm$18.08) & 0.80 ($\pm$0.21) & 0.84 ($\pm$0.13) & 0.79 ($\pm$0.19) & 0.37 ($\pm$0.29) & 0.63 ($\pm$0.29) & 63.3\%\\
%     D$_{Y, GPT2*}$ & 30.98 ($\pm$13.95) & 0.86 ($\pm$0.15) & 0.90 ($\pm$0.06) & 0.84 ($\pm$0.11) & \textcolor{blue}{0.76} ($\pm$0.29) & 0.29 ($\pm$0.29) & 80.0\%\\
%     D$_{O, GPT2*}$ & 34.81 ($\pm$26.76) & 0.84 ($\pm$0.17) & 0.85 ($\pm$0.13) & 0.77 ($\pm$0.23) & 0.31 ($\pm$0.25) & \textcolor{red}{0.69} ($\pm$0.25) & 75.8\%\\
%     \bottomrule
%     \end{tabular}
%     \caption{Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Young and old accuracy are the assigned probabilities of belonging to the young or old age categories.}
%     \label{tab:ctg_results}
% \end{table*}


%%% THIS ISN'T RELEVANT ANYMORE --CONSIDER DELETING IT %%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Notes on Table \ref{tab:ctg_results}}
% \begin{itemize}
%     \item These are initial results.
%     \item All metrics are averaged over 120 samples: 30 samples per group of sequence lengths 8, 16, 32, and 64.
%     \item Young and old accuracy (last two columns) denote the probability of belonging to the young or old age-groups assigned by the best performing BERT-based classifier.
%     \item I use the same parameter settings as Table 6 of \cite{dathathri2019plug} to make the results comparable, i.e.:
%     \begin{itemize}
%         \item Step-size 0.02. Step-size is $\alpha$ in Equation \ref{eq:H_update_rule}.
%         \item Temperature 1.0.
%         \item Number of update iterations: 3.
%         \item $\gamma$ 1.5.
%         \item GM-scale 0.9.
%         \item KL-scale 0.01.
%     \end{itemize}
%     \item The current baseline is uncontrolled/unperturbed GPT-2.
%     \item There are four settings for BoW-based control:
%     \begin{itemize}
%         \item Young frequency-based wordlist.
%         \item Old frequency-based wordlist.
%         \item Young + most informative unigrams as wordlist.
%         \item Old + most informative unigrams as wordlist.
%     \end{itemize}
%     \item Initial observations:
%     \begin{itemize}
%         \item Fractions of distinct uni-,bi, and trigrams do not change.
%         \item Perplexity seems to improve when controlling generation for each age-group, which isn't necessarily what one would expect.
%         \item The baseline starts off with a higher average probability of belonging to the young age group
%         \item Controlling for young-language does result in a slightly greater assigned probability of belonging to young age-bracket.
%         \item Controlling for old-language results in a doubling of the assigned probability of belonging to the old age-bracket.
%     \end{itemize}
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
    \item \len{Include examples of same original sequence being perturbed differently at the unigram-level between corresponding young and old BoW-based CTG. E.g., \textit{I think you're a nice person (old)} vs. \textit{I think you're a nice guy (young)}}
\end{itemize}

% \subsubsection{Frequency-based wordlist generation}

% Steps taken to create age-specific wordlists (full imbalanced BNC used):
% \begin{itemize}
%     \item Remove all stopwords. List of stopwords from NLTK's English stopword list. \textbf{TODO: does this make sense? What if differences in use of stopwords are strong indicators of an age-group's speech?}
%     \item Order all unique words by frequency per age-group.
%     \item For both lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
%     \item From both sets of words, remove the words that are in the \textit{union} (i.e., the overlapping set) of the young and old sets.
%     \item For both sets, order the words by frequency.
%     \item For both remaining lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
%     \item \textbf{TODO:} remove curse-words?
%     \item Resulting wordlist lengths:
%         \begin{itemize}
%             \item Young (19-29): 90 words
%             \item Old (50 plus): 225 words
%         \end{itemize}
% \end{itemize}

% \subsubsection{Most informative unigrams as wordlists}

% \subsection{Discriminator-based control}




% \textbf{Notes on the experimental details of Table \ref{tab:ctg_results_ws}:}

% \begin{itemize}
%     \item All sequences are generated unconditionally. I.e., from $\texttt{|<endoftext>|}$ token.
%     \item All results are averaged over 240 samples.
%     \item 
% \end{itemize}






\section{Controlled Dialogue Generation Analyses}
\label{sec:exp2_analyses}

By means of quantitative and qualitative analyses, we seek to study which relationships affect the grammatical quality and attribute relevance of the generated responses. The discriminator-based setups, and the BoW-models with the highest average target probabilities (i.e., BoW$_{FB}$ for GPT-2, and BoW$_{100MIU}$ for DialoGPT) are considered for the analyses. In the following sections, we presents a series of analyses about the relationship between perplexity and target probability (Section \ref{subsec:ctg_anal_ppl_target_prob}), the effects of response length on generation quality (Section \ref{subsec:ctg_anal_response_length}), the impact of the prompt's style on generation style and quality (Section \ref{subsec:ctg_anal_prompt_class}), and qualitatively observable patterns in generated samples \ref{subsec:ctg_anal_qualitative}.



\len{TODO - Add examples of generated sequences along with their model's configurations, age-group, etc. Similar to dialogue snippets earlier.}

\subsection{The Relationship between Perplexity and Target Probability \len{Think of better title}}
\label{subsec:ctg_anal_ppl_target_prob}


% \len{NB - THE FOLLOWING PARAGRAPH IS ABOUT THE (DEPRECATED) UNPROMPTED SETUP RESULT. FROM HERE...}
% Figure~\ref{fig:ctg_lineplot_fluency_vs_control} attempts to depict the relationship between fluency and control, as measured by perplexity and BERT's classification accuracy, respectively. On the y-axis, ``mean accuracy'' refers to the average fraction of generated sequences, controlled for young or old language, that are correctly labeled as such by Experiment 1's best BERT classifier. The bars around the averages in Figure~\ref{fig:ctg_lineplot_fluency_vs_control} are 90\% confidence intervals.
% % It can be seen as a proxy for control, because it indicates how resemblant of an age group's vernacular a generated text is deemed to be. 
% % Perplexity, measured by a different language model (GPT-1 \citep{radford2018improving}), is a measure of a language model's uncertainty when posed with the task of predicting a succession of words. Assuming a language model to be a reliable representation of relationships within an actual language, low perplexity can serve as a rough proxy for fluency of a text. However, a major caveat of perplexity is that it only measures uncertainty w.r.t. one language model, making it less generalizable. To slightly reduce this effect, we choose to evaluate perplexity with respect to a different language model than the one used for generation.
% % \len{Disclose the confidence interval level}
% It appears that increasing perplexity is slightly negatively correlated with accuracy. It is also clear from Figure~\ref{fig:ctg_lineplot_fluency_vs_control} that uncertainty about prediction strongly increases for greater perplexity. These two observations indicate that sentences deemed less coherent by GPT-1 tend to be harder to classify by BERT$_{FT}$ with certainty. BERT$_{FT}$ is pre-trained and fine-tuned to pick up syntactic features from dialogue that can indicate a speaker's age. It is therefore plausible that structural deviations from proper syntax (i.e., high perplexity) can obfuscate the age-related linguistic signal BERT$_{FT}$ leverages. Finally, it seems that, on average, the discriminator-based models are more capable of producing correctly classifiable high-perplexity sentences. \len{Why would that be?} However, none of the differences between the average accuracies are statistically significant at the 10\% level, so this conclusion should be taken tentatively.
% \len{...TO HERE.}

\len{\textbf{TODO!!!: Re-write this paragraph s.t. it fits with the new plots.}}
\begin{itemize}
    \item Low, medium, and high perplexity are found using the terciles (i.e., the two points that divide the distribution of perplexities into 3 parts, each containing a third of the population.
    \item low perplexity: $\text{ppl}\leq27.52$
    \item medium perplexity: $27.52 < \text{ppl} \leq 35.63$
    \item high perplexity: $>35.63$
\end{itemize}

\len{TODO - Refine the following text. Especially the last part:}
Figures~\ref{fig:barplot_ppl_target_prob_np_gpt2} and \ref{fig:barplot_ppl_target_prob_np_dgpt} show bar charts depicting the relationship between the average target probability ($y$-axes) and perplexity ($x$-axes) assigned to the samples generated by various PPLM-model setups. The error bars around the average target probabilities are 95\% confidence intervals. Based on the distributions of perplexity among generated samples and the necessity to have sufficiently large sub-sample sizes, perplexity is binned into three consecutive intervals, corresponding to low (0-25), medium (25-50), and high (50+) perplexity. 

The GPT2-based old models (Figures~\ref{subfig:barplot_ppl_target_prob_np_gpt2_disc_old} and \ref{subfig:barplot_ppl_target_prob_np_gpt2_bow_old}) show a clear pattern of increasing perplexity coinciding with higher and more precise assigned target probabilities. Responses with relatively high perplexities (50+) are (at a 5\% level) significantly more likely to contain features learned to be old by BERT$_{FT}$. High-perplexity responses of the GPT2-old models are also assigned probabilities with more precision, as indicated by the narrower confidence regions. For the GPT-2 Young models' responses (Figures~\ref{subfig:barplot_ppl_target_prob_np_gpt2_disc_young} and \ref{subfig:barplot_ppl_target_prob_np_gpt2_bow_young}) we observe a similar pattern of slight increase of average assigned target probability between low (0-25) and medium (25-50) perplexity. However, in both cases this is followed by a large drop in both average assigned target probability and precision for high-perplexity responses. It must be noted that there are no significant differences at the 5\% level between the average $\bar{P}_Y$, so conclusions about the relationship between perplexity and target probability should be taken tentatively.


DialoGPT's strong proclivity to generate younger sounding responses is noticeable in Figures~\ref{subfig:barplot_ppl_target_prob_np_dgpt_disc_young} and \ref{subfig:barplot_ppl_target_prob_np_dgpt_bow_young}, as depicted by the high average target probabilities and relatively narrow confidence intervals. That being said, there seem to be no clear patterns between perplexity and target probability in DialoGPT-based models. It could be that DialoGPT's strong bias makes it less reliable to draw conclusions about the effects of PPLM-control for age-related style, given default parameter settings.

BERT$_{FT}$ seems to have least certainty (i.e., low precision aka high variance) about low-perplexity responses in every case, except DialoGPT-BoW Young (Figure~\ref{subfig:barplot_ppl_target_prob_np_dgpt_bow_young}).

Both sets of graphs show that patterns in the relationship between target probability and perplexity seem to persist between different types of control (i.e., discriminator-based or BoW-based), when holding the age and underlying language model constant.

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_gpt2_disc_np_y.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_gpt2_disc_young}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_gpt2_disc_np_o.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_gpt2_disc_old}
     \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_gpt2_bow_fb_np_y.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_gpt2_bow_young}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_gpt2_bow_fb_np_o.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_gpt2_bow_old}
     \end{subfigure}
    \caption{Mean target probability ($x$-axes) assigned to GPT-2-based models' samples by BERT$_{FT}$ for increasing ranges of GPT-1 perplexity ($y$-axes). Error bars are 95\% confidence intervals.}
    \label{fig:barplot_ppl_target_prob_np_gpt2}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_dgpt_disc_np_y.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_dgpt_disc_young}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_dgpt_disc_np_o.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_dgpt_disc_old}
     \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_dgpt_bow_miu_np_y.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_dgpt_bow_young}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/fluency_control/barplot_ppl_target_prob_dgpt_bow_miu_np_o.png}
        \caption{}
        \label{subfig:barplot_ppl_target_prob_np_dgpt_bow_old}
     \end{subfigure}
    \caption{Mean target probability ($y$-axes) assigned to DialoGPT-based models' samples by BERT$_{FT}$ for increasing ranges of GPT-1 perplexity ($x$-axes). Error bars are 95\% confidence intervals.}
    \label{fig:barplot_ppl_target_prob_np_dgpt}
\end{figure}

\len{\textbf{Observed patterns in perplexity target-prob plots}}:

\begin{itemize}
    % \item For GPT2-based Old models (both discrim (Figure~\ref{subfig:barplot_ppl_target_prob_np_gpt2_disc_old}) and bow (Figure~\ref{subfig:barplot_ppl_target_prob_np_gpt2_bow_old})), there is a very clear pattern of increasing perplexity coinciding with higher assigned target probabilities. Responses with relatively high perplexities (50+) are significantly more likely to contain features learned to be old by BERT$_{FT}$. This suggests there could be a tradeoff between increased levels of attribute relevance (i.e., control) and fluency (i.e., lower perplexity). High-perplexity responses are of the GPT2-old models are also assigned probabilities with more precision (i.e., smaller confidence region).
    % \item For the GPT-2 Young models' responses we observe a pattern of slight increase of average assigned target probability between low (0-25) and medium (25-50) perplexity, followed by a extreme decrease of assigned target probability and associated precision for high-perplexity responses.
    % \item DialoGPT's young-bias is noticeable in Figures~\ref{subfig:barplot_ppl_target_prob_np_dgpt_disc_young} and \ref{subfig:barplot_ppl_target_prob_np_dgpt_bow_young}: all high average target probabilities with relatively high certainty.
    % \item No clear pattern of tradeoff between perplexity and target probability in DialoGPT-based models.
    % \item BERT$_{FT}$ seems to have least certainty (i.e., low precision aka high variance) about low-perplexity responses in every case, except DialoGPT-BoW Young (Figure~\ref{subfig:barplot_ppl_target_prob_np_dgpt_bow_young}).
    \item \len{Using the term fluency as a generalization of perplexity can be misleading. Think of another word, just use perplexity, or provide a disclaimer.}
\end{itemize}


\subsection{The Effects of Generated Response Length \len{Think of better title}}
\label{subsec:ctg_anal_response_length}

The number of tokens in a generated response coincides with noticeable differences in our automated evaluation metrics. It is therefore important to get a clearer picture of how the various measures for fluency and control change for different sequence lengths. Moreover, properly understanding these relationship can inform developers of adaptive dialogue systems about preserving output quality and adaptation of responses of arbitrary lengths.

Response length (on the $x$-axes) is plotted against various evaluation metrics in Figures~\ref{fig:lineplots_length_acc_np_gpt2_dgpt} (average BERT$_{FT}$ accuracy), \ref{fig:lineplots_length_ppl_np_gpt2_dgpt} (GPT-1 perplexity), \ref{fig:lineplots_length_dist1_np_gpt2_dgpt} (normalized number of distinct unigrams), \ref{fig:lineplots_length_dist2_np_gpt2_dgpt} (normalized number of distinct bigrams), and \ref{fig:lineplots_length_dist3_np_gpt2_dgpt} (normalized number of distinct trigrams).

% \begin{itemize}
%     \item \textit{Main question: how is generated sequence length related to fluency and control?}
    
%     \item \textit{Study the relationship between generated sequence length (measured in number of tokens) and automated evaluation metrics (i.e., perplexity, dist-n, and accuracy).}
    
%     \item \textit{For every metric and for (all?) models, plot sequence length on the x-axis, and the average metric with confidence intervals on the y-axis.}
    
%     \item \textit{Which patterns do you observe?} 
% \end{itemize}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
\includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_acc_best_gpt2_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_acc_np_gpt2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_acc_best_dgpt_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_acc_np_dgpt}
     \end{subfigure}
        \caption{Mean BERT$_{FT}$ accuracy. GPT-2-based models (left), DialoGPT-based models (right). Translucent error bands represent 95\% confidence intervals. Plots best viewed in color.}
        \label{fig:lineplots_length_acc_np_gpt2_dgpt}
\end{figure}

% \len{\textbf{Patterns in Figure~\ref{fig:lineplots_length_acc_np_gpt2_dgpt}}}:
% \begin{itemize}
%     % \item No clear trends.
%     % \item BoW-based Old (GPT-2 and DialoGPT) has significantly lower accuracy at almost every length bracket.
%     % \item Shortest responses generated by GPT-2-based models appear to be most challenging to classify for BERT$_{FT}$.
% \end{itemize}

Figure~\ref{subfig:lineplot_length_acc_np_gpt2} shows a slight upward trend in average accuracy with greater uncertainty for increasing response length for all GPT-2-based models, except for the BoW-based old generation model. That is, longer sequences are, on average, slightly easier to classify, though with less precision. This is probably due to the fact that longer sentences contains more information to base predictions on. 
By contrast, the DialoGPT-based models in Figure~\ref{fig:lineplots_length_acc_np_gpt2_dgpt} do not seem to show a clear general trend that mean accuracy follows for increasing response length. However, it does seem that DialoGPT's strong bias towards generating younger sounding responses causes output from DialoGPT-based young generation models to be much easier to classify than that from the old generation models. Overall, it can be seen that the BoW-based old models (GPT-2 and DialoGPT) are significantly more challenging to classify at almost every length bracket.

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
\includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_ppl_best_gpt2_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_ppl_np_gpt2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_ppl_best_dgpt_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_ppl_np_dgpt}
     \end{subfigure}
        \caption{Perplexity. GPT-2-based models (left), DialoGPT-based models (right). Translucent error bands represent 95\% confidence intervals. Plots best viewed in color.}
        \label{fig:lineplots_length_ppl_np_gpt2_dgpt}
\end{figure}

% \len{\textbf{Patterns in Figure~\ref{fig:lineplots_length_ppl_np_gpt2_dgpt}}:}
% \begin{itemize}
%     % \item Perplexity decreases for increasing response length. Longer responses are deemed more plausible by GPT-1.
%     % \item GPT2-Discrim Old significantly worse perplexity at 5\% level for medium length responses. (is also best model setup w.r.t. target-prob improvement over baseline if i'm not mistaken).
% \end{itemize}

Figure~\ref{fig:lineplots_length_ppl_np_gpt2_dgpt} shows a clear downward trend for both sets of PPLM-setups. Irrespective of the underlying language model being used, longer responses are deemed less perplexing, with more certainty, by GPT-1 than shorter ones. It is worth emphasizing that the model with the highest target probability improvement over its relevant baseline, G-Discrim$_{Old}$, is found to produce significantly more perplexing responses than its GPT-2-based counterparts at most length brackets (See Figure~\ref{subfig:lineplot_length_ppl_np_gpt2}). This finding resonates with Figure~\ref{subfig:barplot_ppl_target_prob_np_gpt2_disc_old} and the idea, that especially for old-generation models, increased levels of attribute relevance coincide with worse perplexity.

However, it must be noted that the downward slope of perplexity for increasing response length could be attributable to the nature of calculating perplexity, rather than generation properties of the models. Namely, perplexity essentially averages the sum of the negative exponentiated probabilities $p(\texttt{word} | \texttt{context})$, for every word in a sentence. Because the context increases with every successive word, and larger contexts typically result in less uncertainty, shorter sequences are often given unfairly high perplexities.

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
\includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist1_best_gpt2_disc_bow__neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist1_np_gpt2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist1_best_dgpt_disc_bow__neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist1_np_dgpt}
     \end{subfigure}
        \caption{Dist-1. GPT-2-based models (left), DialoGPT-based models (right). Translucent error bands represent 95\% confidence intervals. Plots best viewed in color.}
        \label{fig:lineplots_length_dist1_np_gpt2_dgpt}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
\includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist2_best_gpt2_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist2_np_gpt2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist2_best_dgpt_disc_bow_neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist2_np_dgpt}
     \end{subfigure}
        \caption{Dist-2. GPT-2-based models (left), DialoGPT-based models (right). Translucent error bands represent 95\% confidence intervals. Plots best viewed in color.}
        \label{fig:lineplots_length_dist2_np_gpt2_dgpt}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
\includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist3_best_gpt2_disc_bow__neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist3_np_gpt2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/sequence_length/lineplot_len_dist3_best_dgpt_disc_bow__neutral_prompt_ci_95_errstyle_band.png}
        \caption{}
        \label{subfig:lineplot_length_dist3_np_dgpt}
     \end{subfigure}
        \caption{Dist-3. GPT-2-based models (left), DialoGPT-based models (right). Translucent error bands represent 95\% confidence intervals. Plots best viewed in color.}
        \label{fig:lineplots_length_dist3_np_gpt2_dgpt}
\end{figure}

% \len{\textbf{Patterns in Figures~\ref{fig:lineplots_length_dist1_np_gpt2_dgpt}, \ref{fig:lineplots_length_dist2_np_gpt2_dgpt}, and \ref{fig:lineplots_length_dist3_np_gpt2_dgpt}}:}

% \begin{itemize}
    % \item Figure~\ref{fig:lineplots_length_dist1_np_gpt2_dgpt} shows that diversity w.r.t. unigrams increases for longer responses. This is most likely just due to the fact that longer sentences have an \textit{a priori} higher probability of repeating words. E.g., stopwords like "the" and "of" are likely to appear multiple times in longer sentences. \len{Is this a valid suggestion?}
    % \item The same figure shows an interesting difference: GPT-2-BoW models generate significantly more diverse responses w.r.t. unigrams for almost every bracket of response length. This could be attributable to BoW-based control altering base-GPT2's generated sentences at the token-level, thus being more likely to preserve the unigram diversity of the unperturbed baseline (G-Baseline's Dist-1 is always in the upper register when looking at the columns in Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model}.
    % \item Figure~\ref{fig:lineplots_length_dist2_np_gpt2_dgpt} and shows that variety w.r.t. bigrams makes an initial upward jump between response length of 1-10 and 10-20. Dist-2 then follows a mild downward trend for both GPT2- and DialoGPT-based models. The GPT2-based models' trend is more pronounced and less volatile than DialoGPT's.
    % \item Figure \ref{subfig:lineplot_length_dist2_np_gpt2} shows the BoW-based GPT-2 models also produce significantly more diverse language w.r.t. bigrams for most response lengths.
    % \item Figure~\ref{fig:lineplots_length_dist3_np_gpt2_dgpt} shows similar patterns: initial upward jump in trigram diversity between shortest and second-to-shortest length brackets. GPT-2 models then show a slight downward trend from the 20-30 lengths onward. However, DialoGPT actually show a decreasing increase in trigram diversity for all models.
% \end{itemize}

Figure~\ref{fig:lineplots_length_dist1_np_gpt2_dgpt} shows that diversity w.r.t. unigrams decreases for longer responses. This is most likely due to the fact that longer sentences have an \textit{a priori} higher probability of containing repeated words. E.g., stopwords like "the" and "of" are likely to appear multiple times in longer sentences. The same figure shows that GPT-2-BoW models generate significantly more diverse responses w.r.t. unigrams for almost every bracket of response length. This could be attributable to BoW-based control altering base-GPT2's generated sentences at the token-level, thus being more likely to preserve the unigram diversity of the unperturbed baseline (the GPT-2 baseline's Dist-1 is always in the upper register in Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model}).

Figure~\ref{fig:lineplots_length_dist2_np_gpt2_dgpt} and shows that variety w.r.t. bigrams makes an initial upward jump between response length of 1-10 and 10-20. Dist-2 then follows a mild downward trend for both GPT2- and DialoGPT-based models However, detailed inspection of Figure~\ref{subfig:lineplot_length_dist1_np_gpt2} shows that only the discriminator-based setups have a negative slope, whereas the BoW-based setups follow a very slight upward trend. Thus, the BoW-based GPT-2 models produce significantly more diverse language w.r.t. bigrams for most response lengths, attributable to the same reason mentioned above.

Figure~\ref{fig:lineplots_length_dist3_np_gpt2_dgpt} shows similar patterns: an initial upward jump in trigram diversity between shortest and second-to-shortest length brackets. GPT-2 models then show a slight downward trend for the discriminator-based setups from the 20-30 lengths onward, while the BoW-based models become slightly more diverse w.r.t. trigrams.

Overall, it can again be seen that BoW-based models generate detectably more diverse responses (with greater precision), and remain to do so as response length increases. The decreasing diversity of discriminator-based generated responses further confirms that more invasive control during generated impedes textual variety.

% \textbf{Patterns in (obsolete) unprompted results \len{DELETE OR MOVE TO APPENDIX. FROM HERE....}:}
% \begin{itemize}
%     \item \len{NB: These are all patterns observed in the unprompted results. Once prompted results are in, we can interpret them as if length of dialogue response.}
%     \item Increasing sequence length is correlated with decreasing perplexity. Longer sentences are deemed more coherent by GPT-2
%     \item Increasing length seems very slighty positively correlated with average accuracy (and more uncertainty). I.e., longer sequences are, on average, easier to classify, though less precision.
%     \item BoW-based models significantly more distinct  w.r.t. unigrams than dirscrim-based.
%     \item Overall, repetitiveness seems to increase as generated sequences become longer.
%     \item The differences between young and old perplexity are smaller for BoW-based models than for Discrim-based. Same pattern holds for repetitiveness.
% \end{itemize}

% \len{NB: These are all patterns observed in the unprompted results. Once prompted results are in, we can interpret them as if length of dialogue response.}

% The number of tokens being generated in an utterance seems to coincide with noticeable differences in our automated evaluation metrics. It is therefore important to get a clearer picture of how the various measures for fluency and control change for varying sequence lengths. Properly understanding this relationship can inform developers of adaptive dialogue systems about preserving output quality for responses of arbitrary lengths.

% Figure~\ref{fig:ctg_lineplots_len_vs_metrics} presents plots of the relationships between generated sequence length (on the x-axes) and average perplexity (Figure~\ref{fig:ctg_lineplot_len_vs_ppl}), average BERT$_{FT}$ accuracy (Figure~\ref{fig:ctg_lineplot_len_vs_acc}), and average normalized number of distinct unigrams (Figure~\ref{fig:ctg_lineplot_len_vs_dist1}), bigrams (Figure~\ref{fig:ctg_lineplot_len_vs_dist2}), and trigrams (Figure~\ref{fig:ctg_lineplot_len_vs_dist3}).


% Starting with Figure~\ref{fig:ctg_lineplot_len_vs_ppl}, it appears that, for all models, increases in generated utterance length coincide with decreases in perplexity. This is most likely attributable to the nature of calculating perplexity than generation properties of the models. Namely, perplexity essentially averages the sum of the negative exponentiated probabilities $p(\texttt{word} | \texttt{context})$, for every word in a sentence. Because the context increases with every successive word, and larger contexts typically result in less uncertainty, shorter sequences are often given unfairly high perplexities.

% In Figure~\ref{fig:ctg_lineplot_len_vs_acc}, we can see that increasing length seems very slightly positively correlated with average accuracy (and more uncertainty). That is, longer sequences are, on average, slightly easier to classify, though with less precision. This is probably due to the fact that longer sentences contains more information to base predictions on.

% Focusing on repetitive use of unigrams, it appears that BoW-based models generate significantly more diverse utterances than discriminator-based models. 

% Repetitiveness w.r.t. bigrams and trigrams seems to increase as generated sequences become longer.

% Overall, the differences between young and old perplexity are smaller for BoW-based models than for Discriminator-based. Same pattern holds for repetitiveness.

% \len{...TO HERE}


% \begin{figure}[H]
%      \centering
%      \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist1_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
%         \caption{...}
%         \label{fig:ctg_lineplot_len_vs_dist1}
%      \end{subfigure}
%     %  \hfill
%      \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist2_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
%         \caption{...}
%         \label{fig:ctg_lineplot_len_vs_dist2}
%      \end{subfigure}
%         \caption{}
%         \label{}
%     % \hfill
%      \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist3_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
%         \caption{...}
%         \label{fig:ctg_lineplot_len_vs_dist3}
%      \end{subfigure}
%         \caption{}
%         \label{}
% \end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=.4\textwidth]{figures/exp2/lineplot_len_ppl_best_gpt2_disc_bow_ci_90_errstyle_bars.png}\quad
% \includegraphics[width=.4\textwidth]{figures/exp2/lineplot_len_acc_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
% % \includegraphics[width=.3\textwidth]{figures/exp2/lineplot_len_dist1_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}

% \medskip
% \includegraphics[width=.4\textwidth]{figures/exp2/lineplot_len_dist1_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}\quad
% \includegraphics[width=.4\textwidth]{figures/exp2/lineplot_len_dist2_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}\quad
% \includegraphics[width=.4\textwidth]{figures/exp2/lineplot_len_dist3_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}

% \caption{Generated sequence length against various automated metrics.}
% \label{fig:seq_len_vs_auto_metrics}
% \end{figure}

\subsection{The Effects of Prompt Class \len{Think of better title}}
\label{subsec:ctg_anal_prompt_class}

Recall that given a conditioning prompt $\texttt{prompt}$, a predefined style attribute $a$, and some controlled dialogue generation model parameterized by $\theta$, generating a stye-controlled piece of text $\textbf{x}$ entails modeling $p_{\theta}(\textbf{x} | a, \texttt{prompt})$. It is therefore reasonable to expect the output distribution of controlled generation model $p_{\theta}$ to depend (to some extent) on the content and style of the conditioning text, $\texttt{prompt}$. Indeed, the content and style of prompts are found to strongly influence the output of neural text generation models \citep{fan-etal-2018-hierarchical, lester2021power}. Thus, studying the effects of a prompt's age-style (i.e., whether a prompt is considered young, old, or neutral by BERT$_{FT}$) on the style and grammatical quality of PPLM-setups is of great importance, as it could inform developers of adaptive dialogue systems about mitigation of prompt-induced biases.

It is worth mentioning that the effects of prompt-style on PPLM-generation are not considered by \cite{dathathri2019plug} or \cite{madotto-etal-2020-plug}. Studying these effects is an important extension of their methods, as not quantitatively taking into account the effects prompt-style obfuscates the degree to which one can conclude whether attribute-relevance is the result of controlled generation or prompt-induced bias.
% This is actually a non-negligible fault in their research, because you can bias the style of generation by giving it a biased prompt. 

Figures~\ref{fig:catplot_prompt_class_target_prob} and \ref{fig:catplot_prompt_class_ppl} depict the average target probability and perplexity over responses generated by the baseline, best BoW-based model, and discriminator-based model, when prompted with a prompt of either young, neutral, or old style. More specifically, each bar represents a metric (target probability or perplexity) averaged over $N=270$ samples generated by a single model, when presented with five prompts of the same age-style. E.g., the blue bar in Figure~\ref{subfig:catplot_prompt_class_target_prob_gpt2_young} represents the average probability of samples generated by GPT-2 + frequency-based BoW to contain features learned to be young by BERT$_{FT}$, when the model was presented young-sounding prompts. The explicit numerical values of Figures~\ref{fig:catplot_prompt_class_target_prob} and \ref{fig:catplot_prompt_class_ppl} are found in Tables \ref{tab:ctg_results_ws_young_prompt_young_model}, \ref{tab:ctg_results_ws_young_prompt_old_model}, \ref{tab:ctg_results_ws_old_prompt_young_model}, and \ref{tab:ctg_results_ws_old_prompt_old_model}.

Ideally, the neutrally prompted baseline's assigned probability of generating age-specific responses should be around 0.50. Furthermore, a prompt should shift the target probability in the direction of the prompt class, e.g., a young prompt should shift a young-model's target prob upwards, and an old-model's target prob downwards. We know from previous results in Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model} that the language model baselines are (in varying degrees) biased towards generating younger sounding responses to neutral prompts. Nevertheless, we should expect the impact of prompt-style to persist, albeit to differing degrees based on the (dis)similarity in style between the prompt and response, and the type of attribute model being used (BoW or discriminator).


Figures~\ref{fig:catplot_prompt_class_target_prob} and \ref{fig:catplot_prompt_class_ppl}, show that the models' target probabilities indeed move accordingly with the prompts' styles. E.g., young-prompted young-model achieves highest young target prob, then neutral prompted, and then old prompted (Figures \ref{subfig:catplot_prompt_class_target_prob_gpt2_young} and \ref{subfig:catplot_prompt_class_target_prob_dgpt_young}). The same pattern holds the other way around: an old-prompted old-model has the highest (old) target probability, then neutrally prompted, and then the young- prompted ones (Figures \ref{subfig:catplot_prompt_class_target_prob_gpt2_old} and \ref{subfig:catplot_prompt_class_target_prob_dgpt_old}). In these last two sub-figures, we also clearly see that the discriminator-based old generation models achieve substantial target probability improvements over the baseline (and BoW-based models), for every style of prompt. By contrast, the young-generation models (Figures \ref{subfig:catplot_prompt_class_target_prob_gpt2_young} and \ref{subfig:catplot_prompt_class_target_prob_dgpt_young}) do not show the same pattern: discriminator-based models achieve similarly subtle improvements in target probability over their baselines as the BoW-based models do. Figure~\ref{subfig:catplot_prompt_class_target_prob_gpt2_young} even shows the discriminator-based models to perform worse than the baseline and BoW-based models.

So the class of the prompt strongly influences the style of the generated response.

Overall, Figures~\ref{fig:catplot_prompt_class_target_prob} and \ref{fig:catplot_prompt_class_ppl} and Tables \ref{tab:ctg_results_ws_young_prompt_young_model}, \ref{tab:ctg_results_ws_young_prompt_old_model}, \ref{tab:ctg_results_ws_old_prompt_young_model}, and \ref{tab:ctg_results_ws_old_prompt_old_model} show that the style of the prompt clearly nudges the assigned probability of containing age-related features in the direction of the prompt's style. Moreover, using strongly young-prompt results in heavily reinforced young-bias for GPT2 and DialoGPT. Similarly, using an older sounding prompt results in a slightly neutralized young-bias for both language models. Additionally, the discriminator-based old models (GPT-2 and DialoGPT) always yield the highest relative improvements over the baselines. However, they fail to do so when attempting to generate younger sounding responses, which could suggest that the stylistic features learned to be young and old by BERT$_{FT}$ lie at different syntactical levels, and are not equally challenging to control for. Finally, the aforementioned tables show that the effects of prompt-style are largely limited to the target probabilities. For perplexity and distinctiveness, the young- and old-prompted results show similar patterns to the neutral-prompt setting: BoW-based models achieve smaller increases in control, but maintain relatively desirable perplexity and diversity. Whereas, the discriminator-based models achieve higher levels of control, at the cost of worse perplexity and Dist-scores.

\len{\textbf{TODO - Add table with used prompts and their assigned target probabilities.}}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_target_prob_gpt2_young_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_target_prob_gpt2_young}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_target_prob_gpt2_old_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_target_prob_gpt2_old}
     \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_target_prob_dgpt_young_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_target_prob_dgpt_young}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_target_prob_dgpt_old_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_target_prob_dgpt_old}
     \end{subfigure}
    \caption{Target probability. The plots are best viewed in color. \len{TODO: (1) Change Target probability labels to Young or Old probability. The term target probability doesn't hold for baseline models.}}
    \label{fig:catplot_prompt_class_target_prob}
\end{figure}

% \len{\textbf{Patterns in Figure \ref{fig:catplot_prompt_class_target_prob}}:}
% \begin{itemize}
%     % \item The style of the prompt clearly nudges the assigned probability in the direction of the prompt's class.
%     % \item The discrim. old models (GPT-2 and DialoGPT) always yield the highest relative improvements over the baselines.
%     % \item Ideally, you would want (1) the neutrally prompted baseline to be around 0.50. (2) A young or old prompt should shift the target probability in the direction of the prompt class; i.e., a young prompt should shift a young-model's target prob upwards, and an old-model's target prob downwards. (3) ...
%     % \item The models target probabilities (baseline, bow, and discrim) move accordingly with the prompt's class: young prompted young-model has highest young target prob, then neutral prompted, and then old prompted. Same pattern goes the other way around: an old prompted old-model has the highest (old) target prob, then neutral prompted, and then young prompted.
%     % \item So the class of the prompt strongly influences the style of the generated response.
%     % \item \len{Make sure to emphasize that this is also something that hasn't been studied by \cite{dathathri2019plug} or \cite{madotto-etal-2020-plug}; i.e., they never study the effect of their prompt style or sentiment on the style/attribute relevance of generation. This is actually a non-negligible fault in their research, because you can bias the style of generation by giving it a biased prompt. Furthermore, not quantitatively taking into account the prompt style or effect obfuscates the degree to which one can conclude whether attribute relevance is the result of controlled generation or prompt-bias.}
% \end{itemize}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_ppl_gpt2_young_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_ppl_gpt2_young}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_ppl_gpt2_old_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_ppl_gpt2_old}
     \end{subfigure}
    \medskip
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_ppl_dgpt_young_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_ppl_dgpt_young}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/prompt_class/catplot_prompt_class_ppl_dgpt_old_models.png}
        \caption{}
        \label{subfig:catplot_prompt_class_ppl_dgpt_old}
     \end{subfigure}
    \caption{Perplexity. Mind the differences in scale between the $y$-axes. The plots are best viewed in color. \len{TODO - Move to Appendix}}
    \label{fig:catplot_prompt_class_ppl}
\end{figure}



% Initial observations and interpretations of Tables \ref{tab:ctg_results_ws_young_prompt_young_model}, \ref{tab:ctg_results_ws_young_prompt_old_model}, \ref{tab:ctg_results_ws_old_prompt_young_model}, \ref{tab:ctg_results_ws_old_prompt_old_model}.

% \begin{itemize}
%     % \item Using strongly young-prompt results in heavily reinforced young-bias for GPT2 and DialoGPT.
%     % \item Similarly, old-prompt results in slightly neutralized young-bias for both language models.
%     \item Neutrally prompted patterns persist...
%     \begin{itemize}
%         \item Trade-off between control and fluency + diversity.
%         \item BoW achieves smaller increases in control, but leaves fluency and dist intact.
%         \item Discrim higher levels of control increase wrt baseline, worse quality of text wrt fluency and diversity.
%     \end{itemize}
%     \item Overall, stylistic aspects of prompts heavily influence controllability of response.
% \end{itemize}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & \textcolor{blue}{28.05} ($\pm$6.12) & 0.85 ($\pm$0.13) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.80 ($\pm$0.33) & -\\
    G-100MCW & \textbf{27.71} ($\pm$6.20) & 0.85 ($\pm$0.12) & 0.91 ($\pm$0.09) & 0.88 ($\pm$0.09) & 0.75 ($\pm$0.37) & -\\
    \midrule
    G-B$_{FB, Y}$ & 28.81 ($\pm$7.09) & 0.86 ($\pm$0.12) & \textbf{0.92} ($\pm$0.08) & \textbf{0.89} ($\pm$0.08) & 0.82 ($\pm$0.32) & 83.3\%\\
    G-B$_{100MIU, Y}$ & 28.49 ($\pm$6.49) & 0.86 ($\pm$0.12) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.83 ($\pm$0.32) & 83.0\%\\
    \midrule
    G-D$_{Y}$ & 39.32 ($\pm$37.49) & 0.84 ($\pm$0.21) & 0.61 ($\pm$0.40) & 0.57 ($\pm$0.40) & 0.70 ($\pm$0.40) & 70.7\%\\
    \midrule
    \midrule
    D-baseline & 36.69 ($\pm$9.11) & 0.87 ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.06) & 0.87 ($\pm$0.08) & \textcolor{blue}{0.90} ($\pm$0.24) & -\\
    D-100MCW & 36.93 ($\pm$9.18) & 0.86 ($\pm$0.11) & \textcolor{blue}{0.91} ($\pm$0.06) & \textcolor{blue}{0.88} ($\pm$0.07) & 0.90 ($\pm$0.25) & -\\
    \midrule
    D-B$_{FB, Y}$ & 37.35 ($\pm$8.60) & \textcolor{blue}{0.88} ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.90 ($\pm$0.26) & \textcolor{blue}{90.0\%}\\
    D-B$_{100MIU, Y}$ & 37.87 ($\pm$8.32) & \textcolor{blue}{0.88} ($\pm$0.10) & 0.91 ($\pm$0.07) & 0.87 ($\pm$0.09) & \textbf{0.91} ($\pm$0.24) & \textbf{92.6\%}\\
    \midrule
    D-D$_{Y}$ & 39.22 ($\pm$14.96) & \textbf{0.89} ($\pm$0.12) & 0.86 ($\pm$0.19) & 0.79 ($\pm$0.23) & 0.89 ($\pm$0.25) & 91.1\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Young prompt - Young models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_young_prompt_young_model}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & \textcolor{blue}{28.05} ($\pm$6.12) & 0.85 ($\pm$0.13) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.20 ($\pm$0.33) & -\\
    G-100MCW & \textbf{27.71} ($\pm$6.20) & 0.85 ($\pm$0.12) & 0.91 ($\pm$0.09) & 0.88 ($\pm$0.09) & 0.25 ($\pm$0.37) & -\\
    \midrule
    G-B$_{FB, O}$ & 28.54 ($\pm$6.45) & 0.86 ($\pm$0.12) & \textbf{0.92} ($\pm$0.08) & \textbf{0.89} ($\pm$0.08) & 0.23 ($\pm$0.36) & 22.6\%\\
    G-B$_{100MIU, O}$ & 28.18 ($\pm$5.70) & 0.87 ($\pm$0.11) & \textbf{0.92} ($\pm$0.08) & \textcolor{blue}{0.89} ($\pm$0.09) & 0.21 ($\pm$0.34) & 21.5\%\\
    \midrule
    G-D$_{O}$ & 85.40 ($\pm$150.28) & 0.67 ($\pm$0.30) & 0.62 ($\pm$0.31) & 0.62 ($\pm$0.32) & \textbf{0.71} ($\pm$0.40) & \textbf{70.5\%}\\
    \midrule
    \midrule
    D-baseline & 36.69 ($\pm$9.11) & \textcolor{blue}{0.87} ($\pm$0.10) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.10 ($\pm$0.24) & -\\
    D-100MCW & 36.93 ($\pm$9.18) & 0.86 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.88 ($\pm$0.07) & 0.10 ($\pm$0.25) & -\\
    \midrule
    D-B$_{FB, O}$ & 37.25 ($\pm$9.45) & 0.87 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.12 ($\pm$0.29) & 11.1\%\\
    D-B$_{100MIU, O}$ & 37.04 ($\pm$8.78) & \textbf{0.88} ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.05) & 0.88 ($\pm$0.07) & 0.15 ($\pm$0.32) & 15.2\%\\
    \midrule
    D-D$_{O}$ & 38.46 ($\pm$14.91) & 0.82 ($\pm$0.15) & 0.87 ($\pm$0.15) & 0.83 ($\pm$0.17) & \textcolor{blue}{0.48} ($\pm$0.44) & \textcolor{blue}{47.4\%}\\
    \bottomrule
    \end{tabular}
    \caption{\len{Young prompt - Old models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_young_prompt_old_model}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & \textcolor{blue}{29.34} ($\pm$10.30) & 0.86 ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.60 ($\pm$0.43) & -\\
    G-100MCW & \textbf{29.14} ($\pm$10.11) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.60 ($\pm$0.44) & -\\
    \midrule
    G-B$_{Y, FB}$ & 29.61 ($\pm$10.28) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & \textbf{0.91} ($\pm$0.06) & 0.62 ($\pm$0.43) & 61.1\%\\
    G-B$_{Y, 100MIU}$ & 29.51 ($\pm$0.09) & \textcolor{blue}{0.87} ($\pm$0.09) & 0.93 ($\pm$0.05) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.68 ($\pm$0.42) & 68.5\%\\
    \midrule
    G-D$_{Y}$ & 32.34 ($\pm$19.88) & 0.77 ($\pm$0.20) & 0.84 ($\pm$0.19) & 0.80 ($\pm$0.23) & 0.65 ($\pm$0.43) & 65.4\%\\
    \midrule
    \midrule
    D-baseline & 38.18 ($\pm$12.03) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.72 ($\pm$0.38) & -\\
    D-100MCW & 37.73 ($\pm$11.88) & 0.85 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.73 ($\pm$0.39) & -\\
    \midrule
    D-B$_{Y, FB}$ & 38.24 ($\pm$11.53) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.10) & 0.81 ($\pm$0.34) & \textcolor{blue}{82.6\%}\\
    D-B$_{Y, 100MIU}$ & 38.66 ($\pm$11.57) & 0.85 ($\pm$0.12) & 0.90 ($\pm$0.07) & 0.86 ($\pm$0.09) & \textcolor{blue}{0.81} ($\pm$0.33) & 80.7\%\\
    \midrule
    D-D$_{Y}$ & 42.93 ($\pm$20.18) & \textbf{0.90} ($\pm$0.14) & 0.79 ($\pm$0.22) & 0.68 ($\pm$0.28) & \textbf{0.84} ($\pm$0.30) & \textbf{85.2\%}\\
    \bottomrule
    \end{tabular}
    \caption{\len{Old prompt - Young model} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_old_prompt_young_model}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & 29.34 ($\pm$10.30) & \textcolor{blue}{0.86} ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.43) & -\\
    G-100MCW & 29.14 ($\pm$10.11) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.44) & -\\
    \midrule
    G-B$_{O, FB}$ & \textbf{28.81} ($\pm$10.10) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.05) & \textbf{0.90} ($\pm$0.06) & 0.41 ($\pm$0.43) & 41.1\%\\
    G-B$_{O, 100MIU}$ & 29.05 ($\pm$9.80) & \textcolor{blue}{0.86} ($\pm$0.09) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.43) & 39.6\%\\
    \midrule
    G-D$_{O}$ & 95.21 ($\pm$174.42) & 0.65 ($\pm$0.27) & 0.78 ($\pm$0.18) & 0.78 ($\pm$0.18) & \textbf{0.90} ($\pm$0.25) & \textbf{90.3\%}\\
    \midrule
    \midrule
    D-baseline & 38.18 ($\pm$12.03) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.28 ($\pm$0.38) & -\\
    D-100MCW & 37.73 ($\pm$11.88) & 0.85 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.27 ($\pm$0.39) & -\\
    \midrule
    D-B$_{O, FB}$ & 37.80 ($\pm$11.74) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.07) & \textcolor{blue}{0.87} ($\pm$0.08) & 0.28 ($\pm$0.39) & 29.3\%\\
    D-B$_{O, 100MIU}$ & 36.93 ($\pm$11.68) & \textbf{0.87} ($\pm$0.12) & 0.90 ($\pm$0.09) & 0.86 ($\pm$0.09) & 0.31 ($\pm$0.41) & 29.6\%\\
    \midrule
    D-D$_{O}$ & 40.08 ($\pm$16.77) & 0.85 ($\pm$0.14) & 0.88 ($\pm$0.10) & 0.83 ($\pm$0.14) & \textcolor{blue}{0.61} ($\pm$0.42) & \textcolor{blue}{61.1\%}\\
    \bottomrule
    \end{tabular}
    \caption{\len{Old prompt - Old models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_old_prompt_old_model}
\end{table*}


% \subsubsection{Quantitative 3: The Effects of PPLM-Parameters on Fluency and Control}

% \begin{itemize}
%     \item \textit{Plot and examine the relationship between fluency and control, and various PPLM-parameters (step-size, number of iterations, temperature, top $k$, gamma, KL-scale).}
%     \item \textit{Which patterns do you observe?}
%     \item \len{How much sense does it make to study this, though? Is that the purpose of my thesis? Hasn't this been studied enough in the PPLM-paper? Which parameters do I choose?}
% \end{itemize}

\subsection{BERT$_{FT}$ Attention Patterns}

\begin{itemize}
    \item \textit{\textbf{NB:} This is more relevant to the classification experiments, than to the controlled generation experiments.}
    
    \item \textit{Use BertViz \citep{vig-2019-multiscale} to visualize what parts of sequences BERT's transformer heads and neurons are focusing on.}
    
    \item \url{https://github.com/jessevig/bertviz}
    
    \item \url{https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8}
    
    \item Discussion about interpretability of attention. See Attention is not explanation, Attention is not not explanation, and The Elephant in the Interpretability Room.
\end{itemize}

\len{\textbf{Proposed structure:}}

\begin{itemize}
    \item We use BertViz \citep{vig-2019-multiscale} to visualize and study the attention mechanism maps of BERT$_{FT}$.
    \item An attention weight is naturally interpretable as how much a particular token will be weighted when computing the next representation for the current token \citep{clark-etal-2019-bert}.
    \item However, there is debate in NLP literature about the interpretability of attention mechanisms \len{References to Attention is not explanation, Attention is not \textbf{not} explanation, and The Elephant in the Interpretability Room.}
    \item We refer to an attention head as Head \texttt{layer}-\texttt{head}, where \texttt{layer} and \texttt{head} range from 0 to 11: e.g., Head 3-1 refers to Layer 3, Head 1.
    \item There are $12 \times 12 = 144$ attention heads in BERT-base-uncased.
    \item BERT's pre-processing adds a special token \texttt{[CLS]} to the beginning of an input sequence, and another special token \texttt{[SEP]} to the end. If an input sequence consists of multiple sentences (e.g., a question-answer, or prompt-response input), \texttt{[SEP]} tokens are also used to separate the sentences.
    \item Heads in the same layer tend to exhibit similar behaviors.
    \item \len{Clearly explain how to read the plots and what they do and don't imply.}
    \item \len{Clearly explain that the PPLM-method doesn't change the attention mechanisms of the underlying language models, so visualizing their attention maps would not reveal any age-related attention patterns, whereas BERT$_{FT}$ has been fine-tuned to detect age-related patterns, so visualizing its attention maps makes sense.}
    \item Recurring patterns are observed.
    \item Figures shows examples of specific heads showing similar attention patterns.
    \item E.g., broad attention, virtually equally dispersed among tokens.
    \item Focus on next token.
    \item Focus on special BERT-tokens, \texttt{[CLS]} and \texttt{[SEP]}.
    \item Attention heads in layer 9 seems to exhibit patterns that attend to age-related features.
    \item Focuses on tokens that are associated with their respective age groups (based on empirical observations of this thesis and relevant literature).
    \item E.g., focuses on \textit{awesome}, \textit{facebook}, \textit{cool}, \textit{wanna}, \textit{gonna}, and swear word.
    \item Age-related attention pattern much less pronounced for older age group (could explain why classification performance also worse for that target age), but sensible patterns do seem to be subtly present: focus on \textit{grandfather}, \textit{president}, \textit{workers}, \textit{union}, \textit{fellow}, \textit{greetings}
\end{itemize}

We use visualizations of attention mechanisms \citep{vig-2019-multiscale} in BERT$_{FT}$'s attention heads to analyze recurring patterns when assigning target probabilities to generated prompt-response pairs. An attention weight can be interpreted as an indication of how important particular token is when producing the next representation of the current token \citep{DBLP:journals/corr/BahdanauCB14, clark-etal-2019-bert}. Please note that the PPLM-method does not change the attention weights of the underlying language models, so visualizing the attention weights of our generation models would not reveal any age-related attention patterns. By contrast, BERT$_{FT}$ is fine-tuned to detect age-related patterns, so visualizing its attention maps can inform us about which features are important when assigning a target-age probability to a generated sentence. \len{Maybe add the following sentences to the discussion.} Despite this seemingly natural interpretation, there is much debate about the validity of using attention mechanisms (as opposed to, e.g., saliency methods) as explanations for model output \citep{jain-wallace-2019-attention, wiegreffe-pinter-2019-attention, bastings-filippova-2020-elephant}. However, as \cite{vig-2019-multiscale} and \cite{clark-etal-2019-bert} suggest, attention weight visualization can be used tentatively as a complementary analysis tool to add to sets of different analysis methods to inform researchers about, e.g., possible linguistic patterns that may be attended to by attention-based models. \len{...to here.}

To provide a clear reading experience of the analyses presented below, we revise a few important concepts about BERT and how to read attention weight visualizations. During pre-processing, BERT tokenizes the input text and adds a special token \texttt{[CLS]} to the beginning of the text, and another special token \texttt{[SEP]} is appended to the text. If an input sequence consists of multiple sentences (e.g., a question-answer, or prompt-response input), \texttt{[SEP]} tokens are also used to separate the sentences. BERT$_{FT}$ is a fine-tuned version of BERT-base-uncased \citep{devlin-etal-2019-bert}, which consists of 12 layers of 12 attention heads. We refer to a specific attention head as Head \texttt{layer\_number}-\texttt{head\_number}, where \texttt{layer\_number} and \texttt{head\_number} range from 0 to 11. E.g., Head 3-1 refers to Head 1 in Layer 3. Furthermore, attention weights are visualized as colored lines between tokens of the input sequence, where a thicker line corresponds to a greater attention weight, and the color represents the layer in which the head is present. The input text is displayed twice in parallel columns, to make visualizations of self-attention possible (visualized as lines between identical tokens in the same positions). Because BERT is designed to be deeply bidirectional, tokens can also attend to tokens in previous positions in the input text.

We show attention visualizations of BERT$_{FT}$ when processing prompt-response pairs cherry-picked from the age-targeted prompted results, presented in Tables \ref{tab:ctg_results_ws_young_prompt_young_model} and \ref{tab:ctg_results_ws_old_prompt_old_model}. The generated sequences are chosen to display more pronounced examples of recurring attention patterns. We show two young-targeted generated responses to younger sounding prompts in Figures \ref{fig:bertviz_model_view_ypr1} and \ref{fig:bertviz_model_view_ypr2}, and two old-targeted generated responses to older sounding prompts in Figures \ref{fig:bertviz_model_view_opr1} and \ref{fig:bertviz_model_view_opr2}. All prompts and responses received target probabilities from BERT$_{FT}$ of at least 95\%.

Heads in the same layer have the tendency to attend to similar patterns and linguistic phenomena \cite{clark-etal-2019-bert}. Our analyses seem to confirm this behavior, as recurring patterns are observed for heads in the same layers. For instance, we see (in Figures \ref{subfig:bertviz_model_view_ypr1_broad}, \ref{subfig:bertviz_model_view_ypr2_broad}, \ref{subfig:bertviz_model_view_opr1_broad}, and \ref{subfig:bertviz_model_view_opr2_broad}) that heads in the last layer tend to broadly disperse attention among all tokens. Recurring patterns are also observed in earlier layers, such as Head 2-9 attending to the next token in the sequence (Figures \ref{subfig:bertviz_model_view_ypr1_next}, \ref{subfig:bertviz_model_view_ypr2_next}, \ref{subfig:bertviz_model_view_opr1_next}, and \ref{subfig:bertviz_model_view_opr2_next}), and Head 4-4 attending to the special tokens (Figures \ref{subfig:bertviz_model_view_ypr1_special}, \ref{subfig:bertviz_model_view_ypr2_special}, \ref{subfig:bertviz_model_view_opr1_special}, and \ref{subfig:bertviz_model_view_opr2_special}). Certain heads also seem to pay special attention to age-related linguistic features, specifically certain tokens associated with an age group (mentioned in Section \ref{subsec:ctg_anal_qualitative}). This is most noticeable in Head 9-0 (Figures \ref{subfig:bertviz_model_view_ypr1_age}, \ref{subfig:bertviz_model_view_ypr2_age}, \ref{subfig:bertviz_model_view_opr1_age}, and \ref{subfig:bertviz_model_view_opr2_age}), which seems to consistently devote the majority of its attention to tokens that are found to be indicative of age. For instance, Head 9-0 attends strongly to younger sounding tokens like \textit{facebook}, \textit{awesome}, \textit{cool}, and slang and swear words in Figures \ref{subfig:bertviz_model_view_ypr1_age} and \ref{subfig:bertviz_model_view_ypr2_age}. And the same attention head then focuses strongly on tokens associated with older age in Figures \ref{subfig:bertviz_model_view_opr1_age} and \ref{subfig:bertviz_model_view_opr2_age}: e.g., \textit{workers}, \textit{union}, \textit{greetings}, or \textit{fellow}.




\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr1/bert_model_view_ypr1_layer_11_head_1.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends broadly.}
        \label{subfig:bertviz_model_view_ypr1_broad}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr1/bert_model_view_ypr1_layer_2_head_9.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to next token.}
        \label{subfig:bertviz_model_view_ypr1_next}
     \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr1/bert_model_view_ypr1_layer_4_head_4.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to special tokens.}
        \label{subfig:bertviz_model_view_ypr1_special}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr1/bert_model_view_ypr1_layer_9_head_0.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Age-related attention.}
        \label{subfig:bertviz_model_view_ypr1_age}
     \end{subfigure}
    \caption{Attention weights visualizations of four of BERT$_{FT}$'s attentions heads and the patterns to which they presumably attend when processing representations for a cherry-picked prompt-response pair generated by \textbf{young}-targeted GPT2-Discrim.}
    \label{fig:bertviz_model_view_ypr1}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr2/bert_model_view_ypr2_layer_11_head_1.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends broadly.}
        \label{subfig:bertviz_model_view_ypr2_broad}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr2/bert_model_view_ypr2_layer_2_head_9.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to next token.}
        \label{subfig:bertviz_model_view_ypr2_next}
     \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr2/bert_model_view_ypr2_layer_4_head_4.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to special tokens.}
        \label{subfig:bertviz_model_view_ypr2_special}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/ypr2/bert_model_view_ypr2_layer_9_head_0.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Age-related attention.}
        \label{subfig:bertviz_model_view_ypr2_age}
     \end{subfigure}
    \caption{Attention weights visualizations of four of BERT$_{FT}$'s attentions heads and the patterns to which they presumably attend when processing representations for a cherry-picked prompt-response pair generated by \textbf{young}-targeted GPT2-BoW$_{100MIU}$. \len{\textbf{TODO} - Check if it's ok to have a swear word in this figure.}}
    \label{fig:bertviz_model_view_ypr2}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr1/bert_model_view_opr1_layer_11_head_1.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends broadly.}
        \label{subfig:bertviz_model_view_opr1_broad}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr1/bert_model_view_opr1_layer_2_head_9.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to next token.}
        \label{subfig:bertviz_model_view_opr1_next}
     \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr1/bert_model_view_opr1_layer_4_head_4.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to special tokens.}
        \label{subfig:bertviz_model_view_opr1_special}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr1/bert_model_view_opr1_layer_9_head_0.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Age-related attention.}
        \label{subfig:bertviz_model_view_opr1_age}
     \end{subfigure}
    \caption{Attention weights visualizations of four of BERT$_{FT}$'s attentions heads and the patterns to which they presumably attend when processing representations for a cherry-picked prompt-response pair generated by \textbf{old}-targeted GPT2-Discrim.}
    \label{fig:bertviz_model_view_opr1}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr2/bert_model_view_opr2_layer_11_head_1.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends broadly.}
        \label{subfig:bertviz_model_view_opr2_broad}
     \end{subfigure}
    %  \hfill
     \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr2/bert_model_view_opr2_layer_2_head_9.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to next token.}
        \label{subfig:bertviz_model_view_opr2_next}
     \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr2/bert_model_view_opr2_layer_4_head_4.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Attends to special tokens.}
        \label{subfig:bertviz_model_view_opr2_special}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/bertviz/opr2/bert_model_view_opr2_layer_9_head_0.png}
        \captionsetup{font=footnotesize,labelfont=footnotesize}
        \caption{Age-related attention.}
        \label{subfig:bertviz_model_view_opr2_age}
     \end{subfigure}
    \caption{Attention weights visualizations of four of BERT$_{FT}$'s attentions heads and the patterns to which they presumably attend when processing representations for a cherry-picked prompt-response pair generated by \textbf{old}-targeted DGPT-Discrim.}
    \label{fig:bertviz_model_view_opr2}
\end{figure}

\subsection{Qualitative Analyses}
\label{subsec:ctg_anal_qualitative}

% \subsubsection{Qualitative 1: Summary Statistics and Qualitative Inspection of Various Cases}

% \begin{itemize}
%     \item \textit{Similar to (error)case analyses of Experiment 1.}
    
%     \item \textit{Provide summary statistics and (\textbf{qualitative}) inspection of generated sequences per case.}
    
%     \item \textit{Cases could be: (1) sequences with low, average, high, or very-high perplexity. (2) (in)correctly classified generated sequences.}
    
%     \item \textit{What patterns do you observe among, e.g., misclassified sequences with low perplexity?}
    
%     \item \textit{Provide table of examples per case and age-group. Similar to table \ref{tab:qualexamples}}
% \end{itemize}

% \len{\textbf{Structure as follows:}}

% \begin{itemize}
%     \item Split the generated responses by their (1) perplexity and (2) target probability/classification case.
%     \item Provide summary statistics 
% \end{itemize}

The generated responses to neutral prompts are split by (1) whether or not BERT$_{FT}$ correctly classified a response as its target class, and (2) the level of perplexity: low ($\text{ppl.} \leq 27.52$), medium ($27.52 < \text{ppl.} \leq 35.63$), or high ($\text{ppl.} > 35.63$). See Section \ref{subsec:ctg_anal_ppl_target_prob} for an explanation of the rationale behind these intervals. Table \ref{tab:ctg_model_case_pcts} shows how the generated responses are distributed among these cases for the best performing BoW-based models, and discriminator-based models for both underlying language models and target age groups. As can be seen, the majority of the responses generated by GPT-2 BoW-based models lie in the low perplexity range, whereas the discriminator-based GPT-2 models also have percentage peaks in the higher perplexity registers (e.g., 37.2\% high-perplexity correctly classified for G-Discrim$_{Old}$). By contrast, the DialoGPT-based models all show the majority of their distributions lying in the medium-to-high perplexity range.

To narrow down the comparison, the qualitative inspection of samples is limited to responses generated by the discriminator-based models of both language models and age groups (i.e., samples generated by G-Discrim$_{Young}$, G-Discrim$_{Old}$, D-Discrim$_{Young}$, and D-Discrim$_{Old}$). Moreover, these models are all among those with the highest target probability improvements over their respective baselines in Tables \ref{tab:ctg_results_ws_neutral_prompt_young_models} and \ref{tab:ctg_results_ws_neutral_prompt_old_model}, so the differences in language style should be most pronounced between these setups. Furthermore, to emphasize the differences between perplexity, we only consider low versus high perplexity samples. Finally, the samples used for qualitative inspection are also split by whether or not they were correctly classified by BERT$_{FT}$. To summarize, qualitative inspection are performed on a total of 16 splits: by model (G-Discrim$_{Young}$, G-Discrim$_{Old}$, D-Discrim$_{Young}$, or D-Discrim$_{Old}$), perplexity (low or high), and classification outcome (correct or incorrect). Table \ref{tab:ctg_case_examples} shows examples of generated responses containing the patterns and observations discussed in the remainder of this section.

\begin{table}
    \begin{subtable}{0.5\linewidth}\centering
    {\begin{tabular}{r l}
    \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Good weather \\ we're having.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Hi, how's it going?}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{ I've had my first \\ surgery recently.}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
    \hline
    \end{tabular}}
    \caption{Low perplexity \& correctly classified.}\label{subtab:ctg_case_examples_low_ppl_correct}
    \end{subtable}%
    \begin{subtable}{0.5\linewidth}\centering
    {\begin{tabular}{r l}
    \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Good weather \\ we're having.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Tell me about your \\ latest holiday.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
    \hline
    \end{tabular}}
    \caption{Low perplexity \& incorrectly classified.}\label{subtab:ctg_case_examples_low_ppl_incorrect}
    \end{subtable}%
    
    \medskip
    \begin{subtable}{0.5\linewidth}\centering
    {\begin{tabular}{r l}
    \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Good weather \\ we're having.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Tell me about your \\ latest holiday.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
    \hline
    \end{tabular}}
    \caption{High perplexity \& correctly classified.}\label{subtab:ctg_case_examples_high_ppl_correct}
    \end{subtable}%
    \begin{subtable}{0.5\linewidth}\centering
    {\begin{tabular}{r l}
    \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Good weather \\ we're having.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Tell me about your \\ latest holiday.}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
    \hline
    \end{tabular}}
    \caption{High perplexity \& incorrectly classified.}\label{subtab:ctg_case_examples_high_ppl_incorrect}
    \end{subtable}%
\caption{A table \len{\textbf{TODO - FILL TABLE}}}\label{tab:ctg_case_examples}
\end{table}

Manual inspection of the sub-samples show that correctly classified low-perplexity samples from models targeted towards the older age group use more formal words than their young-targeted counterparts, e.g., \textit{quite}, \textit{significant}, \textit{powerful}, or \textit{institutions}. Samples generated by these models also show recurring topics that are typically associated with older age, e.g., children (\textit{my son} or \textit{your daughter}), history, or politics. Responses about healthcare related subjects are also more common among the samples generated by these models, as indicated by the use of words like \textit{surgery}. When viewing high-perplexity samples from the same set of models (i.e., those target towards the older age group and correctly classified), we see a substantial increase in the amount of gibberish and nonsensical sequences (space-less sequences of words, repetitions of the same words, or sequences of punctuation marks). When we inspect the incorrectly classified samples from models targeted towards the older class, we also see a considerable increase in the amount of nonsensical strings. Remarkably, we also observe more linguistic patterns associated with younger age in this sub-sample, e.g., words of excitement like \textit{favourite} or \textit{best}, informal vocabulary (\textit{pretty much}).

The low-perplexity correctly classified samples generated by models targeted towards the younger age group also contain clear indications of their target age. We observe more usage of slang words, swear words, and informal language, like \textit{yeah}, \textit{dude}, \textit{cool}, \textit{kinda}, and \textit{lol}. The use of the word \textit{like} as a colloquial adverb, quotative, or filler is also more commonplace among the young-targeted models. \len{Add good examples of use of \textit{like} or remove sentence.} Furthermore, these sub-samples are also characterized by increased use of words of excitement and exclamation: e.g., \textit{awesome}, \textit{really}, \textit{love}, \textit{fun}, or \textit{amazing}. Especially in the correctly classified low-perplexity responses generated by G-Discrim$_{Young}$, we see a strong presence of topics such as dating (indicated by words such as \textit{girlfriend} or \textit{boyfriend)}, having parents (\textit{my dad}), parties, and student life (\textit{roommate}. And interesting pattern observed in this sub-sample is one often associated with millennial or social networking language: the tendency to end a (serious-sounding) statement with \textit{lol} or \textit{haha} as a means of softening the perceived severity of the statement or as a signal of interlocutor involvement \citep{newitz2019you, tagliamonte2008linguistic}. For example, \textit{We have to stop talking about this all going so stupid lol lol}. When inspecting the high-perplexity and/or incorrectly classified samples generated by the younger-targeted models, we observe similar patterns. Namely, a substantial increase in non-alphabetical strings, and gibberish.

When comparing the samples from GPT-2 and DialoGPT-based models, we see that the responses generated by DialoGPT are more dialogic, whereas GPT-2 sometimes generates sequences that look more like sentence completions. This is to be expected from the differences in pre-training methods between the two language models \citep{zhang2019dialogpt}. Furthermore, we see a lot more nonsensical low-perplexity responses generated by DialoGPT-based models. Perplexity remains a rough proxy for fluency, and observations like these confirm this problem. That is, low perplexity often does not imply lack of gibberish or nonsense. DialoGPT's strong bias towards generating younger-sounding language is also noticeable in its generated samples. For example, DialoGPT-based models targeted towards the older group still produce lots of word of excitement. However, this older-targeted DialoGPT-based model does succeed in producing significantly fewer usages of slang or swear words, when compared to its younger-target counterpart.


% \subsubsection{Qualitative 2: Human evaluation of fluency, grammaticality, and relevancy}

% \begin{itemize}
%     \item \textit{Generate and sample text passages for a variety of model-configurations and age-groups.}
%     \item \textit{Have a group of human participants rate these sequences on a scale from 1 to 5 for their (1) fluency, (2) grammaticality, (3) relevance to the prompt (if there is one)}
%     \item \textit{Average the ratings, and compare the human evaluation metrics to the automated evaluation metrics reported in Table \ref{tab:ctg_results_ws}}
% \end{itemize}



\begin{table}[H]
    \centering
    \begin{tabular}{@{}l  c  c  c  c  c  c @{}}
    \toprule
    \textbf{model} & \textbf{low ppl. | \cmark} & \textbf{low ppl. | \xmark} & \textbf{med ppl. | \cmark} & \textbf{med ppl. | \xmark} & \textbf{high ppl. | \cmark} & \textbf{high ppl. | \xmark}\\
    \midrule
    G-BoW$_{Young}$ & 48.0\% & 15.6\% & 15.6\% & 10.4\% & 6.7\% & 3.7\%\\
    G-BoW$_{Old}$ & 25.6\% & 37.7\% & 11.9\% & 14.8\% & 5.6\% & 4.4\%\\
    G-Discrim$_{Young}$ & 33.3\% & 15.9\% & 17.4\% & 8.5\% & 17.0\% & 7.9\%\\
    G-Discrim$_{Old}$ & 23.8\% & 18.2\% & 13.4\% & 3.3\% & 37.2\% & 4.1\%\\
    D-BoW$_{Young}$ & 5.2\% & 0.4\% & 40.0\% & 5.2\% & 43.3\% & 5.9\%\\
    D-BoW$_{Old}$ & 3.0\% & 5.9\% & 10.4\% & 35.2\% & 8.5\% & 37.0\%\\
    D-Discrim$_{Young}$ & 5.9\% & 2.7\% & 23.0\% & 5.6\% & 56.9\% & 5.9\%\\
    D-Discrim$_{Old}$ & 7.0\% & 4.1\% & 19.3\% & 11.5\% & 30.3\% & 27.8\%\\\bottomrule
    \end{tabular}
    \caption{\len{TODO - Turn this table into grouped barcharts.}}
    % \caption{Percentages of (non-)overlapping (in)correctly predicted cases between trigram and BERT$_{FT}$ models for BNC. *Pre-processed sequence length is measured in tokens.}
    \label{tab:ctg_model_case_pcts}
\end{table}

% \begin{table*}[h]
% \resizebox{\linewidth}{!}{
% % \small
% \begin{tabular}{@{}lllll@{}}
% \toprule
% \bf age & \bf both correct                & \bf both wrong                 & \bf BERT$_{FT}$ correct | trigram wrong                              & \bf trigram correct | BERT$_{FT}$ wrong                                                            \\ \midrule
% 19-29     & oh that's cool              & A retrospective exhibition & what even on the green slope?             & really?                                                                    \\
% 19-29     & a text and then I'll do it  & chuck them in those pots   & yeah you told me to do you told me to do  & and she like won't eat any carbs and she's like                            \\
% 19-29     & yeah                        & mm                         & somebody made the f***ing table           & do you not like total greens? \\ \midrule
% % well er it's not acceptable just to ditch \textless{}unclear\textgreater{} \\ \hline
% 50+       & I said no I don't have them & yeah                       & really?                                   & my under stairs in the kitchen                                             \\
% 50+       & that's of course            & no no that's alright       & it's still we we frequently walk that way & in the first place                                                         \\
% 50+       & oh right                    & what a tragic life         & since this this was new this house?       & thank you very much                                                        \\ \bottomrule
% \end{tabular}
% }
% \caption{Examples
% % cherry-picked examples per age group 
% where both models are correct/wrong or only BERT$_{FT}$/trigram is correct.}\label{tab:qualexamples}
% \end{table*}

% Qualitative inspection focuses on low versus high perplexity (i.e., medium perplexity is omitted, both for feasibility (in total Table \ref{tab:ctg_model_case_pcts} describes 48 sets of responses), and only responses generated by the discriminator-based model with the highest target prob. gaing over baseline (compared to its age-opposite counterpart): so that's GPT-2 Discrim old vs. young. And the same for BoW

\len{\textbf{Observations in a few of the best performing models:}}

\begin{itemize}
    \item GPT2 | Discrim-O | Low ppl | BERT correct:
    \begin{itemize}
        \item Not very dialogic 
        \item Use of larger words like "significant", "powerful", "institutions"
        \item Talks about "my son" and children
        \item Talks about work.
        \item Talks about hospitals, surgery, health care (which is to be expected if you look at the old-wordlists)
        \item Older slang words "quite"
        \item Talks about history and politics.
    \end{itemize}
    \item GPT2 | Discrim-O | Low ppl | BERT incorrect:
    \begin{itemize}
        \item Despite low GPT-1 perplexity, considerably more gibberish that BERT-correct counterpart, space-less repetitions of words mostly though, not nonsensical sequences of characters. First of all, this makes a case for a different proxy for fluency (find different measures to consider for future research, and/or suggest human evaluation).
        \item The non-gibberish responses contain more words of excitement and positive sentiment "favourite", "best".
        \item Gibberish most likely obfuscates BERT, hence worse prediction.
        \item Talks about football clubs and premier league
        \item Younger sounding slang "pretty much"
        \item PPLM-discrim old (BERT incorrect) nonsense pattern is lots of punctuation marks, parentheses, and words without spaces.
    \end{itemize}
    \item GPT2 | Discrim-O | High ppl | BERT correct:
    \begin{itemize}
        \item Predominantly gibberish. 
        Many sequences of non-alphabetical characters/punctuation marks (parentheses, apostrophes, etc. )
        \item When non-gibberish, does talk about "my wife" and "work"
    \end{itemize}
    \item GPT2 | Discrim-O | High ppl | BERT incorrect:
    \begin{itemize}
        \item Very small subset (see Table \ref{tab:ctg_model_case_pcts}, and earlier plots about this model having a clear tradeoff between perplexity and target prob)
        \item Mostly difluencies
        \item Formal words like "precious"
    \end{itemize}
    \item GPT2 | Discrim-Y | Low ppl | BERT correct:
    \begin{itemize}
        \item Substantially more use of younger sounding slang words: "yeah", "dude", "cool", "awesome", "like", "kinda", "hot", "horny", "gonna", "lol". NB: "yeah" was also picked up by the trigram in Chapter 3 (make proper reference) to be strongly indicative of younger language
        \item More swear words "fucking", "shit", "dick", "hell"
        \item More words of excitement "awesome" "really" "love", "fun", "amazing"
        \item More talk about dating "girlfriend", "boyfriend", "horny", "sex"
        \item More talk about depression and anxiety
        \item More tech-talk
        \item Lots of disfluency and repetition though.
        \item Talks about dancing and parties
        \item Interesting pattern of millennial language: uttering something serious, but ending with a neutralizing, tension-breaking humoring expression, e.g., ``We have to stop talking about this all going so stupid lol lol.'' ``Shit just got weird LOL."
        \item More topics that relate to student-life: roommate, parties, drinking, bars, clubs, tv series on Netflix
    \end{itemize}
    \item GPT2 | Discrim-Y | Low ppl | BERT incorrect:
    \begin{itemize}
        \item Considerably more nonsense than the BERT-correct counterpart. 
        \item More non-language sequences of characters, \texttt{<|endoftext|>} tokens.
        \item Similar word-use and topics: tv series (HBO Game of Thrones), swear words, "mom" and "dad(dy)"/
        \item PPLM-discrim young (BERT incorrect) nonsense pattern is lots of \texttt{<|endoftext|>} tokens.
    \end{itemize}
    \item GPT2 | Discrim-Y | High ppl | BERT correct:
    \begin{itemize}
        \item Substantially more nonsense, disfluency, gibberish, non-alphabetical characters.
        \item Similar patterns to low-perplexity counterpart. Actually same patterns, but with much more nonsense.
    \end{itemize}
    \item GPT2 | Discrim-Y | High ppl | BERT incorrect:
    \begin{itemize}
        \item Very small subset (look at Table \ref{tab:ctg_model_case_pcts} and perplexity-targetprob plots).
        \item Same patterns
        \item Mostly nonsense.
    \end{itemize}
    \item DialoGPT | Discrim-O | Low ppl | BERT correct:
    \begin{itemize}
        \item Very small subset (See table and graph).
        \item Talks about surgery, hospital etc.
        \item Talks about daughters.
        \item Almost no slang words
        \item More formal language, and complete sentences.
        \item Quite some nonsense
    \end{itemize}
    \item DialoGPT | Discrim-O | Low ppl | BERT incorrect:
    \begin{itemize}
        \item Very small subset (See table and graph).
        \item A lot of nonsense, repeated eot tokens.
        \item Pretty much similar patterns to setup above.
    \end{itemize}
    \item DialoGPT | Discrim-O | High ppl | BERT correct:
    \begin{itemize}
        \item Lots of gibberish
        \item Noticeably, a lot less typically "older" sounding language than GPT-2 counterpart. Which makes sense when you look at the difference in target probabilities.
        \item More words of excitement and exclamation than other old discrim model. Likely due to DialoGPT's young-bias.
        \item Barely any slang or swear words.
        \item Pretty neat formal sentences when not gibberish.
    \end{itemize}
    \item DialoGPT | Discrim-O | High ppl | BERT incorrect:
    \begin{itemize}
        \item A lot more nonsensical/non-language sequences of characters.
        \item Talks about days off from work
        \item words like ``quite'', ``glad''
        \item talks about other people's parents and children.
    \end{itemize}
    \item DialoGPT | Discrim-Y | Low ppl | BERT correct:
    \begin{itemize}
        \item Very small sample size
        \item More informal language "gonna"
        \item Talks about gifs and comments
    \end{itemize}
    \item DialoGPT | Discrim-Y | Low ppl | BERT incorrect:
    \begin{itemize}
        \item Predominantly nonsensical sequences.
        \item Repetitions of words
        \item No slang, or words of excitement or other clear giveaways of young language.
        \item Very small sample size.
    \end{itemize}
    \item DialoGPT | Discrim-Y | High ppl | BERT correct:
    \begin{itemize}
        \item Lots of words of excitement and exclamation marks
        \item slang and informal words: "dude", "buddy", "cool", referring to the basketball team, the Cleveland Cavalliers, as the "Cavs", "howdy"
        \item Talks about going on vacation to the beach with friends.
        \item Uses the word "like"
        \item Uses emojis ":P", ":D"
        \item Fair amount of gibberish.
    \end{itemize}
    \item DialoGPT | Discrim-Y | High ppl | BERT incorrect:
    \begin{itemize}
        \item Mostly gibberish.
        \item Similar patterns to BERT-correct counterpart, just way more nonsense.
    \end{itemize}
\end{itemize}

Overall, the GPT-2 ones are also often sentence completions, and not as often rebuttals as DialoGPT. This is understandable from the point of view of how the models have been pre-trained.