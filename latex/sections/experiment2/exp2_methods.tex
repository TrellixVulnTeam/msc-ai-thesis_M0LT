% \section{Methods for Controlled Language Generation using Plug-and-Play Language Models}
\section{Methodology and Experimental Setup}
\label{sec:exp2_methods}

% Plug-and-play language generation entails using a attribute model to make activation-space perturbations on the output of a pre-trained language model. In this section, I explain the important architectures involved, and the plug-and-play method in the following sub-sections. Details about how the generation experiments are set up and evaluated are given at the end of the section.

This section describes and motivates the methodology, experimental details of our controlled dialogue generation experiments, and the most important components involved: attribute model development, dialogue response generation, choice of prompt, and evaluation of generated responses. All computations for attribute model development, dialogue response generation, and evaluation are performed on an NVIDIA TitanRTX GPU.

% \len{TODO - Do I need this short intro?}

%====(Tuesday 16 November 2021)=== THE FOLLOWING SUBSECTIONS HAVE BEEN MOVED TO THE BACKGROUND. FROM HERE....========%
% \subsection{Transformers}

% The Transformer architecture plays a central role in most of the recent advances in NLP. The same holds for the methods used in this thesis to investigate controlled dialogue generation and automated detection of age-related linguistic patterns in dialogue utterances. For a more detailed review of the model architecture, the reader is referred to the original paper \citep{vaswani2017attention} or this blog post\footnote{\url{https://jalammar.github.io/illustrated-transformer/}}.

% The Transformer, like most neural sequence processing models, has an encoder-decoder structure. On a high level, the encoder receives an input sequence $\textbf{x} = (x_1, ..., x_n)$ (e.g., a sentence), and maps this to a sequence of latent continuous variables $\textbf{z} = (z_1, ..., z_n)$. The decoder then takes $\textbf{z}$ as input, and maps this to an output sequence $\textbf{y} = (y_1, ..., y_m)$. Note that the use of positional encodings of the input and output embeddings enables the Transformer to process and generate sequences in arbitrary order, allowing for a high degree of parallelization. The generation of $\textbf{y}$ happens element-by-element in an auto-regressive fashion, where at step $t$, element $y_{t - 1}$ is also taken as input.

% Both the encoder and decoder are comprised of $N$ identical layers (denoted by the `N $\times$' in the left part of Figure \ref{fig:transformer_architecture}). Every sub-layer performs a succession of transformations using multi-head self-attention mechanisms and point-wise, fully connected layers, along with residual connections \citep{he2016residual} around every sub-layer followed by layer normalization \citep{DBLP:journals/corr/BaKH16}. The decoder's first self-attention sub-layer is masked to ensure that the output predictions at sequence position $i$ cannot depend on output positions greater than $i$. Finally, the decoder passes its output through a linear and softmax layer to produce a probability distribution over the problem space (e.g., the vocabulary) from which the most likely symbols for the generated output sequence $\textbf{y}$ can be sampled.

% A key aspect of the Transformer architecture is its use of attention \citep{DBLP:journals/corr/BahdanauCB14}. This allows the encoder-decoder architecture to selectively focus on parts of the input sequence to produce a more informative hidden representation. \cite{vaswani2017attention} formulate an attention function as a mapping of queries and sets of key-value pairs to an attention output, where matrices represent the queries $Q$, keys $K$, and values $V$. The attention output is a weighted sum of the values, based on the relevance of the corresponding keys to a query. In particular, they employ scaled dot-product attention:

% \begin{equation}
%     \texttt{Attention}(Q, K, V) = \texttt{softmax} \left( \frac{QK^T}{\sqrt{d_k}}\right) V.
% \end{equation}

% Furthermore, \cite{vaswani2017attention} propose to use multi-head attention by using learned linear projections to project the queries, keys and values $h$ times, and apply the aforementioned attention function to these projections in parallel. The concatenation of these attention outputs, passed through a linear layer, ultimately produces the final output of the Transformer's attention sub-layers. This allows the model to attend to the relevant information from all representation sub-spaces at various sequence positions. See Figure \ref{fig:transformer_architecture} for a schematic illustration of the Transformer's structure described above.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{figures/transformer_lillog.png}
%     \caption{An overview of the full Transformer model architecture. \textit{Collated image source:} Fig. 17 in this blog post \url{https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}. \textit{Original image source:} Figures 1 and 2 in \cite{vaswani2017attention}}
%     \label{fig:transformer_architecture}
% \end{figure}

% \subsection{Causal Language Modeling with Transformers for Dialogue Response Generation}
% As described in Section \ref{subsec:background_dialogue}, here we focus on dialogue response generation. A formulation of dialogue response generation as a language modeling task is described in Section \ref{subsec:dialogue_generation_as_lm}. Following the conventions of \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug}, a dialogue is comprised of multiple alternating turns (sometimes referred to as utterances) between more than one speaker. For simplicity, this project only focuses on dialogues between two speakers. The conversation history at turn $t$ is defined as $\mathcal{D}_t = \{S^{(1)}_1, S^{(2)}_1, ..., S^{(1)}_t\}$, where $S^{(j)}_t$ is speaker $j$'s utterance at time $t$. \cite{madotto-etal-2020-plug} denote speaker $1$ as the user $U$, and speaker $2$ as the conversational system $S$, yielding dialogue history $\mathcal{D}_t = \{U_1, S_1, ..., U_t\}$. This notational convention will also be used for the user-system experiments later on in this thesis.

% A Transformer-based language model (denoted $\texttt{LM}$) is used in this thesis to model the distribution of dialogues, using dialogue history at time $t$, $\mathcal{D}_t$, as a prompt to auto-regressively generate the dialogue continuation $S_t$. More specifically, let the concatenation of the dialogue history at $t$ and its continuation, $\{\mathcal{D}_t, S_t\}$, be represented as a sequence of tokens $\textbf{x}= \{x_0, ..., x_n\}$. Then, by recursively applying the product rule of probability (\cite{bishop2006pattern}), the unconditional probability of the sequence $p(\textbf{x})$ can be expressed as:

% \begin{equation}
%     p(\textbf{x}) = \prod_{i = 1}^n p(x_i | x_0, ..., x_{i - 1}).
% \end{equation}

% \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug} define the Transformer's decoding process in a recursive fashion. Let $H_t$ denote the conversation history's key-value pairs, i.e., $H_t = \left[ (K_t^{(1)}, V_t^{(1)}), ..., (K_t^{(l)}, V_t^{(l)}) \right]$, with $(K_t^{(i)}, V_t^{(i)})$ representing the key-value pairs from the $\texttt{LM}$'s $i$-th layer generated at all time steps $0$ through $t$. This results in the recurrent dedocing process being expressed as:

% \begin{equation}
%     o_{t + 1}, H_{t + 1} = \texttt{LM} \left( x_t, H_t \right),
% \end{equation}

% where $o_{t + 1}$ is the hidden state of the last layer. Finally, after applying a softmax transformation, the next token $x_{t + 1}$ is sampled from the resulting probability distribution, i.e.,  $x_{t + 1} \sim p_{t + 1} = \texttt{softmax} \left( W o_{t + 1} \right)$, where $W$ is a linear mapping from the model's last hidden state to a vector of vocabulary size. This recursive formulation allows for efficient text generation by leveraging cached memories, without repeated forward passes.

% \subsection{Plug-and-Play Language Modeling}
% \label{sec:ppm}

% The Plug-and-Play Language Model (PPLM) \cite{dathathri2019plug} works by using a text classifier, referred to as an attribute model, to control the text generated by a language model. Let $p(X)$ denote the output distribution of a Transformer-based language model (e.g., GPT-2 or DialoGPT), where $X$ represents the generated text. And $p(a | X)$ denotes the attribute model (e.g., a single-layer or BoW classifier) that represents the degree of adherence of text $X$ to a certain attribute $a$ (e.g., topic or sentiment (previous work), or age-group characteristics (this work)). Then PPLM can be seen as modeling the conditional distribution of generated text $X$ given attribute $a$, i.e., $p(X | a)$. Note that Bayes' theorem ties these three definitions together as follows:

% \begin{equation}
%     p(X | a) \overbrace{=}^{\text{Bayes' theorem}} 
%     % \frac{p(a, X)}{p(a)} = 
%     \frac{p(X) p(a | X)}{p(a)} \propto
%     p(X)p(a | X).
% \end{equation}

% % \len{TODOs -
% % \begin{itemize}
% %     \item Maybe this isn't the best spot, but somewhere in the methodology motivate the choice of using a unigram wordlist as BoW, and a linear classifier trained with a transformer-based architecture.
% %     \item Also emphasize the continuing narrative between Experiment 1 and Experiment 2. Namely, how the best performing systems in Exp1 are used to inform choices during Exp2 (trigram and BERT best classifiers --> use unigram and linear layer used for PPLM).
% %     \item Also explain how the PPLM setup is not compatible with lists of $n$-grams (for $n>1$), as it makes perturbations at the unigram level. And adapting the system to fit trigram BoW's implies retraining the entire underlying language model (like GPT-2) for trigrams, defeating the purpose of PPLM (leveraging large scale language models for controllability, without having to finetune the massive architectures).
% %     \item Then go on to argue that the unigram classifier is on par with trigram, making it a viable choice for the 100MIU BoW.
% %     \item Also mention that your work extends the original BoW-PPLM by using various empirically generated BoWs, instead of curated wordlists. Making it more reproducible.
% %     \item Double-check if you should make this last argument in the methodology, and not in related work or experimental details.
% % \end{itemize}}

% To control the generated text, PPLM shifts the aforementioned history $H_t$ (i.e., all Transformer key-value pairs generated up to time $t$) in the direction of the sum of two gradients:

% \begin{enumerate}
%     \item Ascending $\nabla \log p(a | X)$: maximizing the log-likelihood of the desired attribute $a$ under the conditional attribute model. This enforces attribute control.
%     \item Ascending $\nabla \log p(X)$: maximizing the log-likelihood of the generated language under the original (possibly conversational) language model. This promotes fluency of the generated text.
% \end{enumerate}

% These two incentive-representing gradients are combined with various coefficients, yielding a set of tunable parameters to steer the generated text in the direction of the desired fluency, attribute control, and length.

% Let's first focus on the first of the two gradients, i.e., the attribute control promoting $\nabla \log p(a | X)$. $\Delta H_t$ represents the update to history $H_t$ that pushes the distribution of the generated text $X$ in the direction that has a higher likelihood of adhering to desired attribute $a$. The gradient update rule can be expressed as:

% \begin{equation}
%     \Delta H_t \leftarrow \Delta H_t + \alpha
%     \frac{\nabla_{\Delta H_t} \log p(a | H_t + \Delta H_t)}
%     {\norm{\nabla_{\Delta H_t} \log p(a | H_t + \Delta H_t)}^{\gamma}}
% \label{eq:H_update_rule}
% \end{equation}

% where $\alpha$ is the step size, and $\gamma$ denotes the normalization term's scaling coefficient. Both step size ($\alpha$) and the scaling coefficient ($\gamma$) influence attribute control. Attribute control can be softened by either decreasing $\alpha$ or increasing $\gamma$ and vice versa. Note that $\alpha = 0$ recovers the original uncontrolled underlying language model (e.g., GPT-2 or DialoGPT). In practice, $\Delta H_t$ is initialized at zero, and the update rule in Equation \ref{eq:H_update_rule} is applied $m$ times (usually 3 to 10), resulting in the updated key-value pair history $\tilde{H}_t  = H_t + \Delta H_t$. Then the updated history $\tilde{H}_t$ is passed through the language model, yielding the updated logits (final Transformer-layer): $\tilde{o}_{t + 1}, H_t = \texttt{LM}(x_t, \tilde{H}_t)$. And finally the shifted $\tilde{o}_{t + 1}$ is linearly mapped through a softmax layer to produce a new, more attribute-adherent, distribution from which to sample, i.e., $x_{t + 1} \sim \tilde{p}_{t + 1} = \texttt{softmax} \left( W \tilde{o}_{t + 1} \right)$.

% The method described until now will generate attribute-adherent text, but will likely yield fooling examples \citep{nguyen2015deep} that are gibberish to humans, but get assigned high $p(a | x)$ by the attribute model \citep{dathathri2019plug}. That is why \cite{dathathri2019plug} apply two methods to ensure fluency of the generated text. The first is to update $\Delta H_t$ such to minimize the Kullback-Leibler (KL) divergence (denoted $D_{KL})$ between the shifted and original distributions. In practice, $D_{KL}$ is scaled by a coefficient $\lambda_{KL}$, typically found to work well for most tasks when set to 0.01. Repetitive text generation (i.e., high $p(a | x)$ but low $p(x)$) can therefore sometimes be avoided by increasing $\lambda_{KL}$. The second method to ensure fluency is Post-norm Geometric Mean Fusion \citep{stahlberg-etal-2018-simple} which, instead of directly influencing $\Delta H_t$ like minimizing $D_{KL}$, fuses the altered generative distribution $\tilde{p}_{t + 1}$ with the unconditional language distribution $p(x)$. This is done during generation by sampling the next token as follows:

% \begin{equation}
%     x_{t + 1} \sim \frac{1}{\beta}
%     \left( 
%     \tilde{p}_{t + 1}^{\gamma_{gm}} p_{t + 1}^{1 - \gamma_{gm}}
%     \right)
%     \label{eq:gm_fusion}
% \end{equation}

% where $\beta$ is a normalization constant, $p_{t + 1}$ and $\tilde{p}_{t + 1}$ denote the original and modified distributions, respectively, and $\gamma_{gn}$ is a scaling term that interpolates between the two distributions. Because the new sampling distribution in Equation \ref{eq:gm_fusion} converges towards the unconditional language model as $\gamma_{gm} \rightarrow 0$, repetitive text generation can be avoided by decreasing the scaling term.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{figures/pplm_fig1.png}
%     \caption{A schematic overview of the plug-and-play interaction between attribute model $p(a | \textbf{x})$ and language model $p(\textbf{x})$. \textit{Original image source:} Figure 1 of \cite{dathathri2019plug}}
%     \label{fig:pplm_schematic_overview}
% \end{figure}


% % \subsubsection{PPLM is not fine-tuning}

% It is worth repeating that the plug-and-play method applied by \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug} is different from fine-tuning. Note that in Equation \ref{eq:H_update_rule} the gradient updates are restricted to the history $H_t$, and do not affect the model's parameters. Because the key-value pairs $(K_t^{(i)}, V_t^{(i)})$ that comprise $H_t$ are activations and not model-weights, the updates only take place in the activation-space. This means that PPLM leaves the underlying (conversational) language model untouched.

% Contrary to fine-tuning often massive LMs, PPLM does not incur a significant training cost (depending of course on the complexity of the discriminator or attribute model). However, \cite{madotto-etal-2020-plug} show that PPLM needs a fixed number of $m$ update-steps to for every generated token. This makes the original PPLM setup unsuitable for online interactive applications, like conversational systems. Addressing this problem, they introduce plug-and-play conversational model (PPCM), which extends PPLM by using the original model setup to generate dialogue datasets with the desired attribute $a$, and then use optimized residual adapters \citep{bapna-firat-2019-simple} to control $\texttt{LM}$'s output distribution. More specifically, \cite{madotto-etal-2020-plug} use PPLM to generate $n$ attribute-adherent dialogue datasets $\mathscr{D}^a = \{\mathcal{D}^1, ..., \mathcal{D}^n\}$, for attribute $a$. These generated dialogue datasets are then used to train the residual adapter modules that control the language model's output distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% METHODOLOGY ABOUT PPCM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% CONSIDER WHERE THIS INFORMATION MIGHT STILL BE USEFUL. MAYBE A VERY SHORT SUMMARY IN THE DISCUSSION AS A PROPERLY THOUGHT OUT FUTURE RESEARCH DIRECTION

% Residual adapters are optimizable modules stacked on every Transformer-layer of a pre-trained (language) model. The adapter module then steers the Transformer's output distribution without changing the pre-trained model's weights. A Layer Normalization module \citep{DBLP:journals/corr/BaKH16} followed by an auto-encoder with residual a connection constitutes a residual adapter module. More specifically, the residual adapter block can be expressed as the following function composition:

% \begin{equation}
% \begin{gathered}
%     f_{\theta_i} (x) = \texttt{ReLU}(\texttt{LayerNorm} (x) \cdot W_i^E) \cdot W_i^D, \\
%     \texttt{Adapter} (o_{:t}^i) = f_{\theta_i}(o_{:t}^i) + o_{:t}^i
% \end{gathered}
% \end{equation}


% where $o_{:t}^i \in \mathbb{R}^{t \times d}$ denotes the Transformer's $i$-th layer's latent output at step $t$, $d$ is the hidden state's size, $W_i^E$ and $W_i^D$ are learnable parameter-matrices of sizes $d \times m$ and $m \times d$, respectively. Finally, $m$ is the auto-encoder's bottle-neck dimension, which is a tunable hyper-parameter for changing the residual adapter's capacity. In practice, \cite{madotto-etal-2020-plug} use PPLM to generate $n$ attribute-adherent dialogue datasets $\mathscr{D}^a = \{\mathcal{D}^1, ..., \mathcal{D}^n\}$, for attribute $a$. These generated dialogue datasets are then used to train the residual adapter, which they aptly name a plug-and-play adapter, so it can be used to control the language model's output distribution. So for every attribute $a$, they train the plug-and-play adapter's parameters $\Theta_a := \{\theta^{a}_0, ..., \theta^{a}_l\}$, where $\theta^{a}_i := \{W_i^{E, a},W_i^{D, a}\}$, such that negative log-likelihood over the corresponding dialogue dataset $\mathscr{D}^a$ is minimized:

% \begin{equation}
%     \Theta_a \text{ s.t. } 
%     \min \mathcal{L} (\mathscr{D}^a) = - \sum_{k}^{|\mathscr{D}^a|} \sum_{i}^n 
%     \log p(s_i^k | s_{<i}^k, \mathcal{D}^k_t),
% \end{equation}

% where $s_i^k$ is the $i$-th generated token of response $S_t^k = \{s_0^k, ..., s_n^k\}$ with maximum sequence length $n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \textbf{TODO: Motivate the use of discriminators (as opposed to BoW) as attribute models. Emphasize that discriminators do not require human selected word-lists. Frequency-based word-lists, though easily produced based on simple heuristics, still often require human second-opinion to confirm their validity. And mention the use of discriminators that are more complex than single-layer linear classifiers.}

% Finally, in the original PPLM paper, the authors experiment with controlled language generation using as attribute models both trained single-layer discriminators, as bag-of-words (BoW) classifiers, where BoW essentially requires providing a human-selected word-list.

% \len{TODOs -
% \begin{itemize}
%     \item Maybe this isn't the best spot, but somewhere in the methodology motivate the choice of using a unigram wordlist as BoW, and a linear classifier trained with a transformer-based architecture.
%     \item Also emphasize the continuing narrative between Experiment 1 and Experiment 2. Namely, how the best performing systems in Exp1 are used to inform choices during Exp2 (trigram and BERT best classifiers --> use unigram and linear layer used for PPLM).
%     \item Also explain how the PPLM setup is not compatible with lists of $n$-grams (for $n>1$), as it makes perturbations at the unigram level. And adapting the system to fit trigram BoW's implies retraining the entire underlying language model (like GPT-2) for trigrams, defeating the purpose of PPLM (leveraging large scale language models for controllability, without having to finetune the massive architectures).
%     \item Then go on to argue that the unigram classifier is on par with trigram, making it a viable choice for the 100MIU BoW.
%     \item Also mention that your work extends the original BoW-PPLM by using various empirically generated BoWs, instead of curated wordlists. Making it more reproducible.
%     \item Double-check if you should make this last argument in the methodology, and not in related work or experimental details.
% \end{itemize}}

%================================================TO HERE ========================================%

% \subsection{Experimental Details}

% The overall goal of this chapter, Experiment 2, is controlled dialogue generation. 
Controlled dialogue generation experiments are performed using PPLM-setups that differ with respect to \textbf{(1)} pre-trained Transformer-based language model (GPT-2 or DialoGPT), \textbf{(2)} type of attribute model (BoW or discriminator), \textbf{(3)} style attribute (younger, older, or uncontrolled), and \textbf{(4)} the style of the prompt on which generation is conditioned (younger, neutral, or older). Note that when the style attribute is ``uncontrolled'', it implies the unaltered original underlying language model (i.e., GPT-2 or DialoGPT) is recovered. 
% An unprompted model conditions its generated output on the \texttt{<|endoftext|>} token.



% The workflow of my controlled text generation experiments can be divided into three phases, attribute model development, generation, and evaluation. The following paragraphs describe and motivate the steps and choices per phase. 
% All experiments are conducted on an NVIDIA TitanRTX GPU.

\subsection{Attribute Model Development}
\label{subsec:att_model_dev}
As mentioned in the introduction to this chapter, in the PPLM-method, controlled generation is achieved by using an attribute model to make activation space perturbations to an underlying language model. This alters the output distribution of the language model to make it more likely to generate text that aligns with a predefined writing style. Such an attribute model is significantly smaller and less costly to train than the underlying large-scale language model (e.g., GPT-2 or DialoGPT). An attribute model can either be a simple bag-of-words, or a more complex neural discriminator. When developing an attribute model, either a discriminator is trained on the dialogue data, or a bag-of-words is statistically constructed from the same dialogue dataset (see Section \ref{subsec:dialogue_dataset}).

\paragraph{Bag-of-Words} A simple bag-of-words can also serve as an attribute model. 
% Lists of unigrams are used as BoWs, because the PPLM setup is not compatible with lists of $n$-grams for $n > 1$, as it relies on perturbations at the unigram-level. Making a PPLM-system compatible with, e.g., trigrams, would amount of retraining the entire underlying language model (like GPT-2), thereby defeating the purpose of PPLM, i.e., leveraging large LMs for controllability, without incurring significant re-training costs. However by allowing the best-performing classifiers to inform decisions about generation, we use the best unigram-based classifier's list of most informative features as an attribute model. 
% This is a viable choice, because the unigram and trigram classifiers are on par (See Table~\ref{tab:bnc_classification_ws}).
We extend the approach of \cite{dathathri2019plug} that relies on curated wordlists, by applying two empirical approaches to extract wordlists from the dialogue corpus. An empirical approach has the benefit of being more reproducible, and not requiring a domain expert to manually curate a list. In the first approach, the BoW consists of the 100 most informative unigrams of the unigram classifier used during the text classification experiments (see Table \ref{tab:bnc_classification_ws} for the results). The most informative unigrams per age group are deemed the most distinguishing features by the unigram classifier. They could therefore be used to make sensible perturbations to a language model's output, yielding more effective control.

The second method of wordlist extraction is fully frequency-based.
% , these setups are labeled $FB$ in Table \ref{tab:ctg_results_ws}
The goal of this extraction method is to yield two distinct sets of words that are representative of each age group's language. The frequency-based wordlists per age group are constructed from the \textit{imbalanced} dialogue dataset. Recall that the 19-29 age group is over-represented in the original imbalanced dialogue dataset, and we discarded utterances from that class to balance the dataset when using it to train classifiers. The use of the original dataset for wordlist extraction is motivated by the fact that the original dataset contains more utterances, and a frequency-based wordlist from that corpus would be a better representation of the younger age group's speaking style.
Wordlist extraction is carried out as follows: Per age group, all unique words are ordered by frequency of occurrence in all utterances. For both ordered lists of word counts, the most frequent words are kept that make up for at least 85\% of the cumulative occurrences.
% the cumulative probability mass of the full age-specific distribution of words. 
Then, the words are removed that appear in both lists (i.e., the overlapping set of words is discarded). Of the resulting two non-overlapping ordered lists of words and their numbers of occurrences, only the words are kept that account for at least 85\% of the respective wordlist's cumulative occurrences. The resulting lists consist of 56 (younger age group related), and 92 (older age group related) words. The 85-th percentile cutoff points are chosen to yield wordlists of similar lengths as those used by \cite{dathathri2019plug}. Both pairs of wordlists are included in Section \ref{sec:wordlists}.

% \len{Provide a table showing intuitive examples of how BoW-based control works. Make it look like table \ref{tab:dialogue_gen_example_exp2}}

% \begin{table}[h]%{0.5\linewidth}
%     \centering
%     {\begin{tabular}{r l}
%     \hline
%         \ \cellcolor{yellow!25}\textbf{\textit{MODEL}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Generated utterance}}\\
%         \hline 
%         \cellcolor{gray!25}\textbf{\textit{GPT-2}} & \multicolumn{1}{>{\columncolor{gray!10}}l}{\tabularCenterstack{l}{Like about what?}}\\
%         \cellcolor{red!25}\textbf{\textit{GPT-2 + BoW-Young}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{Like about what?}}\\
%         \ \cellcolor{blue!25}\textbf{\textit{GPT-2 + BoW-Old}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{What would you like to share about?}}\\
%     \hline
%     \end{tabular}}
%     \vspace{3mm}
%     \caption{...}\label{} % examples generated by NP | DGPT | Discrim | Young and NP | DGPT | Discrim | Old
% \end{table}%

\paragraph{Discriminators} When training the discriminators, the dialogue dataset is randomly split into a training (90\%) and test (10\%) set. 
Note that there is no validation set, because the discriminator attribute models have no hyperparameters to be tuned. Namely, they are single-layer linear classifiers, whose input layer size is equal to the output layer size of the underlying language model (GPT-2 or DialoGPT). Furthermore, the dimensionality of the linear layer can not be varied once an underlying language model has been chosen, as it must be of equal size to the LM's output layer. Hence, the linear layer's dimensionality is not a changeable hyperparameter. Moreover, the effects of random initializations are found to be negligible, most likely due to the comparatively small size of the attribute model discriminator.
The frozen embeddings of either GPT-2-medium \citep{radford2019language} or DialoGPT-medium \citep{zhang2019dialogpt} are fed into trainable linear classifiers, seeking to distinguish between transcribed dialogue utterances from younger (ages 19 to 29) and older (ages 50 and over) speakers. The discriminators are trained using Adam \citep{DBLP:journals/corr/KingmaB14} with a learning rate of $1\cdot10^{-4}$ 
and default values for all other parameters from PyTorch's implementation of Adam\footnote{\url{https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\#torch.optim.Adam}}, 
with a maximum sequence length of 512 tokens, for 20 epochs, and a batch size of 64. The discriminator parameters we use are those from the epoch with the highest test accuracy (67.4\% for GPT-2-based discriminator, and 67.6\% for DialoGPT-based discriminator). 
% It is also important to realize that the previously seen classifiers from Experiment 1 are not compatible with PPLM, because they need to fit the output distributions of the underlying language models. Furthermore, one of the goals of Experiment 1 was to develop the best performing age-classifiers, which are ultimately used for (1) evaluation of the attribute adherence of the generated responses in Experiment 2, and (2) informing us about the choice of BoW-based attribute models (i.e., most informative unigrams). It also adds to the generality of our results to use a separate classifier for evaluation than the discriminators used as attribute models.


% \len{TODO - rephrase this last paragraph to make it clearer.}

% Convert these steps into a description of how the FB wordlists are constructed:

% Steps taken to create age-specific wordlists (full imbalanced BNC used):
% \begin{itemize}
%     \item Remove all stopwords. List of stopwords from NLTK's English stopword list. \textbf{TODO: does this make sense? What if differences in use of stopwords are strong indicators of an age-group's speech?}
%     \item Order all unique words by frequency per age-group.
%     \item For both lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
%     \item From both sets of words, remove the words that are in the \textit{union} (i.e., the overlapping set) of the young and old sets.
%     \item For both sets, order the words by frequency.
%     \item For both remaining lists, keep the words that account for at least 80\% of the respective cumulative probability densities.
%     \item \textbf{TODO:} remove curse-words?
%     \item Resulting wordlist lengths:
%         \begin{itemize}
%             \item Young (19-29): 90 words
%             \item Old (50 plus): 225 words
%         \end{itemize}
% \end{itemize}

\subsection{Choice of Prompts}

% \len{Motivate choice of prompts.}

% \len{Add table of prompts with BERT probs}

% \len{Explain difference between prompted/unprompted and refer to unprompted results in Appendix (Section \ref{sec:unprompted})}

The prompt is the text on which a language generation model conditions its output. In the specific case of controlled language generation, given a prompt, $\texttt{prompt}$, a predefined style attribute $a$, and some controlled language generation model parameterized by $\theta$, generating a style-controlled piece of text $\textbf{x}$ entails modeling $p_{\theta}(\textbf{x} | a, \texttt{prompt})$. The output distribution of the controlled generation model $p_{\theta}$ is therefore likely to be affected by $\texttt{prompt}$, which could manifest itself as the prompt's style influencing the output text's style. In line with this presumption, \cite{fan-etal-2018-hierarchical} and \cite{lester2021power} show that the content and style of prompts can influence the output of neural text generation models. 

Because our research is about the style of generated dialogue responses, we wish to take into account the effect that the style of the prompt might have. We therefore distinguish between three categories of prompt style: younger, neutral, and older. We use 3 random initializations of BERT$_{FT}$ classifiers from Experiment 1 (different from the one used for evaluation) to assign target probabilities to prompts written by us. A younger prompt is defined as having an average assigned ``younger'' probability (i.e., BERT$_{FT}$'s assigned probability of containing features learned to be associated with the younger age group) of at least 80\%, with a standard deviation no greater than 0.15, to ensure that the prompt is deemed sufficiently characteristic of an age group's speaking style with certainty. The same holds for an older prompt, but for its assigned ``older'' probability. A neutral prompt must have an average assigned younger (or older, as they are complementary values) probability between 40\% and 60\%, with a standard deviation no greater than 0.15. We use 5 prompts per style class, and they are shown, along with their average target probabilities and standard deviations, in Table \ref{tab:prompts_per_class}. 

\begin{table}[h]%{0.5\linewidth}
    \centering
    {\begin{tabular}{r c c c}
    \hline
        \multicolumn{1}{>{\columncolor{white!25}}l}{\tabularCenterstack{l}{\textbf{Prompt}}} & \textbf{Average} $\boldsymbol{P_Y}$ & \textbf{Average} $\boldsymbol{P_O}$ & \textbf{Standard deviation}\\
        \hline
        \hline
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textbf{Young prompts}}} & \cellcolor{red!10} & \cellcolor{red!10} & \cellcolor{red!10}\\
        \hline
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textit{What are your plans this week?}}} & \cellcolor{red!10}0.938 & \cellcolor{red!10}0.062 & \cellcolor{red!10}0.056\\
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textit{What do you wanna eat?}}} & \cellcolor{red!10}0.958 & \cellcolor{red!10}0.042 & \cellcolor{red!10}0.026\\
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textit{Do you have any hobbies?}}} & \cellcolor{red!10}0.963 & \cellcolor{red!10}0.037 & \cellcolor{red!10}0.021\\
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textit{Can I add you on Facebook?}}} & \cellcolor{red!10}0.995 & \cellcolor{red!10}0.005 & \cellcolor{red!10}0.005\\
        \multicolumn{1}{>{\columncolor{red!25}}l}{\tabularCenterstack{l}{\textit{Awesome! I actually haven't been there.} \\ \textit{When did you go? }}} & \cellcolor{red!10}0.995 & \cellcolor{red!10}0.005 & \cellcolor{red!10}0.004\\
        \hline
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textbf{Neutral prompts}}} & \cellcolor{yellow!10} & \cellcolor{yellow!10} & \cellcolor{yellow!10}\\
        \hline
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textit{Good weather we're having.}}} & \cellcolor{yellow!10}0.489 & \cellcolor{yellow!10}0.511 & \cellcolor{yellow!10}0.133\\
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textit{Can we talk?}}} & \cellcolor{yellow!10}0.412 & \cellcolor{yellow!10}0.588 & \cellcolor{yellow!10}0.116\\
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textit{Hi, how's it going?}}} & \cellcolor{yellow!10}0.437 & \cellcolor{yellow!10}0.563 & \cellcolor{yellow!10}0.079\\
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textit{Hello, tell me about your latest holiday.}}} & \cellcolor{yellow!10}0.496 & \cellcolor{yellow!10}0.504 & \cellcolor{yellow!10}0.059\\
        \multicolumn{1}{>{\columncolor{yellow!25}}l}{\tabularCenterstack{l}{\textit{Hey.}}} & \cellcolor{yellow!10}0.574 & \cellcolor{yellow!10}0.426 & \cellcolor{yellow!10}0.046\\
        \hline
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textbf{Old prompts}}} & \cellcolor{blue!10} & \cellcolor{blue!10} & \cellcolor{blue!10}\\
        \hline
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textit{Tell me about your family.}}} & \cellcolor{blue!10}0.061 & \cellcolor{blue!10}0.939 & \cellcolor{blue!10}0.050\\
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textit{Good afternoon.}}} & \cellcolor{blue!10}0.040 & \cellcolor{blue!10}0.960 & \cellcolor{blue!10}0.044\\
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textit{I had a splendid weekend.}}} & \cellcolor{blue!10}0.030 & \cellcolor{blue!10}0.970 & \cellcolor{blue!10}0.034\\
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textit{Hello, how are you?}}} & \cellcolor{blue!10}0.041 & \cellcolor{blue!10}0.959 & \cellcolor{blue!10}0.031\\
        \multicolumn{1}{>{\columncolor{blue!25}}l}{\tabularCenterstack{l}{\textit{Hello, tell me about yourself.}}} & \cellcolor{blue!10}0.025 & \cellcolor{blue!10}0.975 & \cellcolor{blue!10}0.026\\
        % \ \cellcolor{blue!25}\textbf{\textit{Age 50+ style response}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{What would you like to share about?}}\\
    \hline
    \end{tabular}}
    \vspace{3mm}
    \caption{Younger (red rows), neutral (yellow rows), and older (blue rows) prompts used for controlled dialogue generation in Experiment 2. Average $P_Y$ and $P_O$ are the averaged young and old probabilities assigned to the prompts by three initializations of BERT$_{FT}$. }\label{tab:prompts_per_class} % examples generated by NP | DGPT | Discrim | Young and NP | DGPT | Discrim | Old
\end{table}%

The focus of the main quantitative results (covered in Section \ref{sec:exp2_results}) is to see how much the writing style can be adapted towards that of an age group, so it uses neutral prompts, to avoid any unwanted biases induced by the prompts. The effects of using strongly styled prompts are analyzed in Section \ref{subsec:ctg_anal_prompt_class}. It is worth mentioning that it is also possible to use PPLM in an unprompted setting, i.e., unconditioned language generation. In practice, this is done by conditioning the generated output on the \texttt{<|endoftext|>} token. We also present quantitative results for this unprompted setting in the Appendix, in Section \ref{sec:unprompted}.

% \begin{itemize}
%     \item The prompt is the text on which language generation is conditioned.
%     \item In our dialogue response generation setting, the prompt is the first utterance in a single-turn dialogue, and the second utterance is the response generated conditioned on the prompt. 
%     \item The writing style of the prompt can affect the outcome style of the generated response (find reference)
%     \item Because our research is about the style of generated responses, we wish to take into account the effect that the style of the prompt might have.
%     \item We therefore use 3 random initializations of BERT$_{FT}$ classifiers, different from the one used for evaluation, to assign target probabilities to prompts written by us.
%     \item We distinguish between three categories: young, neutral, and old.
%     \item Young is defined as having an average assigned young probability of at least 80\%, with a standard deviation no greater than 0.15.
%     \item Same for old.
%     \item Neutral has an average assigned young probability between 40\% and 60\%, with a standard deviation no greater than 0.15
%     \item We use 5 prompts per category.
%     \item See appendix table for overview of prompts.
%     \item The focus of the main results is to see how much the writing style can be adapted towards that of an age group, so it uses neutral prompts, to avoid any unwanted biases induced by the prompts.
%     \item The effects of using biases prompts are studied in analysis section blablbla
%     \item Difference prompted unprompted and refer to appendix.
% \end{itemize}

% \len{An unprompted model conditions its generated output on the \texttt{<|endoftext|>} token. Maybe just explain this in the appendix section}

\subsection{Experimental Setup of Dialogue Generation Models}
\label{subsec:experimental_details_generation}

% \len{
% \begin{itemize}
%     \item Argue why you choose to use GPT-based models as base LMs for PPLM setup and not best BERT classifier.
%     \item Make my results easier to compare to results in PPLM (and PPCM?) paper.
%     \item BERT is an encoder-based architecture (i.e., more suitable for classification). GPT(-2) is a decoder-based architecture (i.e., more suitable for generation). FIND A REFERENCE FOR THIS STATEMENT OR AT LEAST VERIFY IT.
%     \item exp1 was about comparing model classes (ngram, rnn, transformer). Same type of comparison persists (bow ie ngrams, gpt2 classifier head (transformer))
%     \item EMPHASIZE HOW EXP1 and EXP2 ARE CONNECTED. Here if fits into narrative, but do it somewhere.
% \end{itemize}
% }

% Controlled text generation experiments are performed using PPLM-setups that differ with respect to \textbf{(1)} pre-trained language model (GPT-2 or DialoGPT), \textbf{(2)} type of attribute model (BoW or discriminator), \textbf{(3)} attribute (young, old, or uncontrolled), and \textbf{(4)} whether the model was prompted or not. An unprompted model conditions its generated output on the \texttt{<|endoftext|>} token. 
% BoW-based configurations can also differ in their wordlist extraction method (most informative unigrams or frequency-based). 

Every PPLM-setup generates 30 sequences per output length 6, 12, 24, 30, 36, 42, 48, 54, and 60 tokens. 
% This means that the averaged results presented in Section \ref{sec:exp2_results} are averaged over $N = 30 \cdot 9 = 270$ samples. 
The lengths are chosen to provide sufficiently distanced sub-samples for later analyses about the effects of generated response length on the quality and control of our PPLM-setups.
Sub-sample sizes of 30 are chosen to satisfy the Central Limit Theorem (CLT), making it possible to assume the distribution of sub-sample averages $\bar{X}_n$ approximates the normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$ \citep{CLT2008springer}, where $n$ is the sub-sample size.
% The results in Table \ref{tab:ctg_results_ws} are averaged over $N = 30 \cdot 9 = 270$ samples.
% \len{Motivate the choice of lengths and sample size} 
Note that perturbations of the base language model's output can affect the controlled sequence length, so the final sequence length may differ by a few tokens from the given one. All other PPLM-parameters are kept at their default values, as recommended by \cite{dathathri2019plug}. 

Each reported PPLM-configuration represents the best initialization, if the term applies. The pre-trained language models are kept equal across configurations, as using different initializations of these large models is infeasible, and would defeat the purpose of PPLM. A BoW-based setup uses a single list of words as an attribute model, thereby not having random parameters to initialize. And finally, discriminator-based setups use comparatively small linear classifiers (a few hundred parameters), the initialization effects of which have been found to be negligible on performance.

\subsection{Evaluation}

The generated sequences are evaluated along two main axes: fluency and control. Fluency refers to the degree to which a text passage appears natural, grammatical, and non-repetitive. Control is the extent to which the produced language resembles that of the attribute being controlled for. Evaluation is done using a set of automated metrics. Fluency is measured automatically by perplexity (denoted ppl) 
% with respect to a different language model, GPT-1 \citep{radford2018improving}, 
expressed as

\begin{equation}
    \text{ppl}(\textbf{x}) = \exp \left\{ - \frac{1}{t} \sum_{i}^t \ln p_{\theta}(x_i | x_{<i})\right\}    
\end{equation}

where $\textbf{x}$ represents a sequence of tokens, $t$ is sequence length, $x_i$ is the $i$-th token, and $\theta$ denotes GPT's parameters. Perplexity is a measure of a language model's uncertainty when posed with the task of predicting a succession of words. Assuming a language model to be a reliable representation of relationships within a natural language, low perplexity can serve as a rough proxy for fluency of a text. However, a major caveat of perplexity is that it only measures uncertainty w.r.t. one language model, making it less generalizable. To slightly reduce this effect, we choose to evaluate perplexity with respect to a different language model (GPT-1 \citep{radford2018improving}) than the one used for generation (GPT-2 or DialoGPT).
Furthermore, the normalized number of distinct unigrams (Dist-1), bigrams (Dist-2), and trigrams (Dist-3), are used as measures of text diversity. 

Experiment 1's best BERT-classifier's accuracy on a set of sequences generated by a single generation model is used as an automated measure of attribute control. It can be seen as a proxy for control, because it indicates how resemblant of an age group's speaking style a generated text is deemed to be.

Two types of baselines are used when evaluating text generation performance: an uncontrolled pre-trained model baseline (i.e., a GPT-2-baseline or DialoGPT-baseline), and a corpus-specific baseline. 
% The pre-trained model baseline refers to the uncontrolled language model setting, being either GPT-2 or DialoGPT. 
Therefore all controlled generation models that use GPT-2 as their language model, should be compared to the uncontrolled GPT-2 baseline. The same holds for DialoGPT. The second type of baseline combines the underlying language model with a bag-of-words consisting of the 100 most common words in the balanced dialogue corpus, irrespective of age. This setting is included to give an indication of how biased the balanced BNC's frequently occurring words might be towards a specific age group.

% We use state-of-art models, GPT-2 \citep{radford2019language} and DialoGPT \citep{zhang2019dialogpt}, for (controlled) language generation. 
The deliberate choice to use BERT-based models for classification (and evaluation of generated output), and GPT-based models for generation is motivated by the following reasons. BERT's encoder-based bidirectional architecture makes it more suitable for sequence classification than for generation \citep{devlin-etal-2019-bert}. By similar reasoning, GPT's decoder-based structure makes it a more suitable choice for generation tasks. Furthermore, Experiment 1's best classifier is a fully fine-tuned BERT model (ca 110M parameters). However, it is computationally highly expensive to fine-tune GPT-2-medium (ca 345M parameters) which makes it infeasible given our computational resources. Finally, using a separate model class (i.e., BERT) for evaluation of GPT-based generation models makes the results more generalizable.

% \len{TODO - Describe human evaluation}


% \textit{How do you measure how representative of the stylistic attribute $a$ the generated text is? Specifically, is the generated text similar to that of the age-group you're controlling for?}

% \paragraph{Fluency}

% \textit{How do you measure how grammatically correct and fluent the generated texts are?}

% \begin{itemize}
%     \item Perplexity
%         \begin{itemize}
%             \item \textbf{TODO:} When explaining and motivating the use of perplexity as an evaluation metric for (controlled) language models, re-read this piece of documentation about perplexity by Hugging face: \url{https://huggingface.co/transformers/perplexity.html}
            
%             \item $\text{PPL}(\textbf{x}) = \exp \left\{ - \frac{1}{t} \sum_{i}^t \ln p_{\theta}(x_i | x_{<i})\right\}$
%         \end{itemize}
%     \item Also checkout this blogpost by The Gradient about Evaluation Metrics for Language Modeling (NB: contains BibTeX citation at the bottom): \url{https://thegradient.pub/understanding-evaluation-metrics-for-language-models/}
% \end{itemize}

% \subsubsection{Human evaluation}

% \textbf{TODO:} find humans.