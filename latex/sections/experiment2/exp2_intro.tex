% \section{Introduction}

% \len{TODO - Emphasize the connection between Experiment 1 and Experiment 2: \textit{we’ve shown that it’s possible to classify younger vs older age groups based on language; we now want to check whether it’s possible to generate language than encompasses these features (2). We’ll use state-of-art models for language generation (1)….}}

% \len{Sandro's advice for this introduction and chapter: \textit{I think something important to stress is the connection at the dialogue-level: we’ve used BNC to train the classifier, which is a dialogue dataset. Now, in generating, I think it’s important that we generate something similar to a dialogue turn, i.e., a response to a dialogue “prompt”.}}

% \len{TODO - Motivate the choice of GPT for generation, and BERT for classification}

% \len{
% \begin{itemize}
%     \item Here make the argument for using the best BERT-ft classifier from Exp1
%     \item BERT more suitable for classification.
%     \item Exp1 uses fully finetuned BERT (ca 110M parameters). For GPT2 (ca 345M parameters) this is infeasible with my computational resources.
%     \item Using a separate model class (i.e., BERT) for evaluation of GPT-based generation models makes the results more generalizable.
% \end{itemize}
% }

\section{Introduction}

\subsection{Recap of previous chapter and Connection to Experiment 1}

% In Experiment 1 (Chapter \ref{ch:experiment1}), we showed that it is possible to classify language by speakers and writers from different age groups based on linguistic features.
In Experiment 1, we studied the extent to which text-based NLP models are able to detect speaker and writer age-related linguistic features in dialogue and discourse data. We then investigated which features drive the predictions made by those models. We showed that a fine-tuned version of BERT, BERT$_{FT}$, is capable classifying language by speakers from different age groups based on linguistic features. We also found that much simpler models based on $n$-grams achieve classification performance comparable to that of BERT$_{FT}$, suggesting that, in dialogue, local features (i.e., at the lexical level) can also be indicative of a speaker's age. This was shown to be the case, as both lexical and stylistic cues seem to be informative to these models when predicting speaker age from dialogue utterances.

Now, in Experiment 2, we aim to test whether it is possible to generate dialogue responses that possess these age-indicative features, identified in Experiment 1. 
% We also use the insights gained from Experiment 1 (e.g., about the applicability of $n$-gram-based representations of age-specific speaking style) to inform us about the development of controlled dialogue generation models. 
More specifically, we seek to use Plug-and-Play language models (PPLM) \citep{dathathri2019plug} to develop controlled dialogue generation models that can produce conversation responses contain linguistic elements of the speaking style of a certain age group to the extent that it would be classified as such by Experiment 1's best classifier. The use of the PPLM method is motivated by its capability to control the style of the text generated by a large pre-trained Transformer-based language model (e.g., GPT-2 or DialoGPT), without the computationally expensive requirement to re-train or fine-tune it. Namely, the Plug-and-Play approach uses substantially smaller and less costly to train attribute models to make perturbations to the large language model's activation space, thereby shifting its output distribution towards a desired style. It is important to realize that the PPLM method is fundamentally different from fine-tuning in that it leaves the parameters of underlying language model unchanged.

The analyses of Experiment 1 largely concerned a comparison of performance between simpler $n$-gram-based models and more sophisticated neural discriminators (i.e., BERT). This comparison is continued throughout Experiment 2, because now we aim to compare the applicability of PPLM for age-adaptation when adaptation is enforced by an aforementioned attribute models, which can be a simple bag-of-words (BoW) model, or a more complex neural discriminator. Using PPLM to control the writing style of generated output text using the former is called \textit{BoW-based control}, and using the latter is referred to as \textit{discriminator-based control}. We also use the insights gained from Experiment 1 (e.g., about the applicability of $n$-gram-based representations of age-specific speaking style) to inform us about the development of attribute models. Furthermore, in Experiment 1, the classifiers are trained on the spoken component of the BNC, which is a dialogue dataset. Now, in Experiment 2, it is the goal that we generate something similar to a dialogue turn, i.e., a response to a dialogue prompt, in the style of a certain age group. Table \ref{tab:dialogue_gen_example_exp2} shows examples of how age-adapted dialogue responses to a prompt might look.

\begin{table}[h]%{0.5\linewidth}
    \centering
    {\begin{tabular}{r l}
    \hline
        \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Can we talk?}}\\
        \hline 
        \cellcolor{red!25}\textbf{\textit{Age 19-29 style response}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{Like about what?}}\\
        \ \cellcolor{blue!25}\textbf{\textit{Age 50+ style response}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{What would you like to share about?}}\\
        % \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        % \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
        % \hline
        % \ \cellcolor{yellow!25}\textbf{\textit{PROMPT}} & \multicolumn{1}{>{\columncolor{yellow!10}}l}{\tabularCenterstack{l}{Hi, how's it going?}}\\
        % \hline 
        % \cellcolor{red!25}\textbf{\textit{GPT-2$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        % \ \cellcolor{blue!25}\textbf{\textit{GPT-2$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{ I've had my first \\ surgery recently.}}\\
        % \ \cellcolor{red!25}\textbf{\textit{DialoGPT$_{Young}$}} & \multicolumn{1}{>{\columncolor{red!10}}l}{\tabularCenterstack{l}{...}}\\
        % \ \cellcolor{blue!25}\textbf{\textit{DialoGPT$_{Old}$}} & \multicolumn{1}{>{\columncolor{blue!10}}l}{\tabularCenterstack{l}{...}}\\
    \hline
    \end{tabular}}
    \vspace{3mm}
    \caption{Examples of controlled dialogue responses to a prompt. The responses have been generated by DialoGPT after being controlled to generate younger sounding (red row) and older sounding (blue row) language.}\label{tab:dialogue_gen_example_exp2} % examples generated by NP | DGPT | Discrim | Young and NP | DGPT | Discrim | Old
\end{table}%

It is also important to realize that the previously seen classifiers from Experiment 1 are not compatible with PPLM, because the method requires a bag-of-words or linear discriminator. Furthermore, one of the goals of Experiment 1 was to develop the best performing age-classifiers, which are ultimately used for (1) evaluation of the attribute adherence of the generated responses in Experiment 2, and (2) informing us about the choice of BoW-based attribute models (i.e., most informative unigrams). It also adds to the generality of our results to use a separate classifier for evaluation than the discriminators used as attribute models.

Lists of unigrams are used as BoWs (as opposed to lists of trigrams from the best performing $n$-gram-based classifier in Table \ref{tab:bnc_classification_ws}), because the PPLM setup is not compatible with lists of $n$-grams for $n > 1$, as it relies on perturbations at the unigram-level. Making a PPLM-system compatible with, e.g., trigrams, would amount of retraining the entire underlying language model (like GPT-2), thereby defeating the purpose of PPLM, i.e., leveraging large LMs for controllability, without incurring significant re-training costs. However by allowing the best-performing classifiers to inform decisions about generation, we use the best unigram-based classifier's list of most informative features as an attribute model.

% We use state-of-art models, GPT-2 \citep{radford2019language} and DialoGPT \citep{zhang2019dialogpt}, for (controlled) language generation. 
% The deliberate choice to use BERT-based models for classification (and evaluation of generated output), and GPT-based models for generation is motivated by the following reasons. BERT's encoder-based bidirectional architecture makes it more suitable for sequence classification than for generation \citep{devlin-etal-2019-bert}. By similar reasoning, GPT's decoder-based structure makes it a more suitable choice for generation tasks. Furthermore, Experiment 1's best classifier is a fully fine-tuned BERT model (ca 110M parameters). However, it is computationally highly expensive to fine-tune GPT-2-medium (ca 345M parameters) which makes it infeasible given our computational resources. Finally, using a separate model class (i.e., BERT) for evaluation of GPT-based generation models makes the results more generalizable.

% This chapter covers the methodology, experimental setup, results, and analyses relating to Experiment 2 of this thesis. 
\subsection{Research Objectives, Hypotheses, and Contributions}

Experiment 2 concerns a Plug-and-Play approach to age-adaptive dialogue generation.
The aim is to use PPLM for age-adaptive dialogue generation. We hypothesize this to be possible to the extent that a text-based NLP model can detect in the generated dialogue responses linguistic features learned to be associated with certain age groups. Furthermore, we expect discriminator-based control to be more detectable and invasive at the structural level than BoW-based control. We also expect BoW-based control to take a smaller toll on the fluency of generated response than discriminator-based control.
% In other words, I seek to use large pre-trained language models for controllable dialogue generation, using activation-space perturbations instead of fine-tuning. Either GPT-2-medium or DialoGPT-medium is used as a base language model. And adaptation of the generated language to a certain age group is achieved using either a linear discriminator or a bag-of-words (BoW), trained or empirically constructed from the dialogue dataset (See Section \ref{subsec:dialogue_dataset}). Using GPT-2 and DialoGPT as baselines, a set of automated and human evaluation metrics are used to evaluate the fluency and control of the proposed models. 
% I expect discriminator-based control to be more detectable and fine-grained than BoW-based control. I also expect BoW-based control to take a smaller toll on fluency.

The Plug-and-Play approach has previously been used by \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug} for language generation, controlled for very concrete writing styles and topics (e.g., negative/positive sentiment, politics, religion, business, tech). We apply the PPLM method to a novel problem, that is, age-related language generation conditioned on dialogue data. More concretely, we extend the work of \cite{dathathri2019plug} in several important ways: (1) we control the generated language for more abstract writing styles, i.e., age-related linguistic style; (2) we use PPLMs to generate dialogue responses; (3) we propose two empirical methods for age-specific BoW development, as opposed to the manually constructed BoWs used in previous work; (4) we carry out in depth analyses about the relationships between evaluation measures of dialogue response quality and style; (5) finally, we address and study the problem of writing style biases induced by the prompt's style.

% \len{EXAPND ON THIS. However, a fully interactive plug-and-play conversational model is out of the scope of this thesis.}

% \begin{itemize}
%     \item However, a fully interactive plug-and-play conversational model is out of the scope of this thesis.
%     \item PPCM is about interactivity. Implementing a PPCM is beyond the scope because of the engineering requirements: generating massive amounts of dialogue datasets, developing and training residual adapters.
%     \item This thesis is about (1) establishing whether age-related linguistic features are automatically detectable, and (2) if we can enforce them as a writing style of PPLM-generated dialogue responses. Interactivity (i.e., speed of response generation) is a separate engineering problem and beyond the scope.
% \end{itemize}

It must be noted that the activation perturbation steps necessary for PPLMs to exert influence on the style of generated text make too slow to be suitable for online interactive conversational applications. This problem is addressed by \cite{madotto-etal-2020-plug}, who propose Plug-and-Play Conversational Model (PPCM), an extension of PPLM that uses separately trained residual adapter modules to speed up the PPLM-method, making it suitable for real-time dialogue response generation. Despite their work also being about controlled dialogue generation, our work distinguishes itself in important ways. They control their output for the same styles and topics as \cite{dathathri2019plug}, and do not experiment with more abstract writing styles, like age-related linguistic characteristics. Overall, this thesis is about establishing whether age-related linguistic features are automatically detectable, and if we can enforce them as a writing style of dialogue responses using PPLMs. The work of \cite{madotto-etal-2020-plug} addresses the problem of interactivity (i.e., speed of response generation) requires solving a separate, very valuable, engineering problem, which is unfortunately beyond the scope of this thesis. Nevertheless, our previously mentioned contributions still hold.

The rest of this chapter is structured as follows. Section \ref{sec:exp2_methods} contains detailed descriptions of the most important methods and models involved: using Transformers for dialogue modeling, and Plug-and-Play Language Models. This section ends with the experimental details, details about attribute model development and the choice of prompts, and an explanation of the chosen evaluation metrics. The results of the language generation experiments are presented and interpreted in Section \ref{sec:exp2_results}, followed by the outcomes of various quantitative and qualitative analyses in Section \ref{sec:exp2_analyses}. A separate section about data is omitted in this chapter, because we use the previously mentioned dialogue dataset for these experiments. The reader is directed to Sub-section \ref{subsec:dialogue_dataset} for a detailed description of that corpus, and our pre-processing steps.

% \len{TODO - Does it suffice to redirect the reader to the previous sub-section about the dialogue dataset? Or am I missing an explanation about the dialogue corpus in this chapter?}

