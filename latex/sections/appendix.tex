\section{Wordlists for BoW-based Approaches}
\label{sec:wordlists}

\len{TODO - Censor foul language?}

\paragraph{100 Most Informative Unigrams - Young (19-29)} um, cool, shit, hmm, uni, cute, tut, massive, awesome, gym, bitch, lol, grand, pizza, like, excited, yawn, Korea, cigarette, fuck, fairness, Jesus, annoying, Facebook, quicker, definitely, guess, Sunderland, oo, wanna, mountain, scared, piss, love, miss, Middlesbrough, mhm, specifically, ooh, website, roundabout, photo, nope, blanket, management, ridiculous, mental, pregnant, beers, hate, log, fucking, cry, cheaper, skinny, plural, burger, hilarious, hint, drunk, fridge, cousin, coke, genuinely, James, mates, smaller, option, balance, saving, basically, leather, nev, shut, frig, mate, yay, invite, maid, nickname, badly, garlic, CD, jokes, Uzbekistan, boyfriend, date, added, Manchester, blah, shitty, lang, tempted, stadium, wee, eh, baking, city, honestly, exam

\paragraph{100 Most Informative Unigrams - Old (50 plus)} ordinary, Chinese, wonderful, yes, tend, father, photographs, vegetables, hospice, operation, shed, pension, areas, mother, hanging, hospices, glasses, chap, anyhow, tank, surgery, container, cheers, born, church, pain, several, workshop, right, horses, building, extraordinary, vegetarian, biscuit, americano, engine, luck, paint, emperor, lipsy, trombone, occasional, supper, lord, architect, council, roast, schools, bath, asbestos, endometrial, concrete, poodle, recall, diabetes, misty, report, heavens, enormous, lawn, potatoes, email, junk, scabies, mousse, Ebola, churches, sewing, plants, rackets, marmalade, engineering, furniture, photograph, sandwiches, unemployment, xylophone, Piccadilly, flu, claim, arab, nineteen, forgotten, sensible, blancmange, spencer, yards, emails, yellow, scruffy, fungi, garden, boiler, lodge, mostly, Robson, tricky, shark, robin, contracture

\paragraph{Frequency-based Young (19-29)} um,
shit, cool, fucking, definitely, guess, friends, everyone, literally, dad, sounds, weekend, loads, watch, fair, fuck, amazing, friend, ha, huh, hate, fun, stay, girl, holiday, blah, hours, uni, month, horrible, massive, Friday, stupid, film, parents, thirty, spend, mate, honest, change, hope, yourself, annoying, wear, wait, ridiculous, anyone, Saturday, tea, dinner, sit, crazy, hell, pound, nine, expensive

\paragraph{Frequency-based Old (50 plus)} building, may, water, mother, perhaps, door, lots, business, cancer, area, although, worked, open, cut, number, under, young, nineteen, everybody, garden, church, case, shop, children, certainly, set, coffee, email, gave, white, along, doctor, hear, often, possibly, group, father, outside, wonderful, taken, seem, places, green, given, hand, early, women, space, front, language, dear, light, huge, supposed, country, hospital, otherwise, asked, putting, bits, gosh, wall, woman, almost, particularly, across, word, age, rest, flat, turned, decided, finished, needed, red, bin, hospice, running, slightly, its, middle, local, percent, Chinese, paper, check, high, milk, piece, near, nobody, usually

\section{Where to put these?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_bert_test_dt_27_May_2021.png}
    \caption{Confusion matrix BERT age classifier on balanced BNC \textbf{test} set.}
    \label{fig:cm_bert_bnc_rb}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_lstm_test_dt_24_May_2021_09_24_30.png}
    \caption{Confusion matrix LSTM age classifier on balanced BNC \textbf{test} set.}
    \label{fig:cm_lstm_bnc_rb}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_lstm_test_dt_24_May_2021_10_03_19.png}
    \caption{Confusion matrix bi-LSTM age classifier on blog corpus \textbf{test} set.}
    \label{fig:cm_lstm_blog}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_3_gram_bnc_rb_dt_08_Jun_2021_12_05_02.png}
    \caption{Confusion matrix for best trigram age classifier on \textbf{balanced} BNC \textbf{test} set.}
    \label{fig:cm_trigram_bnc_rb}
\end{figure}

\section{Age Discrimination on the Imbalanced British National Corpus \len{Do we even need these plots?}}
\label{age_disc_bnc}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_2_gram_bnc_dt_08_Jun_2021_11_33_16.png}
    \caption{Confusion matrix for best bigram age classifier on BNC test set.}
    \label{fig:cm_bigram_bnc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/cm_lstm_test_dt_22_May_2021_12_41_50.png}
    \caption{Confusion matrix bi-LSTM age classifier on BNC test set.}
    \label{fig:cm_lstm_bnc}
\end{figure}

\begin{table}[b!]
    \centering
    \begin{tabular}{@{}l l @{\hspace*{25pt}} l l@{}}
    \toprule
    \multicolumn{2}{c}{19-29} & \multicolumn{2}{c}{50+}\\
    \textbf{coef.} & \textbf{n-gram} & \textbf{coef.} & \textbf{n-gram}\\
    \midrule
    -3.19 & um & 2.29 & yes\\
    -2.91 & cool & 2.21 & wonderful\\
    -2.70 & s**t & 1.91 & building\\
    -2.25 & cute & 1.86 & right right\\
    -2.15 & uni & 1.80 & something like\\
    -2.14 & hmm & 1.73 & garden\\
    -1.97 & wanna & 1.69 & right\\
    -1.93 & f**k & 1.68 & ordinary\\
    -1.91 & like & 1.67 & shed\\
    -1.85 & massive & 1.63 & operation\\
    -1.83 & yeah course & 1.58 & born\\
    -1.81 & love & 1.57 & mother\\
    -1.79 & tut & 1.55 & photographs\\
    -1.74 & b***h & 1.51 & email\\
    -1.68 & like oh & 1.08 & anything like\\
    \bottomrule
    \end{tabular}
    \caption{\len{Excluding stopwords.} For each age group, top 15 most informative $n$-grams used by the trigram model. \textbf{coef.} is the coefficient (and sign) of the corresponding $n$-gram for the logistic regression model: the higher its absolute value, the higher the utterance's odds to belong to one age group.
    % Greater absolute value of \textbf{coef.} indicates occurrence of the $n$-gram results in the model assigning higher odds to the utterance belonging to a certain age group.
    * indicates masking of foul language.}
    % present in the dialogue.}
    \label{tab:top_ngrams}
\end{table}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{@{}l l @{\hspace*{25pt}} l l@{}}
%     \toprule
%     \multicolumn{2}{c}{19-29} & \multicolumn{2}{c}{50+}\\
%     \textbf{coef.} & \textbf{n-gram} & \textbf{coef.} & \textbf{n-gram}\\
%     \midrule
%     -3.20 & um & 2.37 & yes\\
%     -2.84 & cool & 2.12 & you know\\
%     -2.58 & s**t & 2.09 & wonderful\\
%     -2.12 & hmm & 1.90 & how weird\\
%     -2.09 & like & 1.84 & chinese\\
%     -2.02 & was like & 1.73 & right\\
%     -1.96 & love & 1.71 & building\\
%     -1.96 & as well & 1.66 & right right\\
%     -1.88 & as in & 1.55 & so erm\\
%     -1.84 & cute & 1.43 & mm mm\\
%     -1.82 & uni & 1.41 & cheers\\
%     -1.79 & massive & 1.39 & shed\\
%     -1.79 & wanna & 1.37 & pain\\
%     -1.79 & f**k & 1.36 & we know\\
%     -1.72 & tut & 1.08 & yeah exactly\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Including stopwords.} For each age group, top 15 most informative $n$-grams used by the trigram model. \textbf{coef.} is the coefficient (and sign) of the corresponding $n$-gram for the logistic regression model: the higher its absolute value, the higher the utterance's odds to belong to one age group.
%     % Greater absolute value of \textbf{coef.} indicates occurrence of the $n$-gram results in the model assigning higher odds to the utterance belonging to a certain age group.
%     * indicates masking of foul language.}
%     % present in the dialogue.}
%     \label{tab:top_ngrams_ws}
% \end{table}

% \begin{table*}[h]
%     \centering
%     % \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l c c c}
%     \toprule
%     \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(19-29)}$  & $\boldsymbol{F}_1^{(50+)}$ \\ 
%     % -plus)}$ \\
%      & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better \\
%     \midrule
%     Random
%     % Baseline (random guessing) 
%     & 0.500
%     % (0.000) 
%     & 0.500
%     % (0.000) 
%     & 0.500 
%     % (0.000)
%     \\ \midrule
%     unigram & 0.701 (0.007) & 0.708 (0.009)  & 0.693 (0.004)\\
%     bigram & 0.719 (0.002) & 0.724 (0.003) & 0.714 (0.003)\\
%     trigram &  \textcolor{blue}{0.722} (0.001) & \textcolor{blue}{0.727} (0.003) & \textcolor{blue}{0.717} (0.001)\\ \midrule
%     LSTM &  0.693 (0.003) & 0.696 (0.005) & 0.691 (0.007)\\
%     BiLSTM & 0.691 (0.009) & 0.702 (0.017) & 0.679 (0.007) \\ \midrule
%     BERT$_{frozen}$
%     % -base uncased (frozen) 
%     & 0.675 (0.003) & 0.677 (0.008) & 0.673 (0.010)\\
%     BERT$_{FT}$
%     % -base uncased (fine-tuned) 
%     & \textbf{0.729} (0.002) & \textbf{0.730} (0.011) & \textbf{0.727} (0.010)\\
%     % \hline
%     % GPT-2 medium (frozen) & 0.671 (0.003) & 0.665 (0.014) & 0.675 (0.017) \\
%     \bottomrule
%     \end{tabular}
%     % }
%     \caption{Dialogue dataset \len{Including stopwords.}. Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest.}
%     \label{tab:bnc_classification_ws}
% \end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l c c c c}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(13-17)}$ & $\boldsymbol{F}_1^{(23-27)}$ & $\boldsymbol{F}_1^{(33+)}$\\
    % -plus)}$\\
     & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    % Baseline (
    Majority class
    % ) 
    & 0.472
    % (0.000) 
    & * & 0.642
    % (0.000)
    & *\\
    % Best model by \citeauthor
    \citet{schler2006effects} & 0.762 & 0.860 & 0.748 & 0.504 \\
    \midrule
    unigram & 0.601 (0.001) & 0.764 (0.001) & 0.704 (0.001) & 0.498 (0.003)\\
    bigram & 0.625 (0.001) & \textcolor{blue}{0.790} (0.001) & \textcolor{blue}{0.712} (0.001) & \textcolor{blue}{0.518} (0.001)\\
    trigram & 0.623 (0.001) & \textcolor{blue}{0.790} (0.001) & 0.712 (0.002) & 0.498 (0.002)\\
    \midrule
    LSTM & \textcolor{blue}{0.663} (0.005) & 0.748 (0.003) & 0.664 (0.010) & 0.502 (0.004) \\
    BiLSTM & 0.618 (0.008) & 0.732 (0.003) & 0.579 (0.016) & 0.509 (0.004)\\
    \midrule
    BERT$_{frozen}$
    & 0.623 (0.002) & 0.658 (0.006) & 0.678 (0.007) & 0.256 (0.041)\\
    BERT$_{FT}$
    % -base-uncased (fine-tuned)
    & \textbf{0.742} (0.010) & \textbf{0.813} (0.007) & \textbf{0.749} (0.013) & \textbf{0.592} (0.009)\\
    % \hline
    % GPT-2 medium (frozen) & 0.621 (0.003) & 0.654 (0.012) & 0.678 (0.009) & 0.247 (0.032)\\
    \bottomrule
    \end{tabular}
    \caption{Discourse dataset. \len{Including stopwords} Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest. *: $F_1$ is actually $0/0$.}
    % \san{If we agree with the abbreviations used in this table, we could use them also in Table 3}}
    \label{tab:blog_classification}
\end{table*}

\begin{table*}[h]
    \centering
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{l c c c}
    \toprule
    \textbf{Model} & \textbf{Accuracy} & $\boldsymbol{F}_1^{(19-29)}$  & $\boldsymbol{F}_1^{(50+)}$ \\ 
    % -plus)}$ \\
     & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better \\
    \midrule
    Random
    % Baseline (random guessing) 
    & 0.500
    % (0.000) 
    & 0.500
    % (0.000) 
    & 0.500 
    % (0.000)
    \\ \midrule
    unigram & 0.702 (0.006) & 0.713 (0.006)  & 0.690 (0.006)\\
    bigram & 0.703 (0.006) & 0.713 (0.005) & 0.693 (0.008)\\
    trigram &  \textcolor{blue}{0.709} (0.007) & \textbf{0.718} (0.007) & 0.700 (0.008)\\ \midrule
    LSTM & 0.696 (0.005) & 0.689 (0.018) & \textcolor{blue}{0.701} (0.016)\\
    BiLSTM & 0.684 (0.007) & 0.688 (0.018) & 0.679 (0.016) \\ \midrule
    BERT$_{frozen}$
    % -base uncased (frozen) 
    & 0.673 (0.005) & 0.679 (0.013) & 0.667 (0.018)\\
    BERT$_{FT}$
    % -base uncased (fine-tuned) 
    & \textbf{0.710} (0.006) & \textcolor{blue}{0.717} (0.007) & \textbf{0.703} (0.014)\\
    % \hline
    % GPT-2 medium (frozen) & 0.671 (0.003) & 0.665 (0.014) & 0.675 (0.017) \\
    \bottomrule
    \end{tabular}
    % }
    \caption{Dialogue dataset \len{Excluding stopwords}. Test set results averaged over 5 random initializations. Format: \textit{average metric (standard error)}. Values in \textbf{bold} are the highest in the column; in \textcolor{blue}{blue}, the second highest.}
    \label{tab:bnc_classification}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    Baseline*** & 27.45 ($\pm$7.27) & 0.90 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 57.5\%\\
    \midrule
    B$_{100MCW}$*** & 26.68 ($\pm$8.77) & 0.89 ($\pm$0.10) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 51.7\%\\
    B$_{Y, FB}$ & 27.11 ($\pm$7.45) & \textbf{0.91} ($\pm$0.09) & \textbf{0.92} ($\pm$0.04) & \textbf{0.87} ($\pm$0.09) & 68.3\%\\
    B$_{O, FB}$ & 25.99 ($\pm$6.41) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & 0.86 ($\pm$0.09) & 62.5\%\\
    B$_{Y, 100MIU}$ & 28.48 ($\pm$11.96) & 0.88 ($\pm$0.12) & 0.91 ($\pm$0.06) & 0.86 ($\pm$0.10) & 69.2\%\\
    B$_{O, 100MIU}$ & \textbf{25.57} ($\pm$7.44) & 0.88 ($\pm$0.11) & 0.92 ($\pm$0.05) & \textbf{0.87} ($\pm$0.09) & 58.3\%\\
    \midrule
    D$_{Y, GPT2}$ & 33.02 ($\pm$12.24) & 0.85 ($\pm$0.16) & 0.89 ($\pm$0.07) & 0.83 ($\pm$0.12) & 73.9\%\\
    D$_{O, GPT2}$ & 32.86 ($\pm$18.08) & 0.80 ($\pm$0.21) & 0.84 ($\pm$0.13) & 0.79 ($\pm$0.19) & 63.3\%\\
    % D$_{Y, GPT2*}$ & 30.98 ($\pm$13.95) & 0.86 ($\pm$0.15) & 0.90 ($\pm$0.06) & 0.84 ($\pm$0.11) & \textbf{80.0\%}\\
    % D$_{O, GPT2*}$ & 34.81 ($\pm$26.76) & 0.84 ($\pm$0.17) & 0.85 ($\pm$0.13) & 0.77 ($\pm$0.23) & 75.8\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Excluding stopwords.} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Young and old accuracy are the assigned probabilities of belonging to the young or old age categories.}
    \label{tab:ctg_results}
\end{table*}


\section{Placeholders for Final CTG Results Tables}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & & & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & 27.50 ($\pm$6.58) & 0.87 ($\pm$0.09) & 0.94 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.62 ($\pm$0.42) & 0.38 & -\\
    G-100MCW & 27.56 ($\pm$6.60) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.05) & 0.63 ($\pm$0.42) & 0.37 & -\\
    \midrule
    G-B$_{FB,Y}$ & 27.91 ($\pm$7.18) & 0.87 ($\pm$0.10) & 0.93 ($\pm$0.05) & 0.90 ($\pm$0.06) & 0.69 ($\pm$0.41) & 0.31 & 70.4\%\\
    G-B$_{FB,O}$ & 27.58 ($\pm$7.07) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.58 ($\pm$0.42) & 0.42 & 43.0\%\\
    G-B$_{100MIU,Y}$ & 28.37 ($\pm$7.31) & 0.87 ($\pm$0.09) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.67 ($\pm$0.41) & 0.33 & 67.4\%\\
    G-B$_{100MIU,O}$ & 27.25 ($\pm$6.15) & 0.87 ($\pm$0.09) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.62 ($\pm$0.42) & 0.38 & 37.4\%\\
    \midrule
    G-D$_{Y}$ & 32.09 ($\pm$18.98) & 0.77 ($\pm$0.20) & 0.86 ($\pm$0.13) & 0.84 ($\pm$0.15) & 0.66 ($\pm$0.43) & 0.34 & 67.8\%\\
    G-D$_{O}$ & 47.15 ($\pm$47.56) & 0.73 ($\pm$0.24) & 0.75 ($\pm$0.28) & 0.75 ($\pm$0.27) & 0.24 ($\pm$0.36) & 0.76 & 74.3\%\\
    \midrule
    \midrule
    D-baseline & 37.52 ($\pm$12.06) & 0.86 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.85 ($\pm$0.10) & 0.76 ($\pm$0.37) & 0.24 & -\\
    D-100MCW & 37.80 ($\pm$10.89) & 0.85 ($\pm$0.14) & 0.89 ($\pm$0.10) & 0.85 ($\pm$0.10) & 0.82 ($\pm$0.33) & 0.18 & -\\
    \midrule
    D-B$_{FB,Y}$ & 38.53 ($\pm$12.64) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.10) & 0.82 ($\pm$0.33) & 0.18 & 83.0\%\\
    D-B$_{FB,O}$ & 37.85 ($\pm$11.17) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.78 ($\pm$0.35) & 0.22 & 21.5\%\\
    D-B$_{100MIU,Y}$ & 38.67 ($\pm$11.70) & 0.88 ($\pm$0.11) & 0.91 ($\pm$0.07) & 0.86 ($\pm$0.10) & 0.87 ($\pm$0.28) & 0.13 & 88.5\%\\
    D-B$_{100MIU,O}$ & 37.91 ($\pm$12.27) & 0.87 ($\pm$0.11) & 0.90 ($\pm$0.07) & 0.85 ($\pm$0.10) & 0.79 ($\pm$0.34) & 0.22 & 21.9\%\\
    \midrule
    D-D$_{Y}$ & 42.01 ($\pm$16.94) & 0.90 ($\pm$0.12) & 0.86 ($\pm$0.14) & 0.77 ($\pm$0.22) & 0.86 ($\pm$0.29) & 0.14 & 85.9\%\\
    D-D$_{O}$ & 41.17 ($\pm$20.72) & 0.87 ($\pm$0.12) & 0.89 ($\pm$0.13) & 0.83 ($\pm$0.16) & 0.43 ($\pm$0.41) & 0.57 & 56.7\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Neutral prompt} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. $\boldsymbol{\bar{P}_Y}$ and $\boldsymbol{\bar{P}_O}$ are the respective average young and old probabilities assigned by the best BERT$_{FT}$. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_neutral_prompt}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}(Young)}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & \textbf{27.50} ($\pm$6.58) & 0.87 ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.62 ($\pm$0.42) & -\\
%     G-100MCW & \textcolor{blue}{27.56} ($\pm$6.60) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.05) & 0.63 ($\pm$0.42) & -\\
%     \midrule
%     G-B$_{FB,Y}$ & 27.91 ($\pm$7.18) & 0.87 ($\pm$0.10) & 0.93 ($\pm$0.05) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.69 ($\pm$0.41) & 70.4\%\\
%     G-B$_{100MIU,Y}$ & 28.37 ($\pm$7.31) & 0.87 ($\pm$0.09) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.67 ($\pm$0.41) & 67.4\%\\
%     \midrule
%     G-D$_{Y}$ & 32.09 ($\pm$18.98) & 0.77 ($\pm$0.20) & 0.86 ($\pm$0.13) & 0.84 ($\pm$0.15) & 0.66 ($\pm$0.43) & 67.8\%\\
%     \midrule
%     \midrule
%     D-baseline & 37.52 ($\pm$12.06) & 0.86 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.85 ($\pm$0.10) & 0.76 ($\pm$0.37) & -\\
%     D-100MCW & 37.80 ($\pm$10.89) & 0.85 ($\pm$0.14) & 0.89 ($\pm$0.10) & 0.85 ($\pm$0.10) & 0.82 ($\pm$0.33) & -\\
%     \midrule
%     D-B$_{FB,Y}$ & 38.53 ($\pm$12.64) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.10) & 0.82 ($\pm$0.33) & 83.0\%\\
%     D-B$_{100MIU,Y}$ & 38.67 ($\pm$11.70) & \textcolor{blue}{0.88} ($\pm$0.11) & 0.91 ($\pm$0.07) & 0.86 ($\pm$0.10) & \textbf{0.87} ($\pm$0.28) & \textcolor{blue}{88.5\%}\\
%     \midrule
%     D-D$_{Y}$ & 42.01 ($\pm$16.94) & \textbf{0.90} ($\pm$0.12) & 0.86 ($\pm$0.14) & 0.77 ($\pm$0.22) & \textcolor{blue}{0.86} ($\pm$0.29) & \textbf{85.9\%}\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Neutral prompt - Young model} Results of age-controlled language generation. ppl. is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. $\boldsymbol{\bar{P}_Y}$ and $\boldsymbol{\bar{P}_O}$ are the respective average young and old probabilities assigned by the best BERT$_{FT}$. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_neutral_prompt_young_models}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c }
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & \textcolor{blue}{27.50} ($\pm$6.58) & \textbf{0.87} ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.38 ($\pm$0.42) & -\\
%     G-100MCW & 27.56 ($\pm$6.60) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.05) & 0.37 ($\pm$0.42) & -\\
%     \midrule
%     G-B$_{FB,O}$ & 27.58 ($\pm$7.07) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.42 ($\pm$0.42) & 43.0\%\\
%     G-B$_{100MIU,O}$ & \textbf{27.25} ($\pm$6.15) & \textbf{0.87} ($\pm$0.09) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.38 ($\pm$0.42) & 37.4\%\\
%     \midrule
%     G-D$_{O}$ & 47.15 ($\pm$47.56) & 0.73 ($\pm$0.24) & 0.75 ($\pm$0.28) & 0.75 ($\pm$0.27) & \textbf{0.76} ($\pm$0.36) & \textbf{74.3\%}\\
%     \midrule
%     \midrule
%     D-baseline & 37.52 ($\pm$12.06) & 0.86 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.85 ($\pm$0.10) & 0.24 ($\pm$0.37) & -\\
%     D-100MCW & 37.80 ($\pm$10.89) & 0.85 ($\pm$0.14) & 0.89 ($\pm$0.10) & 0.85 ($\pm$0.10) & 0.18 ($\pm$0.33) & -\\
%     \midrule
%     D-B$_{FB,O}$ & 37.85 ($\pm$11.17) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.22 ($\pm$0.35) & 21.5\%\\
%     D-B$_{100MIU,O}$ & 37.91 ($\pm$12.27) & \textcolor{blue}{0.87} ($\pm$0.11) & 0.90 ($\pm$0.07) & 0.85 ($\pm$0.10) & 0.22 ($\pm$0.34) & 21.9\%\\
%     \midrule
%     D-D$_{O}$ & 41.17 ($\pm$20.72) & 0.87 ($\pm$0.12) & 0.89 ($\pm$0.13) & 0.83 ($\pm$0.16) & \textcolor{blue}{0.57} ($\pm$0.41) & \textcolor{blue}{56.7\%}\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Neutral prompt - Old model} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. $\boldsymbol{\bar{P}_Y}$ and $\boldsymbol{\bar{P}_O}$ are the respective average young and old probabilities assigned by the best BERT$_{FT}$. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_neutral_prompt_old_model}
% \end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & & & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & 28.05 ($\pm$6.12) & 0.85 ($\pm$0.13) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.80 ($\pm$0.33) & 0.20 & -\\
    G-100MCW & 27.71 ($\pm$6.20) & 0.85 ($\pm$0.12) & 0.91 ($\pm$0.09) & 0.88 ($\pm$0.09) & 0.75 ($\pm$0.37) & 0.25 & -\\
    \midrule
    G-B$_{FB, Y}$ & 28.81 ($\pm$7.09) & 0.86 ($\pm$0.12) & 0.92 ($\pm$0.08) & 0.89 ($\pm$0.08) & 0.82 ($\pm$0.32) & 0.18 & 83.3\%\\
    G-B$_{FB, O}$ & 28.54 ($\pm$6.45) & 0.86 ($\pm$0.12) & 0.92 ($\pm$0.08) & 0.89 ($\pm$0.08) & 0.77 ($\pm$0.36) & 0.23 & 22.6\%\\
    G-B$_{100MIU, Y}$ & 28.49 ($\pm$6.49) & 0.86 ($\pm$0.12) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.83 ($\pm$0.32) & 0.17 & 83.0\%\\
    G-B$_{100MIU, O}$ & 28.18 ($\pm$5.70) & 0.87 ($\pm$0.11) & 0.92 ($\pm$0.08) & 0.89 ($\pm$0.09) & 0.79 ($\pm$0.34) & 0.21 & 21.5\%\\
    \midrule
    G-D$_{Y}$ & 39.32 ($\pm$37.49) & 0.84 ($\pm$0.21) & 0.61 ($\pm$0.40) & 0.57 ($\pm$0.40) & 0.70 ($\pm$0.40) & 0.30 & 70.7\%\\
    G-D$_{O}$ & 85.40 ($\pm$150.28) & 0.67 ($\pm$0.30) & 0.62 ($\pm$0.31) & 0.62 ($\pm$0.32) & 0.29 ($\pm$0.40) & 0.71 & 70.5\%\\
    \midrule
    \midrule
    D-baseline & 36.69 ($\pm$9.11) & 0.87 ($\pm$0.10) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.90 ($\pm$0.24) & 0.10 & -\\
    D-100MCW & 36.93 ($\pm$9.18) & 0.86 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.88 ($\pm$0.07) & 0.90 ($\pm$0.25) & 0.10 & -\\
    \midrule
    D-B$_{FB, Y}$ & 37.35 ($\pm$8.60) & 0.88 ($\pm$0.10) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.90 ($\pm$0.26) & 0.10 & 90.0\%\\
    D-B$_{FB, O}$ & 37.25 ($\pm$9.45) & 0.87 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.88 ($\pm$0.29) & 0.12 & 11.1\%\\
    D-B$_{100MIU, Y}$ & 37.87 ($\pm$8.32) & 0.88 ($\pm$0.10) & 0.91 ($\pm$0.07) & 0.87 ($\pm$0.09) & 0.91 ($\pm$0.24) & 0.09 & 92.6\%\\
    D-B$_{100MIU, O}$ & 37.04 ($\pm$8.78) & 0.88 ($\pm$0.10) & 0.91 ($\pm$0.05) & 0.88 ($\pm$0.07) & 0.85 ($\pm$0.32) & 0.15 & 15.2\%\\
    \midrule
    D-D$_{Y}$ & 39.22 ($\pm$14.96) & 0.89 ($\pm$0.12) & 0.86 ($\pm$0.19) & 0.79 ($\pm$0.23) & 0.89 ($\pm$0.25) & 0.11 & 91.1\%\\
    D-D$_{O}$ & 38.46 ($\pm$14.91) & 0.82 ($\pm$0.15) & 0.87 ($\pm$0.15) & 0.83 ($\pm$0.17) & 0.52 ($\pm$0.44) & 0.48 & 47.4\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Young prompt} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_young_prompt}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & \textcolor{blue}{28.05} ($\pm$6.12) & 0.85 ($\pm$0.13) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.80 ($\pm$0.33) & -\\
%     G-100MCW & \textbf{27.71} ($\pm$6.20) & 0.85 ($\pm$0.12) & 0.91 ($\pm$0.09) & 0.88 ($\pm$0.09) & 0.75 ($\pm$0.37) & -\\
%     \midrule
%     G-B$_{FB, Y}$ & 28.81 ($\pm$7.09) & 0.86 ($\pm$0.12) & \textbf{0.92} ($\pm$0.08) & \textbf{0.89} ($\pm$0.08) & 0.82 ($\pm$0.32) & 83.3\%\\
%     G-B$_{100MIU, Y}$ & 28.49 ($\pm$6.49) & 0.86 ($\pm$0.12) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.83 ($\pm$0.32) & 83.0\%\\
%     \midrule
%     G-D$_{Y}$ & 39.32 ($\pm$37.49) & 0.84 ($\pm$0.21) & 0.61 ($\pm$0.40) & 0.57 ($\pm$0.40) & 0.70 ($\pm$0.40) & 70.7\%\\
%     \midrule
%     \midrule
%     D-baseline & 36.69 ($\pm$9.11) & 0.87 ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.06) & 0.87 ($\pm$0.08) & \textcolor{blue}{0.90} ($\pm$0.24) & -\\
%     D-100MCW & 36.93 ($\pm$9.18) & 0.86 ($\pm$0.11) & \textcolor{blue}{0.91} ($\pm$0.06) & \textcolor{blue}{0.88} ($\pm$0.07) & 0.90 ($\pm$0.25) & -\\
%     \midrule
%     D-B$_{FB, Y}$ & 37.35 ($\pm$8.60) & \textcolor{blue}{0.88} ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.90 ($\pm$0.26) & \textcolor{blue}{90.0\%}\\
%     D-B$_{100MIU, Y}$ & 37.87 ($\pm$8.32) & \textcolor{blue}{0.88} ($\pm$0.10) & 0.91 ($\pm$0.07) & 0.87 ($\pm$0.09) & \textbf{0.91} ($\pm$0.24) & \textbf{92.6\%}\\
%     \midrule
%     D-D$_{Y}$ & 39.22 ($\pm$14.96) & \textbf{0.89} ($\pm$0.12) & 0.86 ($\pm$0.19) & 0.79 ($\pm$0.23) & 0.89 ($\pm$0.25) & 91.1\%\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Young prompt - Young models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_young_prompt_young_model}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & \textcolor{blue}{28.05} ($\pm$6.12) & 0.85 ($\pm$0.13) & 0.91 ($\pm$0.08) & 0.88 ($\pm$0.08) & 0.20 ($\pm$0.33) & -\\
%     G-100MCW & \textbf{27.71} ($\pm$6.20) & 0.85 ($\pm$0.12) & 0.91 ($\pm$0.09) & 0.88 ($\pm$0.09) & 0.25 ($\pm$0.37) & -\\
%     \midrule
%     G-B$_{FB, O}$ & 28.54 ($\pm$6.45) & 0.86 ($\pm$0.12) & \textbf{0.92} ($\pm$0.08) & \textbf{0.89} ($\pm$0.08) & 0.23 ($\pm$0.36) & 22.6\%\\
%     G-B$_{100MIU, O}$ & 28.18 ($\pm$5.70) & 0.87 ($\pm$0.11) & \textbf{0.92} ($\pm$0.08) & \textcolor{blue}{0.89} ($\pm$0.09) & 0.21 ($\pm$0.34) & 21.5\%\\
%     \midrule
%     G-D$_{O}$ & 85.40 ($\pm$150.28) & 0.67 ($\pm$0.30) & 0.62 ($\pm$0.31) & 0.62 ($\pm$0.32) & \textbf{0.71} ($\pm$0.40) & \textbf{70.5\%}\\
%     \midrule
%     \midrule
%     D-baseline & 36.69 ($\pm$9.11) & \textcolor{blue}{0.87} ($\pm$0.10) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.10 ($\pm$0.24) & -\\
%     D-100MCW & 36.93 ($\pm$9.18) & 0.86 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.88 ($\pm$0.07) & 0.10 ($\pm$0.25) & -\\
%     \midrule
%     D-B$_{FB, O}$ & 37.25 ($\pm$9.45) & 0.87 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 0.12 ($\pm$0.29) & 11.1\%\\
%     D-B$_{100MIU, O}$ & 37.04 ($\pm$8.78) & \textbf{0.88} ($\pm$0.10) & \textcolor{blue}{0.91} ($\pm$0.05) & 0.88 ($\pm$0.07) & 0.15 ($\pm$0.32) & 15.2\%\\
%     \midrule
%     D-D$_{O}$ & 38.46 ($\pm$14.91) & 0.82 ($\pm$0.15) & 0.87 ($\pm$0.15) & 0.83 ($\pm$0.17) & \textcolor{blue}{0.48} ($\pm$0.44) & \textcolor{blue}{47.4\%}\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Young prompt - Old models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_young_prompt_old_model}
% \end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l | c c c c | c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & & & $\uparrow$ better\\
    \midrule
    \midrule
    G-baseline & 29.34 ($\pm$10.30) & 0.86 ($\pm$0.09) & 0.94 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.60 ($\pm$0.43) & 0.40 & -\\
    G-100MCW & 29.14 ($\pm$10.11) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.60 ($\pm$0.44) & 0.40 & -\\
    \midrule
    G-B$_{Y, FB}$ & 29.61 ($\pm$10.28) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & 0.91 ($\pm$0.06) & 0.62 ($\pm$0.43) & 0.38 & 61.1\%\\
    G-B$_{O, FB}$ & 28.81 ($\pm$10.10) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.05) & 0.90 ($\pm$0.06) & 0.59 ($\pm$0.43) & 0.41 & 41.1\%\\
    G-B$_{Y, 100MIU}$ & 29.51 ($\pm$0.09) & 0.87 ($\pm$0.09) & 0.93 ($\pm$0.05) & 0.90 ($\pm$0.06) & 0.68 ($\pm$0.42) & 0.32 & 68.5\%\\
    G-B$_{O, 100MIU}$ & 29.05 ($\pm$9.80) & 0.86 ($\pm$0.09) & 0.93 ($\pm$0.04) & 0.90 ($\pm$0.06) & 0.60 ($\pm$0.43) & 0.40 & 39.6\%\\
    \midrule
    G-D$_{Y}$ & 32.34 ($\pm$19.88) & 0.77 ($\pm$0.20) & 0.84 ($\pm$0.19) & 0.80 ($\pm$0.23) & 0.65 ($\pm$0.43) & 0.35 & 65.4\%\\
    G-D$_{O}$ & 95.21 ($\pm$174.42) & 0.65 ($\pm$0.27) & 0.78 ($\pm$0.18) & 0.78 ($\pm$0.18) & 0.10 ($\pm$0.25) & 0.90 & 90.3\%\\
    \midrule
    \midrule
    D-baseline & 38.18 ($\pm$12.03) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.72 ($\pm$0.38) & 0.28 & -\\
    D-100MCW & 37.73 ($\pm$11.88) & 0.85 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.73 ($\pm$0.39) & 0.27 & -\\
    \midrule
    D-B$_{Y, FB}$ & 38.24 ($\pm$11.53) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.10) & 0.81 ($\pm$0.34) & 0.19 & 82.6\%\\
    D-B$_{O, FB}$ & 37.8 ($\pm$11.74) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.07) & 0.87 ($\pm$0.08) & 0.72 ($\pm$0.39) & 0.28 & 29.3\%\\
    D-B$_{Y, 100MIU}$ & 38.66 ($\pm$11.57) & 0.85 ($\pm$0.12) & 0.90 ($\pm$0.07) & 0.86 ($\pm$0.09) & 0.81 ($\pm$0.33) & 0.19 & 80.7\%\\
    D-B$_{O, 100MIU}$ & 36.93 ($\pm$11.68) & 0.87 ($\pm$0.12) & 0.90 ($\pm$0.09) & 0.86 ($\pm$0.09) & 0.69 ($\pm$0.41) & 0.31 & 29.6\%\\
    \midrule
    D-D$_{Y}$ & 42.93 ($\pm$20.18) & 0.90 ($\pm$0.14) & 0.79 ($\pm$0.22) & 0.68 ($\pm$0.28) & 0.84 ($\pm$0.30) & 0.16 & 85.2\%\\
    D-D$_{O}$ & 45.58 ($\pm$38.59) & 0.86 ($\pm$0.14) & 0.86 ($\pm$0.13) & 0.79 ($\pm$0.20) & 0.40 ($\pm$0.42) & 0.60 & 59.7\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Old prompt} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_old_prompt}
\end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_Y}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & \textcolor{blue}{29.34} ($\pm$10.30) & 0.86 ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.60 ($\pm$0.43) & -\\
%     G-100MCW & \textbf{29.14} ($\pm$10.11) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.60 ($\pm$0.44) & -\\
%     \midrule
%     G-B$_{Y, FB}$ & 29.61 ($\pm$10.28) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.04) & \textbf{0.91} ($\pm$0.06) & 0.62 ($\pm$0.43) & 61.1\%\\
%     G-B$_{Y, 100MIU}$ & 29.51 ($\pm$0.09) & \textcolor{blue}{0.87} ($\pm$0.09) & 0.93 ($\pm$0.05) & \textcolor{blue}{0.90} ($\pm$0.06) & 0.68 ($\pm$0.42) & 68.5\%\\
%     \midrule
%     G-D$_{Y}$ & 32.34 ($\pm$19.88) & 0.77 ($\pm$0.20) & 0.84 ($\pm$0.19) & 0.80 ($\pm$0.23) & 0.65 ($\pm$0.43) & 65.4\%\\
%     \midrule
%     \midrule
%     D-baseline & 38.18 ($\pm$12.03) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.72 ($\pm$0.38) & -\\
%     D-100MCW & 37.73 ($\pm$11.88) & 0.85 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.73 ($\pm$0.39) & -\\
%     \midrule
%     D-B$_{Y, FB}$ & 38.24 ($\pm$11.53) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.10) & 0.81 ($\pm$0.34) & \textcolor{blue}{82.6\%}\\
%     D-B$_{Y, 100MIU}$ & 38.66 ($\pm$11.57) & 0.85 ($\pm$0.12) & 0.90 ($\pm$0.07) & 0.86 ($\pm$0.09) & \textcolor{blue}{0.81} ($\pm$0.33) & 80.7\%\\
%     \midrule
%     D-D$_{Y}$ & 42.93 ($\pm$20.18) & \textbf{0.90} ($\pm$0.14) & 0.79 ($\pm$0.22) & 0.68 ($\pm$0.28) & \textbf{0.84} ($\pm$0.30) & \textbf{85.2\%}\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Old prompt - Young model} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_old_prompt_young_model}
% \end{table*}

% \begin{table*}[h]
%     \centering
%     \begin{tabular}{l | c c c c | c c}
%     \toprule
%     \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & $\boldsymbol{\bar{P}_O}$ & \textbf{Acc.}\\
%     % -plus)}$\\
%      & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
%     \midrule
%     \midrule
%     G-baseline & 29.34 ($\pm$10.30) & \textcolor{blue}{0.86} ($\pm$0.09) & \textbf{0.94} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.43) & -\\
%     G-100MCW & 29.14 ($\pm$10.11) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.44) & -\\
%     \midrule
%     G-B$_{O, FB}$ & \textbf{28.81} ($\pm$10.10) & 0.86 ($\pm$0.10) & 0.93 ($\pm$0.05) & \textbf{0.90} ($\pm$0.06) & 0.41 ($\pm$0.43) & 41.1\%\\
%     G-B$_{O, 100MIU}$ & 29.05 ($\pm$9.80) & \textcolor{blue}{0.86} ($\pm$0.09) & \textcolor{blue}{0.93} ($\pm$0.04) & \textbf{0.90} ($\pm$0.06) & 0.40 ($\pm$0.43) & 39.6\%\\
%     \midrule
%     G-D$_{O}$ & 95.21 ($\pm$174.42) & 0.65 ($\pm$0.27) & 0.78 ($\pm$0.18) & 0.78 ($\pm$0.18) & \textbf{0.90} ($\pm$0.25) & \textbf{90.3\%}\\
%     \midrule
%     \midrule
%     D-baseline & 38.18 ($\pm$12.03) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.28 ($\pm$0.38) & -\\
%     D-100MCW & 37.73 ($\pm$11.88) & 0.85 ($\pm$0.13) & 0.90 ($\pm$0.08) & 0.86 ($\pm$0.09) & 0.27 ($\pm$0.39) & -\\
%     \midrule
%     D-B$_{O, FB}$ & 37.80 ($\pm$11.74) & 0.86 ($\pm$0.12) & 0.90 ($\pm$0.07) & \textcolor{blue}{0.87} ($\pm$0.08) & 0.28 ($\pm$0.39) & 29.3\%\\
%     D-B$_{O, 100MIU}$ & 36.93 ($\pm$11.68) & \textbf{0.87} ($\pm$0.12) & 0.90 ($\pm$0.09) & 0.86 ($\pm$0.09) & 0.31 ($\pm$0.41) & 29.6\%\\
%     \midrule
%     D-D$_{O}$ & 45.58 ($\pm$38.59) & 0.86 ($\pm$0.14) & 0.86 ($\pm$0.13) & 0.79 ($\pm$0.20) & \textcolor{blue}{0.60} ($\pm$0.42) & \textcolor{blue}{59.7\%}\\
%     \bottomrule
%     \end{tabular}
%     \caption{\len{Old prompt - Old models} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
%     \label{tab:ctg_results_ws_old_prompt_old_model}
% \end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    GPT-2 baseline & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    GPT-2 100MCW baseline & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \midrule
    GPT-2 B$_{Y, FB}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    GPT-2 B$_{O, FB}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    GPT-2 B$_{Y, 100MIU}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    GPT-2 B$_{O, 100MIU}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \midrule
    GPT-2 D$_{Y}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    GPT-2 D$_{O}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \midrule
    \midrule
    DGPT baseline & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    DGPT-100MCW & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \midrule
    DGPT B$_{Y, FB}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    DGPT B$_{O, FB}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    DGPT B$_{Y, 100MIU}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    DGPT B$_{O, 100MIU}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \midrule
    DGPT$_{Y}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    DGPT$_{O}$ & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots ($\pm$\dots) & \dots\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Unprompted} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws_unprompted}
\end{table*}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/exp2/lineplot_ppl_acc_best_gpt2_disc_bow_ci_90_errstyle_bars.png}
    \caption{\len{Unprompted} Fluency (perplexity, x-axis) versus control (BERT accuracy, y-axis). Bins chosen to be roughly same order of magnitude. The bars represent 90\% confidence intervals. This plot is best viewed in color.}
    \label{fig:ctg_lineplot_fluency_vs_control}
\end{figure}

\begin{figure}[H]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/lineplot_len_ppl_best_gpt2_disc_bow_ci_90_errstyle_bars.png}
        \caption{}
        \label{fig:ctg_lineplot_len_vs_ppl}
     \end{subfigure}
    %  \hfill
    \quad
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{figures/exp2/lineplot_len_acc_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
        \caption{}
        \label{fig:ctg_lineplot_len_vs_acc}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist1_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
        \caption{}
        \label{fig:ctg_lineplot_len_vs_dist1}
     \end{subfigure}
     
     \medskip
     
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist2_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
        \caption{}
        \label{fig:ctg_lineplot_len_vs_dist2}
     \end{subfigure}
     \quad
     \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp2/lineplot_len_dist3_best_gpt2_disc_bow_ci_90_errstyle_bars_nolegend.png}
        \caption{}
        \label{fig:ctg_lineplot_len_vs_dist3}
     \end{subfigure}
     
     % Main figure caption and label
        \caption{\len{Unprompted} Various averaged automated evaluation metrics of generated sentences, plotted against increasing sequence length (x-axis), with 90\% confidence intervals. The plots are best viewed in color.}
        \label{fig:ctg_lineplots_len_vs_metrics}
\end{figure}

\section{CTG Results for Unprompted Setup \len{Redundant?}}

Table~\ref{tab:ctg_results_ws} reports the automated evaluation results of our controllable text generation models. It can be seen that uncontrolled GPT-2 baseline has a slight bias towards generating "young-sounding" language (57.5\% accuracy). Furthermore, it appears that perturbing GPT-2's output distribution with the 100 most common words across all ages results in a slight de-biasing of the generated text (54.1\% accuracy). Achieving detectable control seems possible, because all GPT-2-based models surpass both baselines in terms of accuracy, with the exception of both BoW-setups using the 100 most informative unigrams.

Frequency-based BoW-models outperform those using the most informative unigrams, as illustrated by their higher average accuracy (66.75\% versus. 53.1\%), and lower average perplexity (27.48 versus 27.90). 
% \len{Offer an explanation.} 
Discriminator-based models achieve noticeably better accuracies, with an average improvement of 8.45\% over the best performing BoW-based models. However, discriminator-based models do show more signs of disfluency and repetitiveness compared to the BoW-models, as depicted by the worse perplexities and Dist-$n |_{n = 1,2,3}$ scores.

The accuracies of our uncontrolled DialoGPT baseline (78.1\%) and the 100MCW baseline (80.7\%), suggest that DialoGPT is heavily biased towards producing young-sounding language. This can be attributable to DialoGPT having been fine-tuned on Reddit threads, as the majority of Reddit users are between the ages 20 and 29~\footnote{\url{https://www.statista.com/statistics/1125159/reddit-us-app-users-age/}}
% \len{Find a reference for this statement} 
\citep{zhang2019dialogpt}. DialoGPT's strong propensity for generating younger sounding language makes it a less desirable choice for our human evaluation experiments, because it requires non-standard parameter settings to produce detectably older sounding text.

Overall, the results show that, for most models, a plug-and-play approach to controlling generated dialogue responses to possess detectable age-specific linguistic features is achievable. The most promising models being either discriminator-based, or frequency-based bag-of-words models. Discriminator-based models achieve more detectable levels of control than their BoW-based counterparts, at the cost of perplexity and repetitiveness. This could be attributable to the more complex activation-space updates that are used by discriminator-models. Furthermore, GPT-2's preference to generate young-sounding language is severely less pronounced than that of DialoGPT, making it easier to control, given equal parameter settings.

% \textbf{Observations:}
% GPT-2
% \begin{itemize}
%     \item All GPT2-based models beat the GPT-2 baseline, except for the BoW-100MIU.
%     \item GPT-2 has a slight bias towards being classified as young. Has a preference to sound young.
%     \item BoW-based control with the 100 most common words appears to de-bias uncontrolled GPT-2
%     \item The frequency-based BoW models are the best ones. They achieve on average better control than 100MIU w.r.t. the baseline, and have on average a lower perplexity than 100MIU. \len{Compute and explicitly mention these differences}.
%     \item Discriminator-based models achieve (1) much higher accuracy, (2) noticeably worse perplexity, and (3) on average more repetitive unigrams, bigrams, and trigrams.
%     \item Controlling for old language seems harder than for young, as its accuracy is consistently lower.
% \end{itemize}

% \textbf{DialoGPT:}
% \begin{itemize}
%     \item DialoGPT is heavily biased towards younger language. This is most likely due to it being finetuned on Reddit threads. Vast majority of U.S. Reddit users are between the ages of 18 and 19 \url{https://www.statista.com/statistics/261766/share-of-us-internet-users-who-use-reddit-by-age-group/}
%     \item 100 most common words across all ages seems to result in more pronounced young-bias. Could also be due to prompt.
% \end{itemize}

\begin{table*}[h!]
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Model} & \textbf{ppl.} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Dist-3} & \textbf{Acc.}\\
    % -plus)}$\\
     & $\downarrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better & $\uparrow$ better\\
    \midrule
    \midrule
    GPT-2 baseline & 29.60 ($\pm$17.57) & \textbf{0.89} ($\pm$0.09) & \textbf{0.93} ($\pm$0.05) & 0.88 ($\pm$0.10) & 57.5\%\\
    GPT-2 100MCW baseline & 27.80 ($\pm$16.44) & 0.83 ($\pm$0.12) & 0.91 ($\pm$0.06) & \textcolor{blue}{0.88} ($\pm$0.09) & 54.1\%\\
    \midrule
    % B$_{Y, FB, 80}$ & 29.13 ($\pm$15.17) & 0.86 ($\pm$0.10) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.11) & 70\%\\
    % B$_{O, FB, 80}$ & \textbf{26.42} ($\pm$8.87) & 0.86 ($\pm$0.10) & \textcolor{blue}{0.92} ($\pm$0.05) & \textbf{0.89} ($\pm$0.10) & 62.2\%\\
    B$_{Y, FB}$ & 28.16 ($\pm$14.52) & 0.87 ($\pm$0.10) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.10) & 69.3\%\\
    B$_{O, FB}$ & 26.79 ($\pm$8.89) & \textcolor{blue}{0.88} ($\pm$0.09) & \textcolor{blue}{0.92} ($\pm$0.05) & 0.88 ($\pm$0.10) & 64.2\%\\
    B$_{Y, 100MIU}$ & 29.16 ($\pm$14.91) & \textbf{0.89} ($\pm$0.09) & 0.92 ($\pm$0.06) & 0.88 ($\pm$0.11) & 52.5\%\\
    B$_{O, 100MIU}$ & \textcolor{blue}{26.63} ($\pm$8.36) & 0.87 ($\pm$0.10) & 0.92 ($\pm$0.07) & 0.88 ($\pm$0.10) & 53.7\%\\
    \midrule
    D$_{Y, GPT2}$ & 31.95 ($\pm$14.29) & 0.82 ($\pm$0.17) & 0.87 ($\pm$0.14) & 0.83 ($\pm$0.16) & \textbf{77.7\%}\\
    D$_{O, GPT2}$ & 33.63 ($\pm$24.40) & 0.80 ($\pm$0.18) & 0.87 ($\pm$0.11) & 0.81 ($\pm$0.21) & \textcolor{blue}{72.7}\%\\
    \midrule
    DGPT baseline & 35.20 ($\pm$10.01) & 0.87 ($\pm$0.11) & 0.90 ($\pm$0.07) & 0.87 ($\pm$0.08) & 78.1\%\\
    DGPT-100MCW & 35.64 ($\pm$9.72) & 0.86 ($\pm$0.10) & 0.90 ($\pm$0.06) & 0.87 ($\pm$0.08) & 80.7\%\\
    D*$_{Y, DGPT}$ & 41.54 ($\pm$10.87) & 0.91 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.86 ($\pm$0.09) & \textbf{84.1}\%\\
    D*$_{O, DGPT}$ & 38.16 ($\pm$10.77) & 0.87 ($\pm$0.11) & 0.91 ($\pm$0.06) & 0.87 ($\pm$0.08) & 55.6\%\\
    \bottomrule
    \end{tabular}
    \caption{\len{Unprompted} Results of age-controlled language generation. Perplexity is perplexity w.r.t. GPT-1. Dist-n is number of distinct n-grams normalized by text length, as a measure of diversity. Acc. is the best BERT model's accuracy when classifying the row's samples.}
    \label{tab:ctg_results_ws}
\end{table*}