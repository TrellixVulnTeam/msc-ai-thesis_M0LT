\section{Summary of Key Findings and Interpretations}

In this thesis, the problems of automated age detection from dialogue and age-adaptive controlled dialogue generation are investigated. First, I studied the extent to which purely text-based NLP models can detect age-related linguistic features in dialogue data, and which features drive their predictions. Subsequently, I studied the extent to which age-adaptive dialogue generation is possible using an approach based on Plug-and-Play language models (PPLM) \citep{dathathri2019plug}.

The results of the age detection experiments indicated that a fine-tuned version of BERT, BERT$_{FT}$, is capable of detecting age-related linguistic features in dialogue utterances with reasonable accuracy. BERT$_{FT}$ was particularly useful for age detection when the dialogue fragment is long enough to contain discriminative signal. At the same time, it is observed that much simpler models based on $n$-grams achieve comparable performance, which suggests that, in dialogue, ‘local’ features can be indicative of the language of speakers from different age groups. This is shown to be the case, with both lexical and stylistic cues being informative to these models in this task.


% \textbf{connecting paragraph:}
% \begin{itemize}
%     \item The age detection results informed me about the development of controllable dialogue generation systems using PPLM.
%     \item The presumed locality of age-indicative features, as suggested by the comparable performance achieved by the $n$-gram models, motivates the use of unigram-based bag-of-words attribute models in PPLM-setups for, in comparison to more sophisticated discriminator-based PPLM-setups.
%     \item The analyses of experiment 1 predominantly compare the classification performances and feature importance of $n$-gram and Transformer-based neural models. This comparison is continued in the controlled dialogue generation experiments, in the form of comparing BoWs versus neural discriminators as attribute models.
%     \item The best performing classifier from the age detection experiments, BERT$_{FT}$, is used in the second phase of experiments to evaluate the prompts and generated responses for their resemblance to the linguistic style of the younger or older age group. 
% \end{itemize}

Furthermore, the age detection results informed the development of controllable dialogue generation systems using PPLM. 
% Namely, the presumed locality of age-indicative features, as suggested by the comparable performance achieved by the $n$-gram models, motivates the use of unigram-based bag-of-words attribute models in PPLM-setups for, in comparison to more sophisticated discriminator-based PPLM-setups.
On the one hand, the presumed locality of age-indicative features, as suggested by the comparable performance achieved by the $n$-gram models, motivates the use of unigram-based bag-of-words attribute models in PPLM-setups.
On the other hand, the use of more sophisticated neural discriminator attribute models in PPLM-setups is motivated by the superior performance of neural discriminators in Experiment 1, and the notion that abstract linguistic styles, like those of specific age groups, are not easily represented as a bag-of-words.
This difficulty of expressing an age group's linguistic style as a manually curated bag-of-words motivates the use of empirically generated bag-of-words attribute models (see Section \ref{subsec:att_model_dev}).
Specifically, the empirically generated lists of unigrams per age group used as BoW attribute models are (1) the lists of unigrams deemed most informative by Experiment 1's best-performing unigram-based classifier, and (2) the lists of the most frequently used words by a specific age group, after omitting the overlapping frequently used words between both age groups.
% Specifically, the two types of empirically generated bag-of-words attribute models used in Experiment 2 are 
% Additionally, I used the list of unigrams as an empirically generated attribute model for a BoW-based PPLM setup, specifically the list of unigrams deemed most informative by the best-performing unigram classifier.
Overall, the analyses of Experiment 1 predominantly compare the classification performance and feature importance of $n$-gram and Transformer-based neural models. This comparison is continued in the controlled dialogue generation experiments, in the form of comparing BoWs versus neural discriminators as attribute models in PPLM-setups. Also, the best performing classifier from the age detection experiments, BERT$_{FT}$, is used in the second phase of experiments to evaluate the prompts and generated responses for their resemblance to the linguistic style of the younger or older age group.

The controlled dialogue generation results indicate that it is possible to use PPLM to generate dialogue responses that have been adapted to the styles of different age groups, to the extent that is identifiable by Experiment 1's best classifier. It can be seen that the discriminator-based PPLM-setups typically achieve higher levels of detectable control (i.e., statistical resemblance to a specific writing style) than the BoW-based setups, but generate significantly more perplexing and repetitive responses. This could be attributable to the fact that BoW-based control is more local (i.e., token-level) and less invasive than discriminator-based control, which can operate at the structural level. The results also indicate that the underlying language models used in the PPLM-setups, GPT-2 and DialoGPT, are both biased towards generating younger sounding language (according to the best performing classifier from Experiment 1). This is most likely due to both models having been pre-trained on texts scraped from web pages, which are typically over-represented by millennials \citep{radford2019language, zhang2019dialogpt}. 
% Quantitative analyses of the dialogue generation results revealed that the outputs of some PPLM-setups display a correlation between improved control and worsened perplexity and diversity.
Quantitative analyses of the dialogue generation results revealed that the outputs of some PPLM-setups show a correlation between improved target probability and worsened perplexity and diversity. This could be interpreted as a tradeoff between control and quality.
% control and worsened perplexity and diversity.
Also, the style of the prompt (i.e., whether it is classified by BERT$_{FT}$ as younger, neutral, or older) steers the style of the generated response in the direction of the prompt's style. This observation indicates the importance of taking prompt-induced biases into account when developing controlled generation models. It also appears that BoW-based models are significantly worse at controlling for older sounding language than they are for younger sounding language.
This could be attributable to the possibility that overcoming the younger biases of the underlying language models (GPT-2 and DialoGPT), to produce older sounding responses, needs more invasive perturbations than BoW-based attribute models can bring about, given the current parameter settings.
% It could also be that there are some detectable, strongly younger sounding, tokens, that make the token-level perturbations made by BoW-based models more likely to contain younger features detectable by my models, whereas older sounding features are more salient at the structural level and thereby harder to control for. 
% BERT$_{FT}$'s attention weight visualizations could also be evidence of this phenomena, because the attention weights of linguistic-feature attending heads are much more focused for strongly younger sounding input, than the more dispersed attention weights of older sounding sentences (see Figures \ref{fig:bertviz_model_view_ypr1}, \ref{fig:bertviz_model_view_ypr2}, \ref{fig:bertviz_model_view_opr1} and \ref{fig:bertviz_model_view_opr2}). 
Finally, a qualitative inspection of generated responses reveals that differences in style are noticeable by the use of age-indicative words, the formality of language, and the prevalence of certain topics.

% \textbf{Paragraph about generation results}
% \begin{itemize}
%     \item The controlled dialogue generation results indicate that it is possible to use PPLM to generate dialogue responses that have been detectably adapted to the styles of different age groups.
%     \item Discriminator-based control typically achieves higher levels of measurable control than BoW-based models, but generates significantly less fluent responses.
%     \item The underlying language models, GPT-2 and DialoGPT, are both biased towards generating younger sounding language. Due to their training data etc.
%     \item For responses generated by some setups, there seems to be correlation between improved adaptation and worsened perplexity and diversity.
%     \item Controlling for the younger age groups seems easier because of (1) bias?, (2) 
%     \item The style of the prompt (i.e., whether it's classified as young, neutral, or old) steers the style of the generated response in the direction of the prompt's style.
%     \item BoW-based models are significantly worse at controlling for old-language than they are for young.
%     \begin{itemize}
%         \item It could be that there are some detectable, very young-sounding, tokens, that make the token-level perturbations made by BoW-based models more likely to contain detectably young-features, whereas old-sounding features are more salient at the structural syntactical level and thereby harder to control for.
%         \item BERT$_{FT}$ attention weight visualization could also be evidence of this phenomena as the attention weights of linguistic-feature attending heads are much more focused for strongly young-input, than the more dispersed attention weights of old sentences.
%         \item However, it could also just be that old features are being detected by different (combinations of) heads. --> Future research.
%     \end{itemize}
%     \item Longer responses are deemed less perplexing.
%     \item Differences in style, brought about by PPLM-control, are noticeable by use of slang words, certain topics, etc. See qualitative analysis.
% \end{itemize}

% \section{Contributions and Implications}\label{sec:contributions}

% My age detection experiments build on the work of \cite{schler2006effects}, who focus on age detection in written discourse using handcrafted features. I extend their work by: \textbf{(1)} eliminating the need for handcrafted features by learning end-to-end representations using state-of-the-art NLP models; \textbf{(2)} applying this approach to dialogue data, using a dataset of transcribed spontaneous open-domain dialogues; \textbf{(3)} showing that text-based models can indeed detect age-related differences, even in the case of very sparse signals at the level of dialogue utterances; \textbf{(4)} carrying out an in-depth analysis of the models' predictions to gain insight about which elements of language use are most informative. Furthermore, the age detection analyses motivate the use of local features (i.e., BoW-based attribute models) for controlled generation as a viable alternative to neural discriminator-based attribute models. My work on age detection from dialogue can be considered a preliminary step to the modeling of age-related linguistic adaptation by AI conversational systems. In particular, these results informed my work on controlled dialogue generation using PPLM.

% My work on controlled dialogue generation using PPLM builds on previous work on controlled language generation by \cite{dathathri2019plug}, who focus on controlled story writing for concrete styles (e.g., sentiment, or topic). I extend their work in several important ways: \textbf{(1)} I control language generation for more abstract writing styles, i.e., age group specific linguistic style; \textbf{(2)} I use PPLM for dialogue response generation; \textbf{(3)} I propose methods for empirical development of BoW attribute models (as opposed to the manually curated BoWs used by \cite{dathathri2019plug}) and demonstrate their applicability for controlled dialogue generation; \textbf{(4)} I thoroughly study the relationships between dialogue response quality, response style, and response length; 
% (5) I also contribute to a clearer understanding of what age-related features a fine-tuned BERT model seems to focus on when classifying generated responses; 
% and finally \textbf{(5)} I carry out an extensive analysis on the effects of prompt-induced biases on the quality and style-attribute adherence of generated language, which has been overlooked by previous work on Plug-and-Play generation. 
% Despite previous work also focusing on 
% Plug-and-Play 
% conversational models, my work demonstrates a Plug-and-Play approach to controlled dialogue generation, without the need to generate attribute-specific dialogue datasets, or separately optimize residual adapter modules \citep{madotto-etal-2020-plug}.

% Overall, this research is a promising step towards the development of adaptive conversational systems. In particular, the development of age-adaptive conversational systems can benefit from these results. Since consistent language style differences
% were found between age groups, systems whose language generation capabilities aim to be consistent with a given age group should therefore reproduce these patterns. This could be achieved, as I have shown, by embedding Plug-and-Play modules that control the generation of a system’s output, which could lead to better, more natural interactions between human speakers and a conversational system.

% \begin{itemize}
%     \item \len{Refine the statements relating to age detection.}
%     \item The age detection results build on the work of \cite{schler2006effects}, who focus on age detection in written discourse using handcrafted features.
%     \item I extend this work by (1) eliminating the need for handcrafted features by learning end-to-end representations using state-of-the-art NLP models; (2) applying this approach to dialogue data, using a dataset of transcribed spontaneous open-domain dialogues; (3) showing that text-based models can indeed detect age-related differences, even in the case of very sparse signal at the level of dialogue utterances; (4) I carry out an in-depth analysis of the models' predictions to gain insight about which elements of language use are most informative.
%     \item The age detection analyses also motivate the use of local features for controlle generation, in combination with neural representations(?).
%     \item My work can be considered a first step toward the modeling of age-related linguistic adaptation by AI conversational systems. In particular, my results can inform future work on controlled text generation for dialogue agents.
%     \item Themore abstract style attribute to control for in a PPLM setup than previous work.
%     \item This work extends original PPLM work by...
%     \begin{itemize}
%         \item using empirically generated BoWs (While previous research has focused on manually curated wordlists as BoWs, these results demonstrate the applicability of various empirical methods for BoW attribute model development..;
%         \item controlling for a more abstract writing style;
%         \item studying the relationships between response quality, age-adaptation, and response length.
%         \item studying the effects of prompt-induced bias, previous work on PPLM doesn't take prompt-bias into account.
%     \end{itemize}
%     \item Also contributes to a clearer understandig of what features finetuned BERT seems to focus on when classifying generated responses.
%     \item Despite previous work by \cite{madotto-etal-2020-plug} also focusing on Plug-and-Play conversational models, my work demonstrates the achievability of a Plug-and-Play approach to controlled dialogue generation, without the necessity to generate attribute-specific dialogue datasets, or separately optimize residual adapter modules.
%     \item Contributes to using low-cost methods for controlled generation
%     \item Overall, my research is a promising step towards the development of personalized virtual assistants.
% \end{itemize}

\section{Limitations}\label{sec:limitations}

% \len{Double check if you're not undermining your research. The discussion of limitations should aim to strengthen your credibility, not emphasize weaknesses or failures.}

There are several limitations to take into account when interpreting the results presented in this thesis. The generalizability of the age detection results is limited by the coarse granularity with which the age groups are defined. One could imagine that if one aims to leverage the predicted age signal in real-world conversational systems, a fine-grained grouping is more reasonable. Furthermore, age group information remains difficult to identify in short utterances. A non-negligible proportion of utterances seem too short for reliable classification, and it remains to be investigated if NLP models or even humans could identify the targets.
Acoustic signals are not considered in this work, but they are important characteristics of speaking style, and could also be taken into account when developing audio-based conversational systems. On the one hand, this richer signal (i.e., the combination of acoustic and textual information) could lead to better classification performance. On the other hand, the use of acoustic signals could lead to ethical privacy-related issues, that aggregated text-based approaches might have to a lesser extent. 
This is discussed in more detail in Section \ref{sec:considerations}.
% Furthermore, for future applications, the combination of speech analysis could improve performance. Acoustic signals are not considered in this work, but they are important characteristics of speaking style, and could also be taken into account when developing audio-based conversational systems. 

% Due to lack of information about the \textit{none} and \textit{no info} topics, the age detection results cannot rule out confounding factors. This is especially important for the 50-plus age group, where the \textit{none} and \textit{no info} topics apply to nearly 50\% of the utterances. Nevertheless, my analysis of classification performance per topic do not seem to indicate severe differences in the identifiability of age-related signals between topic categories. 

With respect to the evaluation of the generated responses (see Section \ref{subsec:exp_setup_eval}), the reliability of the controlled dialogue generation results is impacted by perplexity (measured by GPT-1) being a crude proxy for fluency. Perplexity also lacks generalizability as an evaluation measure, because it only measures the uncertainty assigned by one language model. Automated evaluation of fluency could be improved by considering an aggregated perplexity measure (i.e., perplexity averaged over an ensemble of language models). Alternatively, indications of a generated response's fluency could be made more reliable by having humans participate in an evaluation study to rate the responses, and taking their opinions into account. 

It is also important to realize that the learned representations for younger and older style used in this research are specific to the BNC \citep{love-spoken-bnc-2014}, and should not be interpreted as general representations of the speaking style of people of ages 19-29 and 50-plus. Even within the context of the BNC, the representations used for classification and generation 
correspond to textual features which have been learned by Experiment 1's models to have a high likelihood of coming from a younger or older speaker, 
and are therefore approximations of the age groups' speaking styles. 

Finally, due to the lack of age-labeled transcribed spontaneous dialogue data, I was constrained to using the same dialogue dataset to develop the attribute models (i.e., the BoWs or neural discriminators used in PPLM-setups), and to train the evaluation model (i.e., BERT$_{FT}$). It must be noted that this is still sound practice, because the attribute and evaluation models are not trained jointly, and do not share representations. Moreover, the approach of using an external classifier for evaluation is based on the work of \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug}. At the same time, the assigned target probabilities indicating attribute adherence would be more generalizable if the evaluation model were trained on a different dataset of spontaneous dialogue utterances between speakers of the same age.



% \begin{itemize}
%     \item Can any ML architecture pick up signals from 1-6 token sequences? (See workshop paper submission feedback).
%     \item The age detection classification problem's age groups are defined with coarse granularity. Imagine that if I aim to leverage the predicted age signal in real systems, fine-grained grouping is more reasonable.
%     \item unfortunately, there are still problems especially as far as the identification of age groups with short utterances. Nevertheless, for future applications, the combination of speech analysis could bring to better results. (the sentences seem to short (6~7 words), it is doubtful if  ML models or even human could recognize the targets.)
%     \item What are the "none", "no info", "unknown" labels in the two datasets? This could be very critical given that the none + no info topics take nearly 50\% in the 50+ age group.
%     \item Perplexity is a very crude proxy for fluency. --> human evaluation or more reliable automated evaluation metric for fluency.
%     \item The confounding effects of topic on age detection and controlled generation.
%     \item I need to find out how much of "control" is attributable to method and how much to prompt-bias.
%     \item Interactivity is beyond the scope of this thesis
%     \item The methodological choices about age group granularity were constrained by insufficient age-labeled dialogue data for all age groups of interest.
%     \item The representations for young and old style used in this research are specific to the BNC, and should not be interpreted as generally representative of speaking style of 19-29 and 50-plus.
%     \item Even within the context of the BNC, the representations used for classification and generation are indicative of textual features learned to coincide with utterances from certain age groups, and are not to be confused with general representations of age groups' speaking styles.
%     \item Acoustic signals are not considered in this work, but they are important characteristics of speaking style, and should also be taken into account when developing audio-based conversational systems.
%     \item despite formulation of binary classification requiring young-prob and old-prob to be complementary values, "young" and "old" speaking/writing styles are not semantically opposite styles, like positive vs negative sentiment tend to resemble more \len{Maybe think of a better example of semantic polar opposites. Also, maybe also mention that it still needs to be verified whether they are semantically opposite or not, but that you hypothesize that they are not. Idea for future research.}. So I shouldn't expect certain patterns in, e.g., ppl plots to be exactly opposite.
%     \item Attention is not explanation.?
%     \item Due to the lack of age-labeled transcribed spontaneous dialogue data, I were constrained to using the same dialogue dataset to develop the attribute models (i.e., the BoWs or neural discriminators used in PPLM-setups), and to train evaluation model (i.e., BERT$_{FT}$). It must be noted that this is still sound practice, because the attribute and evaluation models are not trained jointly, and do not share representations. Moreover, the approach is based on the work by \cite{dathathri2019plug} and \cite{madotto-etal-2020-plug}. However, the assigned target probabilities indicating attribute adherence would be more generalizable if the evaluation model were trained on a different dataset of spontaneous dialogue utterances between speakers of the same age.
% \end{itemize}

\section{Future Research Directions}

While the classification task in Experiment 1 was performed at the level of single dialogue utterances, future work may take into account larger dialogue fragments, such as the entire dialogue or a fixed number of turns. This would make the setup more comparable to discourse, but would require making experimental choices and dealing with extra computational challenges. Moreover, it could be tested whether the language used by a speaker is equally discriminative when talking to a same-age (this work) or a different-age interlocutor. Also, future work might involve leveraging acoustic signals for automated age detection and controlled generation. Acoustic signals can contain important features indicative of speaking style, and could therefore substantially improve classification and controlled generation performance.

Developing interactive age-adaptive conversational systems was beyond the scope of this thesis. Therefore, an interesting future research objective would be to use Plug-and-Play Conversational Models (PPCM) for age-adaptive dialogue generation \citep{madotto-etal-2020-plug}. PPCM solves a latency problem of PPLM, making it usable in interactive settings. However, the use of PPCM in combination with more abstract speaking styles and BoW-based attribute models has not been researched (to the best of my knowledge). Using PPCM for age-adaptive dialogue generation would require developing and training residual adapter modules, and generating style-specific dialogue datasets. Furthermore, the age detection results suggest the importance of $n$-gram features for age identification. Therefore, an interesting future research direction would be to adapt PPLM and PPCM to be compatible with $n$-gram lists (for $n>1$) as attribute models. This would require finding a way to by-pass the need to re-train large underlying language models like GPT-2 and DialoGPT for arbitrary $n$-grams. Finally, the current set of tools to analyze which features drive controlled generation in PPLM-setups is limited. It would be valuable to develop probing methods for PPLM-setups. Namely, visualizations or saliency methods targeted towards understanding the activation space perturbations made by PPLM could provide important insights about the effects of attribute models on controlled generated output.

% \len{Another future research idea: PPLM for intersections of styles/attributes, e.g., young-positive, or old-family. As it seems now, age-related style seems to strongly correlate with topics. Would be interesting to see if these age-related style aspects can be isolated from the topics they often coincide with.}
% \begin{itemize}
%     \item While I performed the classification task at the level of single dialogue utterances, future work may take into account larger dialogue fragments, such as the entire dialogue or a fixed number of turns. This would make the setup more comparable to discourse, but but would require making experimental choices and dealing with extra computational challenges. Moreover, it could be tested whether the language used by a speaker is equally discriminative when talking to a same-age (this work) or a different-age interlocutor.
%     \item The confounding effects of topic on age detection and controlled generation. Topic-agnostic age control.
%     \item Developing interactive age-adaptive conversational systems was beyond the scope of this thesis. Therefore, an interesting future research objective would be to use PPCM \citep{madotto-etal-2020-plug} in for age adaptive dialogue generation. 
%     \item Leveraging acoustic signals for automated age-detection and controlled generation.
%     \item Future research idea: adapting the PPLM-setup to work with $n$-gram lists for arbitrary $n$.
%         \begin{itemize}
%             \item Finding a way to by-pass the need to retrain GPT-2 for arbitrary n-grams
%         \end{itemize}
%     \item Future research idea: how to probe PPLM models, because BertViz doesn't work, as the attention weights are unchanged by PPLM.
%     \item Emphasize the importance of PPLM-methods w.r.t. carbon footprint and the ecological cost of (re)training massive language models like GPT-x (Maybe this is better for the introduction?)
% \end{itemize}

\section{Ethical and Environmental Considerations}\label{sec:considerations}

There is growing concern around the ethics, environmental costs, and societal dangers associated with powerful large-scale language models \citep{brown2020language-models-few-shot-gpt3, bender2021dangers}. 
There is a non-negligible societal risk accompanying the recent trend of deploying ever larger language models that are more capable of producing texts that are indistinguishable from human-written ones. 
Namely, such language models can be used to spread misinformation (e.g., fake news), or to perpetuate harmful social biases.
Plug-and-Play language models carry the same risks, if not to a greater extent, as they can be used to produce personalized, and ultimately more convincing text.
As seen in the analysis of generated responses in Section \ref{subsec:ctg_anal_qualitative}, some examples of the models' outputs can be seen as exacerbating biases present in the used dialogue dataset (and society) (e.g., older speakers talking about healthcare, younger speakers talking about partying). This bias-amplification could be attributable to the models also to picking up and generating topic-related features, aside from purely stylistic ones. Future considerations for work on controlled generation should therefore also include methods to ensure adaptation happens at the stylistic level, and avoid amplification of (topical) biases in the data being used.

Furthermore, as previously mentioned in Section \ref{sec:limitations}, style-detection performance of adaptive dialogue systems could be improved by the use of acoustic signals, in addition to purely text-based information. However, privacy concerns should be taken into account when augmenting signals for dialogue systems, as this leads to more user information being processed, making users easier to identify. Researchers should therefore consider adaptation methods that identify users with as sparse a signal as possible.
% Plug-and-Play language models carry the same risks, if not to a greater extent, as they can be used to produce personalized, and ultimately more convincing text. 

However, \cite{dathathri2019plug} also demonstrate the benevolent applicability of PPLM for language detoxification, which can help to reduce the perpetuation of harmful biases.
Furthermore, Plug-and-Play methods, which avoid substantial re-training costs of massive language models, can help to reduce the energy consumption and dampen the environmental impact of developing modern deep learning models \citep{strubell-etal-2019-energy}.
Additionally, the same methods that generate convincing texts (like GPT-2) can be used to build more sophisticated tools for detecting artificially generated texts \cite{gehrmann-etal-2019-gltr}. 
Overall, I believe that the positive applications of PPLM for, e.g., personalized dialogue generation, outweigh the potential risks of misuse. 

% \begin{itemize}
%     \item Carbon footprint of training large language models and how PPLM helps to alleviate this.
%     \item The societal dangers of language generation w.r.t. e.g. fake news
%     \item What are the dangers of these methods? --> Read up on paper by Ebru et al
%     \item The analyses about the effects of using strongly stylized prompts on the style of generation revealed that there is a considerable prompt-induced bias. Despite, efforts having been made in this study to avoid this bias by using neutral prompts, more research is needed to find out how much of "control" is attributable to method and how much to prompt-bias.
%     \item despite formulation of binary classification requiring young-prob and old-prob to be complementary values, "young" and "old" speaking/writing styles are not semantically opposite styles, like positive vs negative sentiment tend to resemble more \len{Maybe think of a better example of semantic polar opposites. Also, maybe also mention that it still needs to be verified whether they are semantically opposite or not, but that you hypothesize that they are not. Idea for future research.}. So I shouldn't expect certain patterns in, e.g., ppl plots to be exactly opposite.
% \end{itemize}

% \section{Bullet points}

% \begin{itemize}
%     \item \len{Re-iterate this list and recap \textit{why} certain decisions were made.}
%     \item The research question was if it was possible to use PPLM for controlled dialogue generation. Before that, I wanted to confirm that age-related features in transcribed dialogue and discourse are prevalent enough to train various classifiers.
%     \item I performed age classification of utterances.
%     \item BERT was best, then trigram. 
%     \item I studied which features were most salient in driving classification, how classification performance related to dialogue topic, and which cases were most challenging to classify for BERT and trigram.
%     \item \len{\textbf{IMPORTANT:} Explicitly Highlight connection/bridge between experiment1 and experiment2}
%     \item After confirming the detectability of age-related linguistic patterns in dialogue, I seek to control generated responses for age-related traits.
%     \item based on the effectiveness of n-gram based classification model compared to transformer-based, I use PPLM-setups with both bow-based and discriminator-based attribute models.
%     \item I generated dialogue responses to neutral, young, and old prompts using models based on either GPT-2 or DialoGPT, and used discriminator- or bow-based attribute models.
%     \item I analyzed the relationship between perplexity and target probability, the relationship between various evaluation metrics and response length, the effects of prompt class, BERT$_{FT}'s$ attention heads, and qualitative properties of generated samples manually.
% \end{itemize}

% \paragraph{Discussion points about classification}
% \begin{itemize}
%     \item Can any ML architecture pick up signals from 1-6 token sequences? (See workshop paper submission feedback).
%     \item age-related linguistic features that inform classification lie more at the syntactic level than at the lexical level.
%     \item A small discussion point on the effects of stopword omission on classification performance
%     \item \len{Re-read the relevant sections of the workshop paper submission}
% \end{itemize}

% \paragraph{Discussion points about generation}
% \begin{itemize}
%     \item What are the effects of prompts on generation? \len{This should probably be an analysis question.}
%     \item What are the limitations of your setup?
%         \begin{itemize}
%             \item Perplexity is a crude proxy for fluency and grammatical correctness.
%         \end{itemize}
%     \item Does this make the world better? How can this help people? --> It can help personalize virtual assistants (especially useful for new speakers of a language. E.g., the difference between young/informal/spoken French and French that is taught in school and courses is large. User-age personalization can adapt use of language of virtual assistants to variant of language spoken by user.)
%     \item What are the dangers of these methods? --> Read up on paper by Ebru et al
%     \item What are interesting future research directions?
%     \item My research is a promising step towards the development of personalized virtual assistants.
%     \item Keep in mind that...
%         \begin{itemize}
%             \item  The representations for young and old style used in this research are specific to the BNC, and should not be interpreted as generally representative of speaking style of 19-29 and 50-plus.
%             \item Even within the context of the BNC, the representations used for classification and generation are indicative of textual features learned to coincide with utterances from certain age groups.
%             \item despite formulation of binary classification requiring young-prob and old-prob to be complementary values, "young" and "old" speaking/writing styles are not semantically opposite styles, like positive vs negative sentiment tend to resemble more \len{Maybe think of a better example of semantic polar opposites. Also, maybe also mention that it still needs to be verified whether they are semantically opposite or not, but that you hypothesize that they are not. Idea for future research.}. So I shouldn't expect certain patterns in, e.g., ppl plots to be exactly opposite.
%         \end{itemize}
%     \item Future research idea: adapting the PPLM-setup to work with $n$-gram lists for arbitrary $n$.
%         \begin{itemize}
%             \item Finding a way to by-pass the need to retrain GPT-2 for arbitrary n-grams
%         \end{itemize}
%     \item Future research idea: real-time interactivity of age-adaptive conversational systems. I.e., a pipeline that (1) "starts off neutral", (2) classifies user's age based on minimal amount of utterances, (2*) uses bayesian modelling or reinforcement learning to constantly update belief, (3) adapts use of language to perceived user age.
%     \item Future research idea: how to probe PPLM models, because BertViz doesn't work, as the attention weights are unchanged by PPLM.
%     \item Emphasize the importance of PPLM-methods w.r.t. carbon footprint and the ecological cost of (re)training massive language models like GPT-x (Maybe this is better for the introduction?)
% \end{itemize}