@inproceedings{
dathathri2019plug,
title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1edEyBKDS}
}

@article{dai2019style,
  title={Style transformer: Unpaired text style transfer without disentangled latent representation},
  author={Dai, Ning and Liang, Jianze and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1905.05621},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@article{pennebaker2003words,
  title={Words of wisdom: Language use over the life span.},
  author={Pennebaker, James W and Stone, Lori D},
  journal={Journal of Personality and Social Psychology},
  volume={85},
  number={2},
  pages={291-301},
  year={2003},
  publisher={American Psychological Association},
  url={https://doi.org/10.1037/0022-3514.85.2.291}
}

@inproceedings{li-etal-2020-optimus,
    title = "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
    author = "Li, Chunyuan  and
      Gao, Xiang  and
      Li, Yuan  and
      Peng, Baolin  and
      Li, Xiujun  and
      Zhang, Yizhe  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.378",
    doi = "10.18653/v1/2020.emnlp-main.378",
    pages = "4678--4699",
    abstract = "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.",
}

@inproceedings{zhang2019dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2020},
    booktitle={ACL, system demonstration}
}

@article{zheng2019personalized,
  title={Personalized dialogue generation with diversified traits},
  author={Zheng, Yinhe and Chen, Guanyi and Huang, Minlie and Liu, Song and Zhu, Xuan},
  journal={arXiv preprint arXiv:1901.09672},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{zeng-etal-2020-meddialog,
    title = "{M}ed{D}ialog: Large-scale Medical Dialogue Datasets",
    author = "Zeng, Guangtao  and
      Yang, Wenmian  and
      Ju, Zeqian  and
      Yang, Yue  and
      Wang, Sicheng  and
      Zhang, Ruisi  and
      Zhou, Meng  and
      Zeng, Jiaqi  and
      Dong, Xiangyu  and
      Zhang, Ruoyu  and
      Fang, Hongchao  and
      Zhu, Penghui  and
      Chen, Shu  and
      Xie, Pengtao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.743",
    doi = "10.18653/v1/2020.emnlp-main.743",
    pages = "9241--9250",
    abstract = "Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets {--} MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",
}


@article{baan-etal-2019-abstractive,
  author    = {Joris Baan and
               Maartje ter Hoeve and
               Marlies van der Wees and
               Anne Schuth and
               Maarten de Rijke},
  title     = {Do Transformer Attention Heads Provide Transparency in Abstractive
               Summarization?},
  journal   = {CoRR},
  volume    = {abs/1907.00570},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.00570},
  archivePrefix = {arXiv},
  eprint    = {1907.00570},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-00570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{love-spoken-bnc-2014,
    author = {R Love and
              C Dembry and
              A Hardie and
              V Brezina and
              T McEnery},
    title = {The Spoken BNC2014: designing and building a spoken corpus of everyday                   conversations.},
    journal = {In International Journal of Corpus Linguistics},
    volume = {22},
    number = {3},
    pages = {319-344},
    year = {2017}
}

@inproceedings{schler2006effects,
  title={Effects of age and gender on blogging.},
  author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},
  booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},
  volume={6},
  pages={199--205},
  year={2006}
}

@article{DBLP:journals/corr/BaKH16,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  archivePrefix = {arXiv},
  eprint    = {1607.06450},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{he2016residual,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
  
  @inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{madotto-etal-2020-plug,
    title = "Plug-and-Play Conversational Models",
    author = "Madotto, Andrea  and
      Ishii, Etsuko  and
      Lin, Zhaojiang  and
      Dathathri, Sumanth  and
      Fung, Pascale",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.219",
    doi = "10.18653/v1/2020.findings-emnlp.219",
    pages = "2422--2433",
    abstract = "There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.",
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}

@inproceedings{nguyen2015deep,
  title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on},
  year={2015},
  organization={IEEE}
}

@inproceedings{stahlberg-etal-2018-simple,
    title = "Simple Fusion: Return of the Language Model",
    author = "Stahlberg, Felix  and
      Cross, James  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6321",
    doi = "10.18653/v1/W18-6321",
    pages = "204--211",
    abstract = "Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.",
}

@inproceedings{bapna-firat-2019-simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}

@inproceedings{nguyen-etal-2011-author,
    title = "Author Age Prediction from Text using Linear Regression",
    author = "Nguyen, Dong  and
      Smith, Noah A.  and
      Ros{\'e}, Carolyn P.",
    booktitle = "Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
    month = jun,
    year = "2011",
    address = "Portland, OR, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W11-1515",
    pages = "115--123",
}

@article{abdallah2020age,
  title={Age and Gender prediction in Open Domain Text},
  author={Abdallah, Emad E and Alzghoul, Jamil R and Alzghool, Muath},
  journal={Procedia Computer Science},
  volume={170},
  pages={563--570},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{nguyen2014gender,
  title={Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment},
  author={Nguyen, Dong and Trieschnigg, Dolf and Do{\u{g}}ru{\"o}z, A Seza and Gravel, Rilana and Theune, Mari{\"e}t and Meder, Theo and De Jong, Franciska},
  booktitle={Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  pages={1950--1961},
  year={2014}
}

@inproceedings{
lample2018multipleattribute,
title={Multiple-Attribute Text Rewriting},
author={Guillaume Lample and Sandeep Subramanian and Eric Smith and Ludovic Denoyer and Marc'Aurelio Ranzato and Y-Lan Boureau},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1g2NhC5KQ},
}