@article{mctear2020conversational,
  title={Conversational AI: Dialogue Systems, Conversational Agents, and Chatbots},
  author={McTear, Michael},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={13},
  number={3},
  pages={1--251},
  year={2020},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{
dathathri2019plug,
title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1edEyBKDS}
}

@article{dai2019style,
  title={Style transformer: Unpaired text style transfer without disentangled latent representation},
  author={Dai, Ning and Liang, Jianze and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1905.05621},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@article{pennebaker2003words,
  title={Words of wisdom: Language use over the life span.},
  author={Pennebaker, James W and Stone, Lori D},
  journal={Journal of Personality and Social Psychology},
  volume={85},
  number={2},
  pages={291-301},
  year={2003},
  publisher={American Psychological Association},
  url={https://doi.org/10.1037/0022-3514.85.2.291}
}

@inproceedings{li-etal-2020-optimus,
    title = "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
    author = "Li, Chunyuan  and
      Gao, Xiang  and
      Li, Yuan  and
      Peng, Baolin  and
      Li, Xiujun  and
      Zhang, Yizhe  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.378",
    doi = "10.18653/v1/2020.emnlp-main.378",
    pages = "4678--4699",
    abstract = "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.",
}

@inproceedings{zhang2019dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Zhang, Yizhe and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2020},
    booktitle={ACL, system demonstration}
}

@article{zheng2019personalized,
  title={Personalized dialogue generation with diversified traits},
  author={Zheng, Yinhe and Chen, Guanyi and Huang, Minlie and Liu, Song and Zhu, Xuan},
  journal={arXiv preprint arXiv:1901.09672},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{zeng-etal-2020-meddialog,
    title = "{M}ed{D}ialog: Large-scale Medical Dialogue Datasets",
    author = "Zeng, Guangtao  and
      Yang, Wenmian  and
      Ju, Zeqian  and
      Yang, Yue  and
      Wang, Sicheng  and
      Zhang, Ruisi  and
      Zhou, Meng  and
      Zeng, Jiaqi  and
      Dong, Xiangyu  and
      Zhang, Ruoyu  and
      Fang, Hongchao  and
      Zhu, Penghui  and
      Chen, Shu  and
      Xie, Pengtao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.743",
    doi = "10.18653/v1/2020.emnlp-main.743",
    pages = "9241--9250",
    abstract = "Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets {--} MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",
}


@article{baan-etal-2019-abstractive,
  author    = {Joris Baan and
               Maartje ter Hoeve and
               Marlies van der Wees and
               Anne Schuth and
               Maarten de Rijke},
  title     = {Do Transformer Attention Heads Provide Transparency in Abstractive
               Summarization?},
  journal   = {CoRR},
  volume    = {abs/1907.00570},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.00570},
  archivePrefix = {arXiv},
  eprint    = {1907.00570},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-00570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{love-spoken-bnc-2014,
    author = {R Love and
              C Dembry and
              A Hardie and
              V Brezina and
              T McEnery},
    title = {The Spoken BNC2014: designing and building a spoken corpus of everyday                   conversations.},
    journal = {In International Journal of Corpus Linguistics},
    volume = {22},
    number = {3},
    pages = {319-344},
    year = {2017}
}

@inproceedings{schler2006effects,
  title={Effects of age and gender on blogging.},
  author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},
  booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},
  volume={6},
  pages={199--205},
  year={2006}
}

@article{DBLP:journals/corr/BaKH16,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  archivePrefix = {arXiv},
  eprint    = {1607.06450},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{he2016residual,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
  
  @inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{madotto-etal-2020-plug,
    title = "Plug-and-Play Conversational Models",
    author = "Madotto, Andrea  and
      Ishii, Etsuko  and
      Lin, Zhaojiang  and
      Dathathri, Sumanth  and
      Fung, Pascale",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.219",
    doi = "10.18653/v1/2020.findings-emnlp.219",
    pages = "2422--2433",
    abstract = "There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.",
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}

@inproceedings{nguyen2015deep,
  title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on},
  year={2015},
  organization={IEEE}
}

@inproceedings{stahlberg-etal-2018-simple,
    title = "Simple Fusion: Return of the Language Model",
    author = "Stahlberg, Felix  and
      Cross, James  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6321",
    doi = "10.18653/v1/W18-6321",
    pages = "204--211",
    abstract = "Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.",
}

@inproceedings{bapna-firat-2019-simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}

@inproceedings{nguyen-etal-2011-author,
    title = "Author Age Prediction from Text using Linear Regression",
    author = "Nguyen, Dong  and
      Smith, Noah A.  and
      Ros{\'e}, Carolyn P.",
    booktitle = "Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
    month = jun,
    year = "2011",
    address = "Portland, OR, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W11-1515",
    pages = "115--123",
}

@article{abdallah2020age,
  title={Age and Gender prediction in Open Domain Text},
  author={Abdallah, Emad E and Alzghoul, Jamil R and Alzghool, Muath},
  journal={Procedia Computer Science},
  volume={170},
  pages={563--570},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{nguyen2014gender,
  title={Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment},
  author={Nguyen, Dong and Trieschnigg, Dolf and Do{\u{g}}ru{\"o}z, A Seza and Gravel, Rilana and Theune, Mari{\"e}t and Meder, Theo and De Jong, Franciska},
  booktitle={Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  pages={1950--1961},
  year={2014}
}

@inproceedings{
lample2018multipleattribute,
title={Multiple-Attribute Text Rewriting},
author={Guillaume Lample and Sandeep Subramanian and Eric Smith and Ludovic Denoyer and Marc'Aurelio Ranzato and Y-Lan Boureau},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1g2NhC5KQ},
}

@article{keskarCTRL2019,
  title={{CTRL - A Conditional Transformer Language Model for Controllable Generation}},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@inproceedings{Kingma2014,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}

@article{gallois2015communication,
  title={Communication accommodation theory},
  author={Gallois, Cindy and Giles, Howard},
  journal={The international encyclopedia of language and social interaction},
  pages={1--18},
  year={2015},
  publisher={Wiley Online Library}
}

@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
  crossref={conf/iclr/2015}
}


@inproceedings{wolters2009age,
  title={Age recognition for spoken dialogue systems: Do we need it?},
  author={Wolters, Maria and Vipperla, Ravichander and Renals, Steve},
  booktitle={Tenth Annual Conference of the International Speech Communication Association (Interspeech)},
  year={2009}
}

@article{ficler2017controlling,
  title={Controlling linguistic style aspects in neural language generation},
  author={Ficler, Jessica and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1707.02633},
  year={2017}
}

@inproceedings{ficler-goldberg-2017-controlling,
    title = "Controlling Linguistic Style Aspects in Neural Language Generation",
    author = "Ficler, Jessica  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Workshop on Stylistic Variation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4912",
    doi = "10.18653/v1/W17-4912",
    pages = "94--104",
    abstract = "Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.",
}


@article{gatt2018survey,
  title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
  author={Gatt, Albert and Krahmer, Emiel},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={65--170},
  year={2018}
}

@inproceedings{zhang-etal-2018-personalizing,
    title = "Personalizing Dialogue Agents: {I} have a dog, do you have pets too?",
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}


@inproceedings{van2019exploring,
  title={Exploring age differences in motivations for and acceptance of chatbot communication in a customer service context},
  author={van der Goot, Margot J and Pilgrim, Tyler},
  booktitle={International Workshop on Chatbot Research and Design},
  pages={173--186},
  year={2019},
  organization={Springer}
}

@article{li2013automatic,
  title={Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
  author={Li, Ming and Han, Kyu J and Narayanan, Shrikanth},
  journal={Computer Speech \& Language},
  volume={27},
  number={1},
  pages={151--167},
  year={2013},
  publisher={Elsevier}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
} 

@inproceedings{nguyen2017plug,
  title={Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space},
  author={Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017},
  organization={IEEE}
}

@article{zhang2018styletranslation,
  author    = {Zhirui Zhang and
               Shuo Ren and
               Shujie Liu and
               Jianyong Wang and
               Peng Chen and
               Mu Li and
               Ming Zhou and
               Enhong Chen},
  title     = {Style Transfer as Unsupervised Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1808.07894},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07894},
  eprinttype = {arXiv},
  eprint    = {1808.07894},
  timestamp = {Mon, 21 Sep 2020 09:25:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-07894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{CLT2008springer,
title="Central Limit Theorem",
bookTitle="The Concise Encyclopedia of Statistics",
year="2008",
publisher="Springer New York",
address="New York, NY",
pages="66--68",
isbn="978-0-387-32833-1",
doi="10.1007/978-0-387-32833-1_50",
url="https://doi.org/10.1007/978-0-387-32833-1_50"
}

@inproceedings{prabhumoye-etal-2020-exploring,
    title = "Exploring Controllable Text Generation Techniques",
    author = "Prabhumoye, Shrimai  and
      Black, Alan W  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.1",
    doi = "10.18653/v1/2020.coling-main.1",
    pages = "1--14",
    abstract = "Neural controllable text generation is an important area gaining attention due to its plethora of applications. Although there is a large body of prior work in controllable text generation, there is no unifying theme. In this work, we provide a new schema of the pipeline of the generation process by classifying it into five modules. The control of attributes in the generation process requires modification of these modules. We present an overview of different techniques used to perform the modulation of these modules. We also provide an analysis on the advantages and disadvantages of these techniques. We further pave ways to develop new architectures based on the combination of the modules described in this paper.",
}

@inproceedings{vig-2019-multiscale,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
}

@book{bohm2013dialogue,
  title={On dialogue},
  author={Bohm, David and Nichol, Lee},
  year={2013},
  publisher={Routledge}
}

@inproceedings{welleck-etal-2019-dialogue,
    title = "Dialogue Natural Language Inference",
    author = "Welleck, Sean  and
      Weston, Jason  and
      Szlam, Arthur  and
      Cho, Kyunghyun",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1363",
    doi = "10.18653/v1/P19-1363",
    pages = "3731--3741",
    abstract = "Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model{'}s consistency.",
}

@inproceedings{Kushneryk2019IntelligentDS,
  title={Intelligent Dialogue System Based on Deep Learning Technology},
  author={Pavlo Kushneryk and Yuriy P. Kondratenko and Ievgen V. Sidenko},
  booktitle={ICTERI PhD Symposium},
  year={2019}
}

@article{scheutz2011toward,
  title={Toward humanlike task-based dialogue processing for human robot interaction},
  author={Scheutz, Matthias and Cantrell, Rehj and Schermerhorn, Paul},
  journal={Ai Magazine},
  volume={32},
  number={4},
  pages={77--84},
  year={2011}
}

@article{edlund2008towards,
  title={Towards human-like spoken dialogue systems},
  author={Edlund, Jens and Gustafson, Joakim and Heldner, Mattias and Hjalmarsson, Anna},
  journal={Speech communication},
  volume={50},
  number={8-9},
  pages={630--645},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{ijcai2017-521,
  author    = {Satwik Kottur and Xiaoyu Wang and Vitor Carvalho},
  title     = {Exploring Personalized Neural Conversational Models},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {3728--3734},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/521},
  url       = {https://doi.org/10.24963/ijcai.2017/521},
}

@inproceedings{li-etal-2016-persona,
    title = "A Persona-Based Neural Conversation Model",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Spithourakis, Georgios  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1094",
    doi = "10.18653/v1/P16-1094",
    pages = "994--1003",
}

@inproceedings{ijcai2018-595,
  title     = {Assigning Personality/Profile to a Chatting Machine for Coherent Conversation Generation},
  author    = {Qiao Qian and Minlie Huang and Haizhou Zhao and Jingfang Xu and Xiaoyan Zhu},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {4279--4285},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/595},
  url       = {https://doi.org/10.24963/ijcai.2018/595},
}

@misc{lester2021power,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@article{newitz2019you,
  title={You'e been monetized lol},
  author={Newitz, Annalee},
  journal={New Scientist},
  volume={243},
  number={3240},
  pages={24},
  year={2019},
  publisher={Elsevier}
}

@article{tagliamonte2008linguistic,
  title={Linguistic ruin? LOL! Instant messaging and teen language},
  author={Tagliamonte, Sali A and Denis, Derek},
  journal={American speech},
  volume={83},
  number={1},
  pages={3--34},
  year={2008},
  publisher={Duke University Press}
}

@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@inproceedings{bastings-filippova-2020-elephant,
    title = "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
    author = "Bastings, Jasmijn  and
      Filippova, Katja",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.14",
    doi = "10.18653/v1/2020.blackboxnlp-1.14",
    pages = "149--155",
    abstract = "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}


@inproceedings{wiegreffe-pinter-2019-attention,
    title = "Attention is not not Explanation",
    author = "Wiegreffe, Sarah  and
      Pinter, Yuval",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1002",
    doi = "10.18653/v1/D19-1002",
    pages = "11--20",
    abstract = "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model{'}s prediction, and consequently reach insights regarding the model{'}s decision-making process. A recent paper claims that {`}Attention is not Explanation{'} (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one{'}s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don{'}t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.",
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@inproceedings{brown2020language-models-few-shot-gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{gehrmann-etal-2019-gltr,
    title = "{GLTR}: Statistical Detection and Visualization of Generated Text",
    author = "Gehrmann, Sebastian  and
      Strobelt, Hendrik  and
      Rush, Alexander",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-3019",
    doi = "10.18653/v1/P19-3019",
    pages = "111--116",
    abstract = "The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54{\%} to 72{\%} without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs.",
}

@inproceedings{chen-etal-2017-deep,
    title = "Deep Learning for Dialogue Systems",
    author = {Chen, Yun-Nung  and
      Celikyilmaz, Asli  and
      Hakkani-T{\"u}r, Dilek},
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-5004",
    pages = "8--14",
    abstract = "In the past decade, goal-oriented spoken dialogue systems have been the most prominent component in today's virtual personal assistants. The classic dialogue systems have rather complex and/or modular pipelines. The advance of deep learning technologies has recently risen the applications of neural models to dialogue modeling. However, how to successfully apply deep learning based approaches to a dialogue system is still challenging. Hence, this tutorial is designed to focus on an overview of the dialogue system development while describing most recent research for building dialogue systems and summarizing the challenges, in order to allow researchers to study the potential improvements of the state-of-the-art dialogue systems. The tutorial material is available at http://deepdialogue.miulab.tw.",
}

@incollection{MerriamWebster2021Dialogue_Entry,
  author    = {Merriam-Webster},
  title     = {Dialogue},
  year      = {n.d.},
  booktitle = {Merriam-Webster.com dictionary},
  url       = {https://www.merriam-webster.com/dictionary/dialogue},
  urldate   = {2021-11-20},
  note      = {Retrieved November 20, 2021}
}

@inproceedings{burtsev-etal-2018-deeppavlov,
    title = "{D}eep{P}avlov: Open-Source Library for Dialogue Systems",
    author = "Burtsev, Mikhail  and
      Seliverstov, Alexander  and
      Airapetyan, Rafael  and
      Arkhipov, Mikhail  and
      Baymurzina, Dilyara  and
      Bushkov, Nickolay  and
      Gureenkova, Olga  and
      Khakhulin, Taras  and
      Kuratov, Yuri  and
      Kuznetsov, Denis  and
      Litinsky, Alexey  and
      Logacheva, Varvara  and
      Lymar, Alexey  and
      Malykh, Valentin  and
      Petrov, Maxim  and
      Polulyakh, Vadim  and
      Pugachev, Leonid  and
      Sorokin, Alexey  and
      Vikhreva, Maria  and
      Zaynutdinov, Marat",
    booktitle = "Proceedings of {ACL} 2018, System Demonstrations",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-4021",
    doi = "10.18653/v1/P18-4021",
    pages = "122--127",
    abstract = "Adoption of messaging communication and voice assistants has grown rapidly in the last years. This creates a demand for tools that speed up prototyping of feature-rich dialogue systems. An open-source library DeepPavlov is tailored for development of conversational agents. The library prioritises efficiency, modularity, and extensibility with the goal to make it easier to develop dialogue systems from scratch and with limited data available. It supports modular as well as end-to-end approaches to implementation of conversational agents. Conversational agent consists of skills and every skill can be decomposed into components. Components are usually models which solve typical NLP tasks such as intent classification, named entity recognition or pre-trained word vectors. Sequence-to-sequence chit-chat skill, question answering skill or task-oriented skill can be assembled from components provided in the library.",
}

@incollection{MerriamWebster2021Discourse_Entry,
  author    = {Merriam-Webster},
  title     = {Discourse},
  year      = {n.d.},
  booktitle = {Merriam-Webster.com dictionary},
  url       = {https://www.merriam-webster.com/dictionary/discourse},
  urldate   = {2021-11-20},
  note      = {Retrieved November 20, 2021}
}

@article{oppy2003turing,
  title={The turing test},
  author={Oppy, Graham and Dowe, David},
  year={2003}
}

@inproceedings{shuster-etal-2021-multi,
    title = "Multi-Modal Open-Domain Dialogue",
    author = "Shuster, Kurt  and
      Smith, Eric Michael  and
      Ju, Da  and
      Weston, Jason",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.398",
    pages = "4863--4883",
    abstract = "Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.",
}

@inproceedings{li-etal-2016-deep,
    title = "Deep Reinforcement Learning for Dialogue Generation",
    author = "Li, Jiwei  and
      Monroe, Will  and
      Ritter, Alan  and
      Jurafsky, Dan  and
      Galley, Michel  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1127",
    doi = "10.18653/v1/D16-1127",
    pages = "1192--1202",
}

@inproceedings{ruan2019condition,
  title={Condition-transforming variational autoencoder for conversation response generation},
  author={Ruan, Yu-Ping and Ling, Zhen-Hua and Liu, Quan and Chen, Zhigang and Indurkhya, Nitin},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7215--7219},
  year={2019},
  organization={IEEE}
}

@inproceedings{mo2018personalizing,
  title={Personalizing a dialogue system with transfer reinforcement learning},
  author={Mo, Kaixiang and Zhang, Yu and Li, Shuangyin and Li, Jiajun and Yang, Qiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{traum1996utterance,
  title={Utterance units in spoken dialogue},
  author={Traum, David R and Heeman, Peter A},
  booktitle={Workshop on Dialogue Processing in Spoken Language Systems},
  pages={125--140},
  year={1996},
  organization={Springer}
}
