@article{mctear2020conversational,
  title={Conversational AI: Dialogue Systems, Conversational Agents, and Chatbots},
  author={McTear, Michael},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={13},
  number={3},
  pages={1--251},
  year={2020},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{
dathathri2019plug,
title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
author={Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and Jason Yosinski and Rosanne Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1edEyBKDS}
}

@article{dai2019style,
  title={Style transformer: Unpaired text style transfer without disentangled latent representation},
  author={Dai, Ning and Liang, Jianze and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1905.05621},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@article{pennebaker2003words,
  title={Words of wisdom: Language use over the life span.},
  author={Pennebaker, James W and Stone, Lori D},
  journal={Journal of Personality and Social Psychology},
  volume={85},
  number={2},
  pages={291-301},
  year={2003},
  publisher={American Psychological Association},
  url={https://doi.org/10.1037/0022-3514.85.2.291}
}

@inproceedings{li-etal-2020-optimus,
    title = "Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space",
    author = "Li, Chunyuan  and
      Gao, Xiang  and
      Li, Yuan  and
      Peng, Baolin  and
      Li, Xiujun  and
      Zhang, Yizhe  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.378",
    doi = "10.18653/v1/2020.emnlp-main.378",
    pages = "4678--4699",
    abstract = "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space). A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks.",
}

@inproceedings{zhang2019dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Zhang, Yizhe and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2020},
    booktitle={ACL, system demonstration}
}

@article{zheng2019personalized,
  title={Personalized dialogue generation with diversified traits},
  author={Zheng, Yinhe and Chen, Guanyi and Huang, Minlie and Liu, Song and Zhu, Xuan},
  journal={arXiv preprint arXiv:1901.09672},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{de2019bertje,
  title={Bertje: A dutch bert model},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
  journal={arXiv preprint arXiv:1912.09582},
  year={2019}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}

@inproceedings{zeng-etal-2020-meddialog,
    title = "{M}ed{D}ialog: Large-scale Medical Dialogue Datasets",
    author = "Zeng, Guangtao  and
      Yang, Wenmian  and
      Ju, Zeqian  and
      Yang, Yue  and
      Wang, Sicheng  and
      Zhang, Ruisi  and
      Zhou, Meng  and
      Zeng, Jiaqi  and
      Dong, Xiangyu  and
      Zhang, Ruoyu  and
      Fang, Hongchao  and
      Zhu, Penghui  and
      Chen, Shu  and
      Xie, Pengtao",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.743",
    doi = "10.18653/v1/2020.emnlp-main.743",
    pages = "9241--9250",
    abstract = "Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets {--} MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and doctor-like medical dialogues. We also study the transferability of models trained on MedDialog to low-resource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation. The datasets and code are available at https://github.com/UCSD-AI4H/Medical-Dialogue-System",
}


@article{baan-etal-2019-abstractive,
  author    = {Joris Baan and
               Maartje ter Hoeve and
               Marlies van der Wees and
               Anne Schuth and
               Maarten de Rijke},
  title     = {Do Transformer Attention Heads Provide Transparency in Abstractive
               Summarization?},
  journal   = {CoRR},
  volume    = {abs/1907.00570},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.00570},
  archivePrefix = {arXiv},
  eprint    = {1907.00570},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-00570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{love-spoken-bnc-2014,
    author = {R Love and
              C Dembry and
              A Hardie and
              V Brezina and
              T McEnery},
    title = {The Spoken BNC2014: designing and building a spoken corpus of everyday                   conversations.},
    journal = {In International Journal of Corpus Linguistics},
    volume = {22},
    number = {3},
    pages = {319-344},
    year = {2017}
}

@inproceedings{schler2006effects,
  title={Effects of age and gender on blogging.},
  author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},
  booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},
  volume={6},
  pages={199--205},
  year={2006}
}

@article{DBLP:journals/corr/BaKH16,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  archivePrefix = {arXiv},
  eprint    = {1607.06450},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{he2016residual,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}
  
  @inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{madotto-etal-2020-plug,
    title = "Plug-and-Play Conversational Models",
    author = "Madotto, Andrea  and
      Ishii, Etsuko  and
      Lin, Zhaojiang  and
      Dathathri, Sumanth  and
      Fung, Pascale",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.219",
    doi = "10.18653/v1/2020.findings-emnlp.219",
    pages = "2422--2433",
    abstract = "There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated responses, and this control is further limited in the absence of annotated conversational datasets for attribute specific generation that can be used for fine-tuning the model. In this paper, we first propose and evaluate plug-and-play methods for controllable response generation, which does not require dialogue specific datasets and does not rely on fine-tuning a large model. While effective, the decoding procedure induces considerable computational overhead, rendering the conversational model unsuitable for interactive usage. To overcome this, we introduce an approach that does not require further computation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.",
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}

@inproceedings{nguyen2015deep,
  title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on},
  year={2015},
  organization={IEEE}
}

@inproceedings{stahlberg-etal-2018-simple,
    title = "Simple Fusion: Return of the Language Model",
    author = "Stahlberg, Felix  and
      Cross, James  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6321",
    doi = "10.18653/v1/W18-6321",
    pages = "204--211",
    abstract = "Neural Machine Translation (NMT) typically leverages monolingual data in training through backtranslation. We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch. To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM. This enables the TM to focus its capacity on modeling the source sentence since it can rely on the LM for fluency. We show that our method outperforms previous approaches to integrate LMs into NMT while the architecture is simpler as it does not require gating networks to balance TM and LM. We observe gains of between +0.24 and +2.36 BLEU on all four test sets (English-Turkish, Turkish-English, Estonian-English, Xhosa-English) on top of ensembles without LM. We compare our method with alternative ways to utilize monolingual data such as backtranslation, shallow fusion, and cold fusion.",
}

@inproceedings{bapna-firat-2019-simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}

@inproceedings{nguyen-etal-2011-author,
    title = "Author Age Prediction from Text using Linear Regression",
    author = "Nguyen, Dong  and
      Smith, Noah A.  and
      Ros{\'e}, Carolyn P.",
    booktitle = "Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
    month = jun,
    year = "2011",
    address = "Portland, OR, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W11-1515",
    pages = "115--123",
}

@article{abdallah2020age,
  title={Age and Gender prediction in Open Domain Text},
  author={Abdallah, Emad E and Alzghoul, Jamil R and Alzghool, Muath},
  journal={Procedia Computer Science},
  volume={170},
  pages={563--570},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{nguyen2014gender,
  title={Why gender and age prediction from tweets is hard: Lessons from a crowdsourcing experiment},
  author={Nguyen, Dong and Trieschnigg, Dolf and Do{\u{g}}ru{\"o}z, A Seza and Gravel, Rilana and Theune, Mari{\"e}t and Meder, Theo and De Jong, Franciska},
  booktitle={Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  pages={1950--1961},
  year={2014}
}

@inproceedings{
lample2018multipleattribute,
title={Multiple-Attribute Text Rewriting},
author={Guillaume Lample and Sandeep Subramanian and Eric Smith and Ludovic Denoyer and Marc'Aurelio Ranzato and Y-Lan Boureau},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1g2NhC5KQ},
}

@article{keskarCTRL2019,
  title={{CTRL - A Conditional Transformer Language Model for Controllable Generation}},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@inproceedings{Kingma2014,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  added-at = {2020-10-15T14:36:56.000+0200},
  author = {Kingma, Diederik P. and Welling, Max},
  biburl = {https://www.bibsonomy.org/bibtex/242e5be6faa01cba2587f4907ac99dce8/annakrause},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  eprint = {http://arxiv.org/abs/1312.6114v10},
  eprintclass = {stat.ML},
  eprinttype = {arXiv},
  file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
  interhash = {a626a9d77a123c52405a08da983203cb},
  intrahash = {42e5be6faa01cba2587f4907ac99dce8},
  keywords = {cs.LG stat.ML vae},
  timestamp = {2021-02-01T17:13:18.000+0100},
  title = {{Auto-Encoding Variational Bayes}},
  year = 2014
}

@article{gallois2015communication,
  title={Communication accommodation theory},
  author={Gallois, Cindy and Giles, Howard},
  journal={The international encyclopedia of language and social interaction},
  pages={1--18},
  year={2015},
  publisher={Wiley Online Library}
}

@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{schuster1997bidirectional,
  title={Bidirectional recurrent neural networks},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}

@inproceedings{DBLP:journals/corr/KingmaB14,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  cdate={1420070400000},
  url={http://arxiv.org/abs/1412.6980},
  booktitle={ICLR (Poster)},
  crossref={conf/iclr/2015}
}


@inproceedings{wolters2009age,
  title={Age recognition for spoken dialogue systems: Do we need it?},
  author={Wolters, Maria and Vipperla, Ravichander and Renals, Steve},
  booktitle={Tenth Annual Conference of the International Speech Communication Association (Interspeech)},
  year={2009}
}

@article{ficler2017controlling,
  title={Controlling linguistic style aspects in neural language generation},
  author={Ficler, Jessica and Goldberg, Yoav},
  journal={arXiv preprint arXiv:1707.02633},
  year={2017}
}

@inproceedings{ficler-goldberg-2017-controlling,
    title = "Controlling Linguistic Style Aspects in Neural Language Generation",
    author = "Ficler, Jessica  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the Workshop on Stylistic Variation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4912",
    doi = "10.18653/v1/W17-4912",
    pages = "94--104",
    abstract = "Most work on neural natural language generation (NNLG) focus on controlling the content of the generated text. We experiment with controling several stylistic aspects of the generated text, in addition to its content. The method is based on conditioned RNN language model, where the desired content as well as the stylistic parameters serve as conditioning contexts. We demonstrate the approach on the movie reviews domain and show that it is successful in generating coherent sentences corresponding to the required linguistic style and content.",
}


@article{gatt2018survey,
  title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
  author={Gatt, Albert and Krahmer, Emiel},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={65--170},
  year={2018}
}

@inproceedings{zhang-etal-2018-personalizing,
    title = "Personalizing Dialogue Agents: {I} have a dog, do you have pets too?",
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}


@inproceedings{van2019exploring,
  title={Exploring age differences in motivations for and acceptance of chatbot communication in a customer service context},
  author={van der Goot, Margot J and Pilgrim, Tyler},
  booktitle={International Workshop on Chatbot Research and Design},
  pages={173--186},
  year={2019},
  organization={Springer}
}

@article{li2013automatic,
  title={Automatic speaker age and gender recognition using acoustic and prosodic level information fusion},
  author={Li, Ming and Han, Kyu J and Narayanan, Shrikanth},
  journal={Computer Speech \& Language},
  volume={27},
  number={1},
  pages={151--167},
  year={2013},
  publisher={Elsevier}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
} 

@inproceedings{nguyen2017plug,
  title={Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space},
  author={Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017},
  organization={IEEE}
}

@article{zhang2018styletranslation,
  author    = {Zhirui Zhang and
               Shuo Ren and
               Shujie Liu and
               Jianyong Wang and
               Peng Chen and
               Mu Li and
               Ming Zhou and
               Enhong Chen},
  title     = {Style Transfer as Unsupervised Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1808.07894},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07894},
  eprinttype = {arXiv},
  eprint    = {1808.07894},
  timestamp = {Mon, 21 Sep 2020 09:25:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-07894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{CLT2008springer,
title="Central Limit Theorem",
bookTitle="The Concise Encyclopedia of Statistics",
year="2008",
publisher="Springer New York",
address="New York, NY",
pages="66--68",
isbn="978-0-387-32833-1",
doi="10.1007/978-0-387-32833-1_50",
url="https://doi.org/10.1007/978-0-387-32833-1_50"
}
